{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating music with RNN's the simple way\n",
    "\n",
    "Whilst most of the RNN applications required large amount of training time and resources, music generation is surprisingly easy to achieve *good* results. In this tutorial heavily based on \\cite{blog} we will apply simple RNN's networks to produce traditional folk tunes.\n",
    "\n",
    "## The data\n",
    "\n",
    "Perhaps the most important question we must answer before getting in greater detail is which data representation we will use, since this will determine the complexity of the model required. Here, since we don't want to waste weeks training our models we will use a very simple approach: The ABC notation.\n",
    "\n",
    "### ABC notation\n",
    "\n",
    "Citing from the wikipedia \\cite{wikipedia}:\n",
    "\n",
    ">ABC notation is a shorthand form of musical notation. In basic form it uses the letters A through G to represent the given notes, with other elements used to place added value on these - sharp, flat, the length of the note, key, ornamentation. Lines in the first part of the tune notation, beginning with a letter followed by a colon, indicate various aspects of the tune such as the index, when there are more than one tune in a file (X:), the title (T:), the time signature (M:), the default note length (L:), the type of tune (R:) and the key (K:). Lines following the key designation represent the tune. It can be translated into traditional music notation using one of the abc conversion tools.\n",
    "\n",
    "For instance here we have an example of \"Greensleves\" written in ABC notation\n",
    "\n",
    "``` abc\n",
    "X:870\n",
    "T:Greensleeves\n",
    "C:anon.\n",
    "O:England\n",
    "R:Broadside ballad\n",
    "Z:Transcribed by Frank Nordberg - http://www.musicaviva.com\n",
    "F:http://abc.musicaviva.com/tunes/england/greensleeves-dorian.abc\n",
    "M:6/4\n",
    "L:1/4\n",
    "Q:1/2=110\n",
    "K:Gdor\n",
    "G|\"Gm\"B2c d>ed|\"F\"c2A F>GA|\"Gm\"B2A G>^FG|\"Dm\"A2^F D2G|\n",
    "\"Gm\"B2c d>ed|\"F\"c2A F>GA|\"Gm\"B>AG \"D\"^F>EF|\"Gm\"G3 G2z|\n",
    "\"Bb\"f3 f>ed|\"F\"c2A \"Dm\"F>GA|\"Gm\"B2G G>^FG|\"Dm\"A2^F D2z|\n",
    "\"Bb\"f3 f>ed|\"F\"c2A \"Dm\"F>GA|\"Gm\"B>AG \"D\"^F>EF|\"G\"G3 G2|]\n",
    "```\n",
    "\n",
    "This notation is great because it enables to generate music only with the common alphabet set, without complicated notations and symbols. Also, a vast amount of tunes is available in the internets, which is important for learning.\n",
    "\n",
    "## The model\n",
    "\n",
    "For this exercise we will be using a simple implementation of rnn for text called char-rnn, originally developed by the omnipresent Karpathy. An implementation written in tersorflow is available at github.\n",
    "\n",
    "It has two scripts, one for training and one for sampling. We can see the many options available calling them with *-h*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'char-rnn-tensorflow'...\n",
      "remote: Counting objects: 143, done.\u001b[K\n",
      "remote: Total 143 (delta 0), reused 0 (delta 0), pack-reused 143\u001b[K\n",
      "Receiving objects: 100% (143/143), 455.78 KiB | 261.00 KiB/s, done.\n",
      "Resolving deltas: 100% (76/76), done.\n",
      "Checking connectivity... done.\n",
      "usage: train.py [-h] [--data_dir DATA_DIR] [--save_dir SAVE_DIR]\n",
      "                [--rnn_size RNN_SIZE] [--num_layers NUM_LAYERS]\n",
      "                [--model MODEL] [--batch_size BATCH_SIZE]\n",
      "                [--seq_length SEQ_LENGTH] [--num_epochs NUM_EPOCHS]\n",
      "                [--save_every SAVE_EVERY] [--grad_clip GRAD_CLIP]\n",
      "                [--learning_rate LEARNING_RATE] [--decay_rate DECAY_RATE]\n",
      "                [--init_from INIT_FROM]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data_dir DATA_DIR   data directory containing input.txt\n",
      "  --save_dir SAVE_DIR   directory to store checkpointed models\n",
      "  --rnn_size RNN_SIZE   size of RNN hidden state\n",
      "  --num_layers NUM_LAYERS\n",
      "                        number of layers in the RNN\n",
      "  --model MODEL         rnn, gru, or lstm\n",
      "  --batch_size BATCH_SIZE\n",
      "                        minibatch size\n",
      "  --seq_length SEQ_LENGTH\n",
      "                        RNN sequence length\n",
      "  --num_epochs NUM_EPOCHS\n",
      "                        number of epochs\n",
      "  --save_every SAVE_EVERY\n",
      "                        save frequency\n",
      "  --grad_clip GRAD_CLIP\n",
      "                        clip gradients at this value\n",
      "  --learning_rate LEARNING_RATE\n",
      "                        learning rate\n",
      "  --decay_rate DECAY_RATE\n",
      "                        decay rate for rmsprop\n",
      "  --init_from INIT_FROM\n",
      "                        continue training from saved model at this path. Path\n",
      "                        must contain files saved by previous training process:\n",
      "                        'config.pkl' : configuration; 'chars_vocab.pkl' :\n",
      "                        vocabulary definitions; 'checkpoint' : paths to model\n",
      "                        file(s) (created by tf). Note: this file contains\n",
      "                        absolute paths, be careful when moving files around;\n",
      "                        'model.ckpt-*' : file(s) with model definition\n",
      "                        (created by tf)\n",
      "usage: sample.py [-h] [--save_dir SAVE_DIR] [-n N] [--prime PRIME]\n",
      "                 [--sample SAMPLE]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help           show this help message and exit\n",
      "  --save_dir SAVE_DIR  model directory to store checkpointed models\n",
      "  -n N                 number of characters to sample\n",
      "  --prime PRIME        prime text\n",
      "  --sample SAMPLE      0 to use max at each timestep, 1 to sample at each\n",
      "                       timestep, 2 to sample on spaces\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/sherjilozair/char-rnn-tensorflow.git\n",
    "!python ./char-rnn-tensorflow/train.py -h\n",
    "!python ./char-rnn-tensorflow/sample.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With regard to the hyperparameters by default, if we take a look into train.py we can see which ones it uses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " parser.add_argument('--data_dir', type=str, default='data/tinyshakespeare',\n",
    "                       help='data directory containing input.txt')\n",
    "    parser.add_argument('--save_dir', type=str, default='save',\n",
    "                       help='directory to store checkpointed models')\n",
    "    parser.add_argument('--rnn_size', type=int, default=128,\n",
    "                       help='size of RNN hidden state')\n",
    "    parser.add_argument('--num_layers', type=int, default=2,\n",
    "                       help='number of layers in the RNN')\n",
    "    parser.add_argument('--model', type=str, default='lstm',\n",
    "                       help='rnn, gru, or lstm')\n",
    "    parser.add_argument('--batch_size', type=int, default=50,\n",
    "                       help='minibatch size')\n",
    "    parser.add_argument('--seq_length', type=int, default=50,\n",
    "                       help='RNN sequence length')\n",
    "    parser.add_argument('--num_epochs', type=int, default=50,\n",
    "                       help='number of epochs')\n",
    "    parser.add_argument('--save_every', type=int, default=1000,\n",
    "                       help='save frequency')\n",
    "    parser.add_argument('--grad_clip', type=float, default=5.,\n",
    "                       help='clip gradients at this value')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.002,\n",
    "                       help='learning rate')\n",
    "    parser.add_argument('--decay_rate', type=float, default=0.97,\n",
    "                       help='decay rate for rmsprop')                       \n",
    "    parser.add_argument('--init_from', type=str, default=None,\n",
    "                       help=\"\"\"continue training from saved model at this path. Path must contain files saved by previous training process: \n",
    "                            'config.pkl'        : configuration;\n",
    "                            'chars_vocab.pkl'   : vocabulary definitions;\n",
    "                            'checkpoint'        : paths to model file(s) (created by tf).\n",
    "                                                  Note: this file contains absolute paths, be careful when moving files around;\n",
    "                            'model.ckpt-*'      : file(s) with model definition (created by tf)\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we can especify the size of the hidden state, the number of layers, the number of steps used for training and the typical stuff related to SGD, batch size, learning rate and others. Those parameters are used later to build the network inside model.py. The relevant part is shown commented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we chose the type of RNN according the parameters provided. The default value is lstm, yet we will use gru.\n",
    "if args.model == 'rnn':\n",
    "    cell_fn = rnn_cell.BasicRNNCell\n",
    "elif args.model == 'gru':\n",
    "    cell_fn = rnn_cell.GRUCell\n",
    "elif args.model == 'lstm':\n",
    "    cell_fn = rnn_cell.BasicLSTMCell\n",
    "else:\n",
    "    raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "# The cell is initialied and replicated as layers have been determined. This value is 128 x 2 layers in the defaults\n",
    "cell = cell_fn(args.rnn_size)\n",
    "\n",
    "self.cell = cell = rnn_cell.MultiRNNCell([cell] * args.num_layers)\n",
    "\n",
    "# The placeholders of the network are initialized, along with the initial state.\n",
    "self.input_data = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "self.targets = tf.placeholder(tf.int32, [args.batch_size, args.seq_length])\n",
    "self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "# The output softmax weights and biases are built here\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [args.rnn_size, args.vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        # The embedding is created here, mapping the inputs to the state of the rnn\n",
    "        embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "        inputs = tf.split(1, args.seq_length, tf.nn.embedding_lookup(embedding, self.input_data))\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "# The loop function passes does a forward pass of the data through the RNN cells        \n",
    "def loop(prev, _):\n",
    "    prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "    prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "    return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "# We get the final output of the network\n",
    "outputs, last_state = seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if infer else None, scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, args.rnn_size])\n",
    "# We input the output of the cell to the softmax\n",
    "self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "self.probs = tf.nn.softmax(self.logits)\n",
    "# We set the loss\n",
    "loss = seq2seq.sequence_loss_by_example([self.logits],\n",
    "        [tf.reshape(self.targets, [-1])],\n",
    "        [tf.ones([args.batch_size * args.seq_length])],\n",
    "        args.vocab_size)\n",
    "# The loss is averaged\n",
    "self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "self.final_state = last_state\n",
    "# Learning rate variable\n",
    "self.lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "# Gradient clipping\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "        args.grad_clip)\n",
    "# Optimize using Adam\n",
    "optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "self.train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want to mess too much with the hyperparameters, we will take the default ones as good enough, with the exception of using GRU instead of LTSM since the amount of data is not large. First we need the data. We will use this (http://www.norbeck.nu/abc/hn201602.zip), but any abc tune collection is equally good. After concat all the files in one we are ready to train. Further cleaning is possible, and it will improve the resulting generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-13 15:17:41--  http://www.norbeck.nu/abc/hn201602.zip\n",
      "Resolving www.norbeck.nu... 194.9.94.73\n",
      "Connecting to www.norbeck.nu|194.9.94.73|:80... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File 'hn201602.zip' not modified on server. Omitting download.\n",
      "\n",
      "x s/\n",
      "x s/hngang0.abc\n",
      "x s/hnhall0.abc\n",
      "x s/hnjp0.abc\n",
      "x s/hnk1p0.abc\n",
      "x s/hnl1p0.abc\n",
      "x s/hnop0.abc\n",
      "x s/hnsch0.abc\n",
      "x s/hnsk0.abc\n",
      "x s/hnsp0.abc\n",
      "x s/hnvals0.abc\n",
      "x i/\n",
      "x i/hnair0.abc\n",
      "x i/hnbarn0.abc\n",
      "x i/hncar0.abc\n",
      "x i/hnhf0.abc\n",
      "x i/hnhp0.abc\n",
      "x i/hnhp1.abc\n",
      "x i/hnj0.abc\n",
      "x i/hnj1.abc\n",
      "x i/hnj2.abc\n",
      "x i/hnj3.abc\n",
      "x i/hnj4.abc\n",
      "x i/hnmarch0.abc\n",
      "x i/hnmaz0.abc\n",
      "x i/hnp0.abc\n",
      "x i/hnp1.abc\n",
      "x i/hnr0.abc\n",
      "x i/hnr1.abc\n",
      "x i/hnr2.abc\n",
      "x i/hnr3.abc\n",
      "x i/hnr4.abc\n",
      "x i/hnr5.abc\n",
      "x i/hnr6.abc\n",
      "x i/hnr7.abc\n",
      "x i/hnr8.abc\n",
      "x i/hnr9.abc\n",
      "x i/hnset0.abc\n",
      "x i/hnsj0.abc\n",
      "x i/hnsl0.abc\n",
      "x i/hnsong0.abc\n",
      "x i/hnstr0.abc\n",
      "x i/hnwaltz0.abc\n",
      "X:7\n",
      "T:New Land, The\n",
      "R:waltz\n",
      "C:Otis Tomas\n",
      "S:Nicholas Quemener\n",
      "H:Originally in F\n",
      "Z:hn-waltz-7\n",
      "M:3/4\n",
      "K:E\n",
      "B GF|E3F (3GFE|B4 Bc|B3 G (3FGF|E3 F (3GFE|\n",
      "A3 G FG|A c3 e2|c3 A {FG}FE|F3 B (3AGF|\n",
      "G3 E GA|G B3 ef|g3 f eB|c4 ea|\n",
      "g3 f eg|f d3 e2|c6-|c3:|\n",
      "|:B ef|g3 f ga|g2 f2 eB|A3 G AB|A2 B2 c2|\n",
      "f3 e fg|f2 e2 d2|G3 F GA|G2 A2 B2|\n",
      "e3 d ef|ec dB cA|F2 FE FG|F3 f ec|\n",
      "B G3 A2|F d3 e2|c6-|c3:|\n",
      "\n",
      "X:8\n",
      "T:Empty Wallet Waltz\n",
      "R:waltz\n",
      "C:Fredrik Jakobsson\n",
      "Z:id:hn-waltz-8\n",
      "M:3/4\n",
      "K:G\n",
      "G2|B3 A G2|E4 G2|A4 B2|A4 B2|d3 B G2|e3 d B2|A3 BAG|E4 G2|\n",
      "B3 A G2|E4 G2|A4 B2|A4 B2|G3 F E2|E2 F2 D2|E6|1 E4:|2 E3 FGA||\n",
      "Bd d2 B2|Ad d2 A2|GF GB AG|FGFE D2|EG G2 E2|DG G2 AB|c2 cB AG|A3 AGA|\n",
      "Bd d2 B2|Ad d2 A2|GF GB AG|FGFE D2|EG G2 E2|DG G2 AB|cB AG GF|G4||\n",
      "P:my version\n",
      "G2|B3 A G2|E4 G2|A3 GAB|A4 B2|d2 dB G2|e3 d B2|A3 BAG|E4 G2|\n",
      "B3 A G2|E4 G2|A2 AG AB|A4 B2|G2 GF E2|E3 F D2|E6|1 E4:|2 E3 FGA||\n",
      "(3Bcd d2 B2|Ad d2 A2|GF GB AG|FGEF D2|EG G2 E2|DG G2 AB|c2 cB AG|A3 AGA|\n",
      "(3Bcd d2 B2|Ad d2 A2|GF GB AG|FGEF D2|EG G2 E2|DG G2 AB|BA AG GF|G4||\n",
      "\n",
      "X:9\n",
      "T:Trip to Skye\n",
      "R:waltz\n",
      "C:John Whelan\n",
      "D:Skolvan\n",
      "D:John Whelan & Eileen Ivers\n",
      "Z:id:hn-waltz-9\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:D\n",
      "f2 ed cB | ce AB ce | d2 ce dc | d2 B2 B2 |\n",
      "f2 ed cB | ce AB ce | d2 ce dc | BA FE FA :|\n",
      "|: F2 Bc d2 | F2 Bc d2 | ce AB ce | dc BA FE |\n",
      "F2 Bc d2 | F2 Bc d2 | ce AB ce | d2 B2 B2 :|\n",
      "\n",
      "X:10\n",
      "T:Valse des Esquimaux\n",
      "T:Ookpik Waltz\n",
      "R:waltz\n",
      "C:Frankie Rodgers, British Columbia, Canada (1936-2009)\n",
      "H:Danish band La Bastringue have changed this tune around a bit. They play it\n",
      "H:in a set together with waltz#11 \"Nerissa\", and then Marco Pollier made those\n",
      "H:into one four part tune for his recording.\n",
      "D:La Bastringue\n",
      "D:Marco Pollier: Ebony & Brass\n",
      "Z:id:hn-waltz-10\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:G\n",
      "P:La Bastringue's version\n",
      "DGA | B3 BcB | A2 AF ED | E G2 FGA | G2 GD (3GBd |\n",
      "e2 ed ef | e2 ed BA | B d2 ^cde | d3 GBd |\n",
      "e2 ed ef | e2 ed BA | B d3 BA | G E3 ED |\n",
      "C2 CE FE | D F3 ED | E G2 FGA |1 G3 :|2 G3 G (3Bcd ||\n",
      "|: e B3 BA | B3 BAB | d4 BA | B3 BAB |\n",
      "c3 cdc | Bc BA GF | ED (3EFG FD |1 E4 (3GBd :|2 E3 ||\n",
      "P:Original version\n",
      "|: DEF | G3 ABd | A F3 (3DED | E G4 A | G3 ABd |\n",
      "e4 ef | e d3 BA | B d3 e2 | d2 g2 f2 |\n",
      "e4 ef | e d3 (3dBA | B3 d (3BAG | E4 ED |\n",
      "C3 D EG | D F3 D2 | E2 G3 A |1 G3 :|2 G3 ABd ||\n",
      "|: e2 B3 A | B4 e2 | d2 B3 A | B4 B2 |\n",
      "A4 AB | A G3 (3GED |1 E G4 A | G2 B2 d2 :|2 E G4 E | FE D2 (3DEF :|\n",
      "\n",
      "X:11\n",
      "T:Nerissa\n",
      "R:waltz\n",
      "C:Bob McQuillen, New Hampshire, USA (1923-2014)\n",
      "H:Danish band La Bastringue play it in a set together with waltz#10, and then\n",
      "H:Marco Pollier made those into one four part tune for his recording.\n",
      "D:La Bastringue\n",
      "D:Marco Pollier: Ebony & Brass\n",
      "Z:id:hn-waltz-11\n",
      "M:3/4\n",
      "L:1/8\n",
      "K:G\n",
      "|: D | E G3-G D | ED EF GA | B d3-d e | d4 BA |\n",
      "G2 GF GB | d4 BA | B3 c/B/ AG | E4-E D |\n",
      "E G3-G D | ED EF GA | B2 d2 de | d3 G (3Bcd |\n",
      "e2 g2 fe | d4 BA | GB A3 G |1 G4z :|2 G2 B2 d2 ||\n",
      "|: g3 gag | e2 d3 g | e2 d3 e | d3 c (3Bcd |\n",
      "g3 bag | e2 dg dg | ed GA (3Bcd | A2 G2 E2 |\n",
      "D2 E2 G2 | A2 G2 A2 | B2 d3 e | d3 G (3Bcd |\n",
      "e2 g2 fe | d4 BA | GB A3 G |1 G2 B2 d2 :|2 G3 ||\n",
      "\n",
      "input.txt       music.txt       \u001b[34mtinyshakespeare\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!wget -N http://www.norbeck.nu/abc/hn201602.zip\n",
    "!tar xvf hn201602.zip\n",
    "!mv ./s/* ./i\n",
    "!cat ./i/*abc > ./char-rnn-tensorflow/data/input.txt\n",
    "!tail -n 100 ./char-rnn-tensorflow/data/input.txt\n",
    "!ls ./char-rnn-tensorflow/data\n",
    "!rm -R ./s ./i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We are ready to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "0/25250 (epoch 0), train_loss = 4.795, time/batch = 0.801\n",
      "model saved to ./char-rnn-tensorflow/save/model.ckpt\n",
      "1/25250 (epoch 0), train_loss = 4.767, time/batch = 0.337\n",
      "2/25250 (epoch 0), train_loss = 4.674, time/batch = 0.343\n",
      "3/25250 (epoch 0), train_loss = 4.439, time/batch = 0.334\n",
      "4/25250 (epoch 0), train_loss = 4.044, time/batch = 0.349\n",
      "5/25250 (epoch 0), train_loss = 3.935, time/batch = 0.331\n",
      "6/25250 (epoch 0), train_loss = 3.809, time/batch = 0.334\n",
      "7/25250 (epoch 0), train_loss = 3.794, time/batch = 0.339\n",
      "8/25250 (epoch 0), train_loss = 3.814, time/batch = 0.341\n",
      "9/25250 (epoch 0), train_loss = 3.756, time/batch = 0.334\n",
      "10/25250 (epoch 0), train_loss = 3.639, time/batch = 0.330\n",
      "11/25250 (epoch 0), train_loss = 3.687, time/batch = 0.341\n",
      "12/25250 (epoch 0), train_loss = 3.682, time/batch = 0.436\n",
      "13/25250 (epoch 0), train_loss = 3.681, time/batch = 0.337\n",
      "14/25250 (epoch 0), train_loss = 3.653, time/batch = 0.419\n",
      "15/25250 (epoch 0), train_loss = 3.684, time/batch = 0.400\n",
      "16/25250 (epoch 0), train_loss = 3.592, time/batch = 0.422\n",
      "17/25250 (epoch 0), train_loss = 3.628, time/batch = 0.331\n",
      "18/25250 (epoch 0), train_loss = 3.599, time/batch = 0.391\n",
      "19/25250 (epoch 0), train_loss = 3.565, time/batch = 0.364\n",
      "20/25250 (epoch 0), train_loss = 3.561, time/batch = 0.387\n",
      "21/25250 (epoch 0), train_loss = 3.531, time/batch = 0.397\n",
      "22/25250 (epoch 0), train_loss = 3.568, time/batch = 0.353\n",
      "23/25250 (epoch 0), train_loss = 3.580, time/batch = 0.408\n",
      "24/25250 (epoch 0), train_loss = 3.499, time/batch = 0.395\n",
      "25/25250 (epoch 0), train_loss = 3.524, time/batch = 0.380\n",
      "26/25250 (epoch 0), train_loss = 3.474, time/batch = 0.409\n",
      "27/25250 (epoch 0), train_loss = 3.376, time/batch = 0.374\n",
      "28/25250 (epoch 0), train_loss = 3.320, time/batch = 0.334\n",
      "29/25250 (epoch 0), train_loss = 3.333, time/batch = 0.382\n",
      "30/25250 (epoch 0), train_loss = 3.448, time/batch = 0.386\n",
      "31/25250 (epoch 0), train_loss = 3.399, time/batch = 0.390\n",
      "32/25250 (epoch 0), train_loss = 3.351, time/batch = 0.397\n",
      "33/25250 (epoch 0), train_loss = 3.334, time/batch = 0.368\n",
      "34/25250 (epoch 0), train_loss = 3.347, time/batch = 0.325\n",
      "35/25250 (epoch 0), train_loss = 3.330, time/batch = 0.378\n",
      "36/25250 (epoch 0), train_loss = 3.329, time/batch = 0.372\n",
      "37/25250 (epoch 0), train_loss = 3.248, time/batch = 0.378\n",
      "38/25250 (epoch 0), train_loss = 3.263, time/batch = 0.365\n",
      "39/25250 (epoch 0), train_loss = 3.223, time/batch = 0.436\n",
      "40/25250 (epoch 0), train_loss = 3.260, time/batch = 0.350\n",
      "41/25250 (epoch 0), train_loss = 3.192, time/batch = 0.370\n",
      "42/25250 (epoch 0), train_loss = 3.235, time/batch = 0.500\n",
      "43/25250 (epoch 0), train_loss = 3.207, time/batch = 0.418\n",
      "44/25250 (epoch 0), train_loss = 3.199, time/batch = 0.433\n",
      "45/25250 (epoch 0), train_loss = 3.174, time/batch = 0.436\n",
      "46/25250 (epoch 0), train_loss = 3.114, time/batch = 0.383\n",
      "47/25250 (epoch 0), train_loss = 3.100, time/batch = 0.383\n",
      "48/25250 (epoch 0), train_loss = 3.048, time/batch = 0.401\n",
      "49/25250 (epoch 0), train_loss = 3.049, time/batch = 0.414\n",
      "50/25250 (epoch 0), train_loss = 3.041, time/batch = 0.418\n",
      "51/25250 (epoch 0), train_loss = 2.994, time/batch = 0.457\n",
      "52/25250 (epoch 0), train_loss = 2.996, time/batch = 0.525\n",
      "53/25250 (epoch 0), train_loss = 3.005, time/batch = 0.500\n",
      "54/25250 (epoch 0), train_loss = 3.014, time/batch = 0.401\n",
      "55/25250 (epoch 0), train_loss = 2.966, time/batch = 0.392\n",
      "56/25250 (epoch 0), train_loss = 2.949, time/batch = 0.410\n",
      "57/25250 (epoch 0), train_loss = 2.929, time/batch = 0.439\n",
      "58/25250 (epoch 0), train_loss = 2.966, time/batch = 0.376\n",
      "59/25250 (epoch 0), train_loss = 2.861, time/batch = 0.390\n",
      "60/25250 (epoch 0), train_loss = 2.874, time/batch = 0.394\n",
      "61/25250 (epoch 0), train_loss = 2.870, time/batch = 0.392\n",
      "62/25250 (epoch 0), train_loss = 2.871, time/batch = 0.370\n",
      "63/25250 (epoch 0), train_loss = 2.848, time/batch = 0.356\n",
      "64/25250 (epoch 0), train_loss = 2.882, time/batch = 0.376\n",
      "65/25250 (epoch 0), train_loss = 2.893, time/batch = 0.450\n",
      "66/25250 (epoch 0), train_loss = 2.917, time/batch = 0.438\n",
      "67/25250 (epoch 0), train_loss = 2.892, time/batch = 0.398\n",
      "68/25250 (epoch 0), train_loss = 2.847, time/batch = 0.362\n",
      "69/25250 (epoch 0), train_loss = 2.844, time/batch = 0.448\n",
      "70/25250 (epoch 0), train_loss = 2.804, time/batch = 0.407\n",
      "71/25250 (epoch 0), train_loss = 2.791, time/batch = 0.388\n",
      "72/25250 (epoch 0), train_loss = 2.831, time/batch = 0.334\n",
      "73/25250 (epoch 0), train_loss = 2.763, time/batch = 0.424\n",
      "74/25250 (epoch 0), train_loss = 2.741, time/batch = 0.393\n",
      "75/25250 (epoch 0), train_loss = 2.748, time/batch = 0.336\n",
      "76/25250 (epoch 0), train_loss = 2.751, time/batch = 0.477\n",
      "77/25250 (epoch 0), train_loss = 2.739, time/batch = 0.427\n",
      "78/25250 (epoch 0), train_loss = 2.664, time/batch = 0.510\n",
      "79/25250 (epoch 0), train_loss = 2.690, time/batch = 0.423\n",
      "80/25250 (epoch 0), train_loss = 2.743, time/batch = 0.416\n",
      "81/25250 (epoch 0), train_loss = 2.661, time/batch = 0.339\n",
      "82/25250 (epoch 0), train_loss = 2.704, time/batch = 0.390\n",
      "83/25250 (epoch 0), train_loss = 2.624, time/batch = 0.369\n",
      "84/25250 (epoch 0), train_loss = 2.627, time/batch = 0.402\n",
      "85/25250 (epoch 0), train_loss = 2.587, time/batch = 0.404\n",
      "86/25250 (epoch 0), train_loss = 2.604, time/batch = 0.399\n",
      "87/25250 (epoch 0), train_loss = 2.569, time/batch = 0.393\n",
      "88/25250 (epoch 0), train_loss = 2.597, time/batch = 0.409\n",
      "89/25250 (epoch 0), train_loss = 2.600, time/batch = 0.395\n",
      "90/25250 (epoch 0), train_loss = 2.612, time/batch = 0.375\n",
      "91/25250 (epoch 0), train_loss = 2.603, time/batch = 0.389\n",
      "92/25250 (epoch 0), train_loss = 2.557, time/batch = 0.393\n",
      "93/25250 (epoch 0), train_loss = 2.512, time/batch = 0.388\n",
      "94/25250 (epoch 0), train_loss = 2.516, time/batch = 0.393\n",
      "95/25250 (epoch 0), train_loss = 2.506, time/batch = 0.390\n",
      "96/25250 (epoch 0), train_loss = 2.517, time/batch = 0.389\n",
      "97/25250 (epoch 0), train_loss = 2.507, time/batch = 0.366\n",
      "98/25250 (epoch 0), train_loss = 2.573, time/batch = 0.393\n",
      "99/25250 (epoch 0), train_loss = 2.502, time/batch = 0.343\n",
      "100/25250 (epoch 0), train_loss = 2.505, time/batch = 0.399\n",
      "101/25250 (epoch 0), train_loss = 2.506, time/batch = 0.429\n",
      "102/25250 (epoch 0), train_loss = 2.531, time/batch = 0.439\n",
      "103/25250 (epoch 0), train_loss = 2.500, time/batch = 0.393\n",
      "104/25250 (epoch 0), train_loss = 2.417, time/batch = 0.394\n",
      "105/25250 (epoch 0), train_loss = 2.448, time/batch = 0.384\n",
      "106/25250 (epoch 0), train_loss = 2.395, time/batch = 0.387\n",
      "107/25250 (epoch 0), train_loss = 2.409, time/batch = 0.418\n",
      "108/25250 (epoch 0), train_loss = 2.402, time/batch = 0.395\n",
      "109/25250 (epoch 0), train_loss = 2.387, time/batch = 0.324\n",
      "110/25250 (epoch 0), train_loss = 2.324, time/batch = 0.404\n",
      "111/25250 (epoch 0), train_loss = 2.385, time/batch = 0.331\n",
      "112/25250 (epoch 0), train_loss = 2.497, time/batch = 0.478\n",
      "113/25250 (epoch 0), train_loss = 2.424, time/batch = 0.493\n",
      "114/25250 (epoch 0), train_loss = 2.360, time/batch = 0.384\n",
      "115/25250 (epoch 0), train_loss = 2.275, time/batch = 0.440\n",
      "116/25250 (epoch 0), train_loss = 2.313, time/batch = 0.410\n",
      "117/25250 (epoch 0), train_loss = 2.249, time/batch = 0.349\n",
      "118/25250 (epoch 0), train_loss = 2.265, time/batch = 0.418\n",
      "119/25250 (epoch 0), train_loss = 2.255, time/batch = 0.399\n",
      "120/25250 (epoch 0), train_loss = 2.329, time/batch = 0.383\n",
      "121/25250 (epoch 0), train_loss = 2.274, time/batch = 0.401\n",
      "122/25250 (epoch 0), train_loss = 2.280, time/batch = 0.419\n",
      "123/25250 (epoch 0), train_loss = 2.265, time/batch = 0.342\n",
      "124/25250 (epoch 0), train_loss = 2.259, time/batch = 0.373\n",
      "125/25250 (epoch 0), train_loss = 2.162, time/batch = 0.387\n",
      "126/25250 (epoch 0), train_loss = 2.137, time/batch = 0.421\n",
      "127/25250 (epoch 0), train_loss = 2.227, time/batch = 0.419\n",
      "128/25250 (epoch 0), train_loss = 2.212, time/batch = 0.449\n",
      "129/25250 (epoch 0), train_loss = 2.227, time/batch = 0.427\n",
      "130/25250 (epoch 0), train_loss = 2.226, time/batch = 0.391\n",
      "131/25250 (epoch 0), train_loss = 2.205, time/batch = 0.393\n",
      "132/25250 (epoch 0), train_loss = 2.197, time/batch = 0.399\n",
      "133/25250 (epoch 0), train_loss = 2.168, time/batch = 0.379\n",
      "134/25250 (epoch 0), train_loss = 2.148, time/batch = 0.389\n",
      "135/25250 (epoch 0), train_loss = 2.145, time/batch = 0.388\n",
      "136/25250 (epoch 0), train_loss = 2.095, time/batch = 0.408\n",
      "137/25250 (epoch 0), train_loss = 2.149, time/batch = 0.413\n",
      "138/25250 (epoch 0), train_loss = 2.195, time/batch = 0.419\n",
      "139/25250 (epoch 0), train_loss = 2.116, time/batch = 0.328\n",
      "140/25250 (epoch 0), train_loss = 2.188, time/batch = 0.427\n",
      "141/25250 (epoch 0), train_loss = 2.149, time/batch = 0.345\n",
      "142/25250 (epoch 0), train_loss = 2.125, time/batch = 0.391\n",
      "143/25250 (epoch 0), train_loss = 2.153, time/batch = 0.376\n",
      "144/25250 (epoch 0), train_loss = 2.118, time/batch = 0.488\n",
      "145/25250 (epoch 0), train_loss = 2.096, time/batch = 0.432\n",
      "146/25250 (epoch 0), train_loss = 2.094, time/batch = 0.434\n",
      "147/25250 (epoch 0), train_loss = 2.146, time/batch = 0.401\n",
      "148/25250 (epoch 0), train_loss = 2.095, time/batch = 0.364\n",
      "149/25250 (epoch 0), train_loss = 2.172, time/batch = 0.484\n",
      "150/25250 (epoch 0), train_loss = 2.077, time/batch = 0.443\n",
      "151/25250 (epoch 0), train_loss = 2.078, time/batch = 0.441\n",
      "152/25250 (epoch 0), train_loss = 2.144, time/batch = 0.561\n",
      "153/25250 (epoch 0), train_loss = 2.124, time/batch = 0.446\n",
      "154/25250 (epoch 0), train_loss = 2.048, time/batch = 0.481\n",
      "155/25250 (epoch 0), train_loss = 2.183, time/batch = 0.653\n",
      "156/25250 (epoch 0), train_loss = 2.104, time/batch = 0.524\n",
      "157/25250 (epoch 0), train_loss = 1.987, time/batch = 0.474\n",
      "158/25250 (epoch 0), train_loss = 2.038, time/batch = 0.416\n",
      "159/25250 (epoch 0), train_loss = 2.033, time/batch = 0.496\n",
      "160/25250 (epoch 0), train_loss = 2.119, time/batch = 0.417\n",
      "161/25250 (epoch 0), train_loss = 1.978, time/batch = 0.433\n",
      "162/25250 (epoch 0), train_loss = 2.092, time/batch = 0.425\n",
      "163/25250 (epoch 0), train_loss = 2.041, time/batch = 0.451\n",
      "164/25250 (epoch 0), train_loss = 2.017, time/batch = 0.399\n",
      "165/25250 (epoch 0), train_loss = 1.997, time/batch = 0.398\n",
      "166/25250 (epoch 0), train_loss = 2.110, time/batch = 0.377\n",
      "167/25250 (epoch 0), train_loss = 2.132, time/batch = 0.423\n",
      "168/25250 (epoch 0), train_loss = 2.012, time/batch = 0.433\n",
      "169/25250 (epoch 0), train_loss = 2.069, time/batch = 0.419\n",
      "170/25250 (epoch 0), train_loss = 2.010, time/batch = 0.381\n",
      "171/25250 (epoch 0), train_loss = 2.013, time/batch = 0.389\n",
      "172/25250 (epoch 0), train_loss = 1.954, time/batch = 0.407\n",
      "173/25250 (epoch 0), train_loss = 1.989, time/batch = 0.430\n",
      "174/25250 (epoch 0), train_loss = 1.995, time/batch = 0.447\n",
      "175/25250 (epoch 0), train_loss = 1.944, time/batch = 0.357\n",
      "176/25250 (epoch 0), train_loss = 1.986, time/batch = 0.386\n",
      "177/25250 (epoch 0), train_loss = 2.033, time/batch = 0.368\n",
      "178/25250 (epoch 0), train_loss = 1.974, time/batch = 0.396\n",
      "179/25250 (epoch 0), train_loss = 2.001, time/batch = 0.352\n",
      "180/25250 (epoch 0), train_loss = 1.824, time/batch = 0.385\n",
      "181/25250 (epoch 0), train_loss = 1.889, time/batch = 0.376\n",
      "182/25250 (epoch 0), train_loss = 1.900, time/batch = 0.385\n",
      "183/25250 (epoch 0), train_loss = 1.848, time/batch = 0.500\n",
      "184/25250 (epoch 0), train_loss = 1.907, time/batch = 0.445\n",
      "185/25250 (epoch 0), train_loss = 2.036, time/batch = 0.381\n",
      "186/25250 (epoch 0), train_loss = 1.904, time/batch = 0.374\n",
      "187/25250 (epoch 0), train_loss = 1.921, time/batch = 0.404\n",
      "188/25250 (epoch 0), train_loss = 1.934, time/batch = 0.508\n",
      "189/25250 (epoch 0), train_loss = 1.873, time/batch = 0.572\n",
      "190/25250 (epoch 0), train_loss = 2.004, time/batch = 0.448\n",
      "191/25250 (epoch 0), train_loss = 2.069, time/batch = 0.498\n",
      "192/25250 (epoch 0), train_loss = 1.953, time/batch = 0.481\n",
      "193/25250 (epoch 0), train_loss = 2.071, time/batch = 0.405\n",
      "194/25250 (epoch 0), train_loss = 1.981, time/batch = 0.390\n",
      "195/25250 (epoch 0), train_loss = 1.959, time/batch = 0.380\n",
      "196/25250 (epoch 0), train_loss = 1.861, time/batch = 0.433\n",
      "197/25250 (epoch 0), train_loss = 1.850, time/batch = 0.483\n",
      "198/25250 (epoch 0), train_loss = 1.869, time/batch = 0.418\n",
      "199/25250 (epoch 0), train_loss = 1.878, time/batch = 0.412\n",
      "200/25250 (epoch 0), train_loss = 1.894, time/batch = 0.391\n",
      "201/25250 (epoch 0), train_loss = 1.836, time/batch = 0.368\n",
      "202/25250 (epoch 0), train_loss = 1.865, time/batch = 0.374\n",
      "203/25250 (epoch 0), train_loss = 1.877, time/batch = 0.488\n",
      "204/25250 (epoch 0), train_loss = 1.873, time/batch = 0.400\n",
      "205/25250 (epoch 0), train_loss = 1.897, time/batch = 0.394\n",
      "206/25250 (epoch 0), train_loss = 1.891, time/batch = 0.377\n",
      "207/25250 (epoch 0), train_loss = 1.887, time/batch = 0.410\n",
      "208/25250 (epoch 0), train_loss = 1.838, time/batch = 0.396\n",
      "209/25250 (epoch 0), train_loss = 1.861, time/batch = 0.377\n",
      "210/25250 (epoch 0), train_loss = 1.902, time/batch = 0.391\n",
      "211/25250 (epoch 0), train_loss = 1.906, time/batch = 0.382\n",
      "212/25250 (epoch 0), train_loss = 1.827, time/batch = 0.424\n",
      "213/25250 (epoch 0), train_loss = 1.822, time/batch = 0.418\n",
      "214/25250 (epoch 0), train_loss = 1.802, time/batch = 0.404\n",
      "215/25250 (epoch 0), train_loss = 1.846, time/batch = 0.399\n",
      "216/25250 (epoch 0), train_loss = 1.835, time/batch = 0.409\n",
      "217/25250 (epoch 0), train_loss = 1.947, time/batch = 0.399\n",
      "218/25250 (epoch 0), train_loss = 1.964, time/batch = 0.419\n",
      "219/25250 (epoch 0), train_loss = 1.958, time/batch = 0.365\n",
      "220/25250 (epoch 0), train_loss = 1.830, time/batch = 0.386\n",
      "221/25250 (epoch 0), train_loss = 1.859, time/batch = 0.507\n",
      "222/25250 (epoch 0), train_loss = 1.875, time/batch = 0.427\n",
      "223/25250 (epoch 0), train_loss = 1.982, time/batch = 0.390\n",
      "224/25250 (epoch 0), train_loss = 1.831, time/batch = 0.418\n",
      "225/25250 (epoch 0), train_loss = 1.774, time/batch = 0.373\n",
      "226/25250 (epoch 0), train_loss = 1.855, time/batch = 0.418\n",
      "227/25250 (epoch 0), train_loss = 1.888, time/batch = 0.388\n",
      "228/25250 (epoch 0), train_loss = 1.760, time/batch = 0.459\n",
      "229/25250 (epoch 0), train_loss = 1.876, time/batch = 0.375\n",
      "230/25250 (epoch 0), train_loss = 1.865, time/batch = 0.399\n",
      "231/25250 (epoch 0), train_loss = 1.893, time/batch = 0.377\n",
      "232/25250 (epoch 0), train_loss = 1.893, time/batch = 0.375\n",
      "233/25250 (epoch 0), train_loss = 1.881, time/batch = 0.369\n",
      "234/25250 (epoch 0), train_loss = 1.807, time/batch = 0.368\n",
      "235/25250 (epoch 0), train_loss = 1.795, time/batch = 0.381\n",
      "236/25250 (epoch 0), train_loss = 1.804, time/batch = 0.390\n",
      "237/25250 (epoch 0), train_loss = 1.771, time/batch = 0.388\n",
      "238/25250 (epoch 0), train_loss = 1.774, time/batch = 0.383\n",
      "239/25250 (epoch 0), train_loss = 1.955, time/batch = 0.394\n",
      "240/25250 (epoch 0), train_loss = 1.818, time/batch = 0.390\n",
      "241/25250 (epoch 0), train_loss = 1.815, time/batch = 0.393\n",
      "242/25250 (epoch 0), train_loss = 1.801, time/batch = 0.352\n",
      "243/25250 (epoch 0), train_loss = 1.761, time/batch = 0.378\n",
      "244/25250 (epoch 0), train_loss = 1.732, time/batch = 0.414\n",
      "245/25250 (epoch 0), train_loss = 1.817, time/batch = 0.426\n",
      "246/25250 (epoch 0), train_loss = 1.730, time/batch = 0.422\n",
      "247/25250 (epoch 0), train_loss = 1.820, time/batch = 0.411\n",
      "248/25250 (epoch 0), train_loss = 1.810, time/batch = 0.388\n",
      "249/25250 (epoch 0), train_loss = 1.800, time/batch = 0.382\n",
      "250/25250 (epoch 0), train_loss = 1.784, time/batch = 0.378\n",
      "251/25250 (epoch 0), train_loss = 1.765, time/batch = 0.377\n",
      "252/25250 (epoch 0), train_loss = 1.772, time/batch = 0.359\n",
      "253/25250 (epoch 0), train_loss = 1.807, time/batch = 0.475\n",
      "254/25250 (epoch 0), train_loss = 1.787, time/batch = 0.441\n",
      "255/25250 (epoch 0), train_loss = 1.755, time/batch = 0.449\n",
      "256/25250 (epoch 0), train_loss = 1.762, time/batch = 0.507\n",
      "257/25250 (epoch 0), train_loss = 1.716, time/batch = 0.492\n",
      "258/25250 (epoch 0), train_loss = 1.728, time/batch = 0.492\n",
      "259/25250 (epoch 0), train_loss = 1.683, time/batch = 0.410\n",
      "260/25250 (epoch 0), train_loss = 1.718, time/batch = 0.434\n",
      "261/25250 (epoch 0), train_loss = 1.667, time/batch = 0.411\n",
      "262/25250 (epoch 0), train_loss = 1.671, time/batch = 0.399\n",
      "263/25250 (epoch 0), train_loss = 1.722, time/batch = 0.414\n",
      "264/25250 (epoch 0), train_loss = 1.674, time/batch = 0.389\n",
      "265/25250 (epoch 0), train_loss = 1.686, time/batch = 0.393\n",
      "266/25250 (epoch 0), train_loss = 1.718, time/batch = 0.383\n",
      "267/25250 (epoch 0), train_loss = 1.776, time/batch = 0.378\n",
      "268/25250 (epoch 0), train_loss = 1.775, time/batch = 0.379\n",
      "269/25250 (epoch 0), train_loss = 1.731, time/batch = 0.416\n",
      "270/25250 (epoch 0), train_loss = 1.666, time/batch = 0.446\n",
      "271/25250 (epoch 0), train_loss = 1.759, time/batch = 0.461\n",
      "272/25250 (epoch 0), train_loss = 1.732, time/batch = 0.523\n",
      "273/25250 (epoch 0), train_loss = 1.696, time/batch = 0.468\n",
      "274/25250 (epoch 0), train_loss = 1.644, time/batch = 0.499\n",
      "275/25250 (epoch 0), train_loss = 1.710, time/batch = 0.486\n",
      "276/25250 (epoch 0), train_loss = 1.779, time/batch = 0.451\n",
      "277/25250 (epoch 0), train_loss = 1.697, time/batch = 0.403\n",
      "278/25250 (epoch 0), train_loss = 1.696, time/batch = 0.507\n",
      "279/25250 (epoch 0), train_loss = 1.785, time/batch = 0.551\n",
      "280/25250 (epoch 0), train_loss = 1.785, time/batch = 0.468\n",
      "281/25250 (epoch 0), train_loss = 1.853, time/batch = 0.416\n",
      "282/25250 (epoch 0), train_loss = 1.691, time/batch = 0.397\n",
      "283/25250 (epoch 0), train_loss = 1.706, time/batch = 0.377\n",
      "284/25250 (epoch 0), train_loss = 1.703, time/batch = 0.352\n",
      "285/25250 (epoch 0), train_loss = 1.746, time/batch = 0.356\n",
      "286/25250 (epoch 0), train_loss = 1.666, time/batch = 0.382\n",
      "287/25250 (epoch 0), train_loss = 1.684, time/batch = 0.359\n",
      "288/25250 (epoch 0), train_loss = 1.678, time/batch = 0.372\n",
      "289/25250 (epoch 0), train_loss = 1.660, time/batch = 0.425\n",
      "290/25250 (epoch 0), train_loss = 1.745, time/batch = 0.354\n",
      "291/25250 (epoch 0), train_loss = 1.747, time/batch = 0.361\n",
      "292/25250 (epoch 0), train_loss = 1.747, time/batch = 0.371\n",
      "293/25250 (epoch 0), train_loss = 1.800, time/batch = 0.350\n",
      "294/25250 (epoch 0), train_loss = 1.764, time/batch = 0.503\n",
      "295/25250 (epoch 0), train_loss = 1.792, time/batch = 0.362\n",
      "296/25250 (epoch 0), train_loss = 1.742, time/batch = 0.377\n",
      "297/25250 (epoch 0), train_loss = 1.664, time/batch = 0.370\n",
      "298/25250 (epoch 0), train_loss = 1.718, time/batch = 0.360\n",
      "299/25250 (epoch 0), train_loss = 1.743, time/batch = 0.374\n",
      "300/25250 (epoch 0), train_loss = 1.753, time/batch = 0.386\n",
      "301/25250 (epoch 0), train_loss = 1.681, time/batch = 0.424\n",
      "302/25250 (epoch 0), train_loss = 1.551, time/batch = 0.394\n",
      "303/25250 (epoch 0), train_loss = 1.638, time/batch = 0.360\n",
      "304/25250 (epoch 0), train_loss = 1.623, time/batch = 0.337\n",
      "305/25250 (epoch 0), train_loss = 1.719, time/batch = 0.357\n",
      "306/25250 (epoch 0), train_loss = 1.661, time/batch = 0.360\n",
      "307/25250 (epoch 0), train_loss = 1.701, time/batch = 0.375\n",
      "308/25250 (epoch 0), train_loss = 1.681, time/batch = 0.383\n",
      "309/25250 (epoch 0), train_loss = 1.637, time/batch = 0.419\n",
      "310/25250 (epoch 0), train_loss = 1.655, time/batch = 0.369\n",
      "311/25250 (epoch 0), train_loss = 1.669, time/batch = 0.351\n",
      "312/25250 (epoch 0), train_loss = 1.625, time/batch = 0.366\n",
      "313/25250 (epoch 0), train_loss = 1.625, time/batch = 0.401\n",
      "314/25250 (epoch 0), train_loss = 1.628, time/batch = 0.354\n",
      "315/25250 (epoch 0), train_loss = 1.595, time/batch = 0.339\n",
      "316/25250 (epoch 0), train_loss = 1.663, time/batch = 0.368\n",
      "317/25250 (epoch 0), train_loss = 1.623, time/batch = 0.397\n",
      "318/25250 (epoch 0), train_loss = 1.668, time/batch = 0.385\n",
      "319/25250 (epoch 0), train_loss = 1.674, time/batch = 0.512\n",
      "320/25250 (epoch 0), train_loss = 1.679, time/batch = 0.475\n",
      "321/25250 (epoch 0), train_loss = 1.602, time/batch = 0.410\n",
      "322/25250 (epoch 0), train_loss = 1.638, time/batch = 0.422\n",
      "323/25250 (epoch 0), train_loss = 1.602, time/batch = 0.414\n",
      "324/25250 (epoch 0), train_loss = 1.670, time/batch = 0.392\n",
      "325/25250 (epoch 0), train_loss = 1.638, time/batch = 0.454\n",
      "326/25250 (epoch 0), train_loss = 1.689, time/batch = 0.476\n",
      "327/25250 (epoch 0), train_loss = 1.687, time/batch = 0.404\n",
      "328/25250 (epoch 0), train_loss = 1.741, time/batch = 0.421\n",
      "329/25250 (epoch 0), train_loss = 1.689, time/batch = 0.409\n",
      "330/25250 (epoch 0), train_loss = 1.657, time/batch = 0.347\n",
      "331/25250 (epoch 0), train_loss = 1.623, time/batch = 0.358\n",
      "332/25250 (epoch 0), train_loss = 1.651, time/batch = 0.358\n",
      "333/25250 (epoch 0), train_loss = 1.704, time/batch = 0.362\n",
      "334/25250 (epoch 0), train_loss = 1.629, time/batch = 0.419\n",
      "335/25250 (epoch 0), train_loss = 1.568, time/batch = 0.389\n",
      "336/25250 (epoch 0), train_loss = 1.615, time/batch = 0.399\n",
      "337/25250 (epoch 0), train_loss = 1.707, time/batch = 0.382\n",
      "338/25250 (epoch 0), train_loss = 1.679, time/batch = 0.364\n",
      "339/25250 (epoch 0), train_loss = 1.640, time/batch = 0.420\n",
      "340/25250 (epoch 0), train_loss = 1.702, time/batch = 0.359\n",
      "341/25250 (epoch 0), train_loss = 1.659, time/batch = 0.357\n",
      "342/25250 (epoch 0), train_loss = 1.640, time/batch = 0.371\n",
      "343/25250 (epoch 0), train_loss = 1.732, time/batch = 0.379\n",
      "344/25250 (epoch 0), train_loss = 1.605, time/batch = 0.490\n",
      "345/25250 (epoch 0), train_loss = 1.685, time/batch = 0.422\n",
      "346/25250 (epoch 0), train_loss = 1.672, time/batch = 0.463\n",
      "347/25250 (epoch 0), train_loss = 1.673, time/batch = 0.397\n",
      "348/25250 (epoch 0), train_loss = 1.676, time/batch = 0.407\n",
      "349/25250 (epoch 0), train_loss = 1.715, time/batch = 0.393\n",
      "350/25250 (epoch 0), train_loss = 1.654, time/batch = 0.449\n",
      "351/25250 (epoch 0), train_loss = 1.543, time/batch = 0.384\n",
      "352/25250 (epoch 0), train_loss = 1.563, time/batch = 0.372\n",
      "353/25250 (epoch 0), train_loss = 1.642, time/batch = 0.389\n",
      "354/25250 (epoch 0), train_loss = 1.622, time/batch = 0.361\n",
      "355/25250 (epoch 0), train_loss = 1.660, time/batch = 0.367\n",
      "356/25250 (epoch 0), train_loss = 1.549, time/batch = 0.435\n",
      "357/25250 (epoch 0), train_loss = 1.660, time/batch = 0.413\n",
      "358/25250 (epoch 0), train_loss = 1.640, time/batch = 0.389\n",
      "359/25250 (epoch 0), train_loss = 1.628, time/batch = 0.371\n",
      "360/25250 (epoch 0), train_loss = 1.626, time/batch = 0.438\n",
      "361/25250 (epoch 0), train_loss = 1.635, time/batch = 0.419\n",
      "362/25250 (epoch 0), train_loss = 1.609, time/batch = 0.389\n",
      "363/25250 (epoch 0), train_loss = 1.645, time/batch = 0.359\n",
      "364/25250 (epoch 0), train_loss = 1.659, time/batch = 0.435\n",
      "365/25250 (epoch 0), train_loss = 1.631, time/batch = 0.472\n",
      "366/25250 (epoch 0), train_loss = 1.615, time/batch = 0.393\n",
      "367/25250 (epoch 0), train_loss = 1.658, time/batch = 0.384\n",
      "368/25250 (epoch 0), train_loss = 1.605, time/batch = 0.518\n",
      "369/25250 (epoch 0), train_loss = 1.614, time/batch = 0.449\n",
      "370/25250 (epoch 0), train_loss = 1.591, time/batch = 0.384\n",
      "371/25250 (epoch 0), train_loss = 1.540, time/batch = 0.398\n",
      "372/25250 (epoch 0), train_loss = 1.566, time/batch = 0.413\n",
      "373/25250 (epoch 0), train_loss = 1.607, time/batch = 0.404\n",
      "374/25250 (epoch 0), train_loss = 1.539, time/batch = 0.394\n",
      "375/25250 (epoch 0), train_loss = 1.687, time/batch = 0.374\n",
      "376/25250 (epoch 0), train_loss = 1.609, time/batch = 0.411\n",
      "377/25250 (epoch 0), train_loss = 1.545, time/batch = 0.384\n",
      "378/25250 (epoch 0), train_loss = 1.543, time/batch = 0.363\n",
      "379/25250 (epoch 0), train_loss = 1.547, time/batch = 0.359\n",
      "380/25250 (epoch 0), train_loss = 1.516, time/batch = 0.381\n",
      "381/25250 (epoch 0), train_loss = 1.538, time/batch = 0.400\n",
      "382/25250 (epoch 0), train_loss = 1.522, time/batch = 0.354\n",
      "383/25250 (epoch 0), train_loss = 1.546, time/batch = 0.363\n",
      "384/25250 (epoch 0), train_loss = 1.485, time/batch = 0.369\n",
      "385/25250 (epoch 0), train_loss = 1.542, time/batch = 0.388\n",
      "386/25250 (epoch 0), train_loss = 1.581, time/batch = 0.379\n",
      "387/25250 (epoch 0), train_loss = 1.596, time/batch = 0.372\n",
      "388/25250 (epoch 0), train_loss = 1.673, time/batch = 0.362\n",
      "389/25250 (epoch 0), train_loss = 1.634, time/batch = 0.381\n",
      "390/25250 (epoch 0), train_loss = 1.562, time/batch = 0.354\n",
      "391/25250 (epoch 0), train_loss = 1.653, time/batch = 0.400\n",
      "392/25250 (epoch 0), train_loss = 1.503, time/batch = 0.380\n",
      "393/25250 (epoch 0), train_loss = 1.506, time/batch = 0.371\n",
      "394/25250 (epoch 0), train_loss = 1.638, time/batch = 0.360\n",
      "395/25250 (epoch 0), train_loss = 1.630, time/batch = 0.405\n",
      "396/25250 (epoch 0), train_loss = 1.618, time/batch = 0.420\n",
      "397/25250 (epoch 0), train_loss = 1.584, time/batch = 0.546\n",
      "398/25250 (epoch 0), train_loss = 1.578, time/batch = 0.500\n",
      "399/25250 (epoch 0), train_loss = 1.649, time/batch = 0.490\n",
      "400/25250 (epoch 0), train_loss = 1.600, time/batch = 0.484\n",
      "401/25250 (epoch 0), train_loss = 1.567, time/batch = 0.432\n",
      "402/25250 (epoch 0), train_loss = 1.585, time/batch = 0.470\n",
      "403/25250 (epoch 0), train_loss = 1.647, time/batch = 0.434\n",
      "404/25250 (epoch 0), train_loss = 1.671, time/batch = 0.448\n",
      "405/25250 (epoch 0), train_loss = 1.703, time/batch = 0.439\n",
      "406/25250 (epoch 0), train_loss = 1.584, time/batch = 0.380\n",
      "407/25250 (epoch 0), train_loss = 1.585, time/batch = 0.391\n",
      "408/25250 (epoch 0), train_loss = 1.590, time/batch = 0.355\n",
      "409/25250 (epoch 0), train_loss = 1.535, time/batch = 0.360\n",
      "410/25250 (epoch 0), train_loss = 1.665, time/batch = 0.360\n",
      "411/25250 (epoch 0), train_loss = 1.587, time/batch = 0.360\n",
      "412/25250 (epoch 0), train_loss = 1.556, time/batch = 0.367\n",
      "413/25250 (epoch 0), train_loss = 1.573, time/batch = 0.376\n",
      "414/25250 (epoch 0), train_loss = 1.595, time/batch = 0.365\n",
      "415/25250 (epoch 0), train_loss = 1.474, time/batch = 0.366\n",
      "416/25250 (epoch 0), train_loss = 1.492, time/batch = 0.374\n",
      "417/25250 (epoch 0), train_loss = 1.565, time/batch = 0.488\n",
      "418/25250 (epoch 0), train_loss = 1.488, time/batch = 0.461\n",
      "419/25250 (epoch 0), train_loss = 1.599, time/batch = 0.421\n",
      "420/25250 (epoch 0), train_loss = 1.519, time/batch = 0.435\n",
      "421/25250 (epoch 0), train_loss = 1.571, time/batch = 0.444\n",
      "422/25250 (epoch 0), train_loss = 1.517, time/batch = 0.402\n",
      "423/25250 (epoch 0), train_loss = 1.605, time/batch = 0.401\n",
      "424/25250 (epoch 0), train_loss = 1.532, time/batch = 0.391\n",
      "425/25250 (epoch 0), train_loss = 1.534, time/batch = 0.420\n",
      "426/25250 (epoch 0), train_loss = 1.579, time/batch = 0.485\n",
      "427/25250 (epoch 0), train_loss = 1.618, time/batch = 0.407\n",
      "428/25250 (epoch 0), train_loss = 1.603, time/batch = 0.382\n",
      "429/25250 (epoch 0), train_loss = 1.545, time/batch = 0.471\n",
      "430/25250 (epoch 0), train_loss = 1.590, time/batch = 0.472\n",
      "431/25250 (epoch 0), train_loss = 1.517, time/batch = 0.397\n",
      "432/25250 (epoch 0), train_loss = 1.555, time/batch = 0.383\n",
      "433/25250 (epoch 0), train_loss = 1.573, time/batch = 0.385\n",
      "434/25250 (epoch 0), train_loss = 1.586, time/batch = 0.363\n",
      "435/25250 (epoch 0), train_loss = 1.532, time/batch = 0.366\n",
      "436/25250 (epoch 0), train_loss = 1.528, time/batch = 0.383\n",
      "437/25250 (epoch 0), train_loss = 1.593, time/batch = 0.378\n",
      "438/25250 (epoch 0), train_loss = 1.562, time/batch = 0.383\n",
      "439/25250 (epoch 0), train_loss = 1.617, time/batch = 0.403\n",
      "440/25250 (epoch 0), train_loss = 1.554, time/batch = 0.415\n",
      "441/25250 (epoch 0), train_loss = 1.508, time/batch = 0.385\n",
      "442/25250 (epoch 0), train_loss = 1.554, time/batch = 0.366\n",
      "443/25250 (epoch 0), train_loss = 1.585, time/batch = 0.365\n",
      "444/25250 (epoch 0), train_loss = 1.606, time/batch = 0.370\n",
      "445/25250 (epoch 0), train_loss = 1.486, time/batch = 0.369\n",
      "446/25250 (epoch 0), train_loss = 1.542, time/batch = 0.395\n",
      "447/25250 (epoch 0), train_loss = 1.491, time/batch = 0.376\n",
      "448/25250 (epoch 0), train_loss = 1.546, time/batch = 0.363\n",
      "449/25250 (epoch 0), train_loss = 1.625, time/batch = 0.387\n",
      "450/25250 (epoch 0), train_loss = 1.594, time/batch = 0.369\n",
      "451/25250 (epoch 0), train_loss = 1.648, time/batch = 0.381\n",
      "452/25250 (epoch 0), train_loss = 1.535, time/batch = 0.393\n",
      "453/25250 (epoch 0), train_loss = 1.546, time/batch = 0.413\n",
      "454/25250 (epoch 0), train_loss = 1.642, time/batch = 0.389\n",
      "455/25250 (epoch 0), train_loss = 1.573, time/batch = 0.403\n",
      "456/25250 (epoch 0), train_loss = 1.595, time/batch = 0.395\n",
      "457/25250 (epoch 0), train_loss = 1.500, time/batch = 0.436\n",
      "458/25250 (epoch 0), train_loss = 1.494, time/batch = 0.406\n",
      "459/25250 (epoch 0), train_loss = 1.450, time/batch = 0.382\n",
      "460/25250 (epoch 0), train_loss = 1.527, time/batch = 0.375\n",
      "461/25250 (epoch 0), train_loss = 1.514, time/batch = 0.460\n",
      "462/25250 (epoch 0), train_loss = 1.578, time/batch = 0.435\n",
      "463/25250 (epoch 0), train_loss = 1.526, time/batch = 0.370\n",
      "464/25250 (epoch 0), train_loss = 1.571, time/batch = 0.347\n",
      "465/25250 (epoch 0), train_loss = 1.516, time/batch = 0.440\n",
      "466/25250 (epoch 0), train_loss = 1.634, time/batch = 0.400\n",
      "467/25250 (epoch 0), train_loss = 1.648, time/batch = 0.393\n",
      "468/25250 (epoch 0), train_loss = 1.589, time/batch = 0.396\n",
      "469/25250 (epoch 0), train_loss = 1.549, time/batch = 0.414\n",
      "470/25250 (epoch 0), train_loss = 1.580, time/batch = 0.539\n",
      "471/25250 (epoch 0), train_loss = 1.522, time/batch = 0.474\n",
      "472/25250 (epoch 0), train_loss = 1.556, time/batch = 0.378\n",
      "473/25250 (epoch 0), train_loss = 1.550, time/batch = 0.410\n",
      "474/25250 (epoch 0), train_loss = 1.483, time/batch = 0.435\n",
      "475/25250 (epoch 0), train_loss = 1.601, time/batch = 0.369\n",
      "476/25250 (epoch 0), train_loss = 1.553, time/batch = 0.386\n",
      "477/25250 (epoch 0), train_loss = 1.560, time/batch = 0.369\n",
      "478/25250 (epoch 0), train_loss = 1.554, time/batch = 0.358\n",
      "479/25250 (epoch 0), train_loss = 1.525, time/batch = 0.374\n",
      "480/25250 (epoch 0), train_loss = 1.496, time/batch = 0.509\n",
      "481/25250 (epoch 0), train_loss = 1.496, time/batch = 0.396\n",
      "482/25250 (epoch 0), train_loss = 1.510, time/batch = 0.402\n",
      "483/25250 (epoch 0), train_loss = 1.470, time/batch = 0.435\n",
      "484/25250 (epoch 0), train_loss = 1.539, time/batch = 0.549\n",
      "485/25250 (epoch 0), train_loss = 1.483, time/batch = 0.492\n",
      "486/25250 (epoch 0), train_loss = 1.522, time/batch = 0.389\n",
      "487/25250 (epoch 0), train_loss = 1.530, time/batch = 0.411\n",
      "488/25250 (epoch 0), train_loss = 1.443, time/batch = 0.442\n",
      "489/25250 (epoch 0), train_loss = 1.523, time/batch = 0.386\n",
      "490/25250 (epoch 0), train_loss = 1.549, time/batch = 0.426\n",
      "491/25250 (epoch 0), train_loss = 1.492, time/batch = 0.418\n",
      "492/25250 (epoch 0), train_loss = 1.526, time/batch = 0.417\n",
      "493/25250 (epoch 0), train_loss = 1.571, time/batch = 0.453\n",
      "494/25250 (epoch 0), train_loss = 1.559, time/batch = 0.457\n",
      "495/25250 (epoch 0), train_loss = 1.507, time/batch = 0.462\n",
      "496/25250 (epoch 0), train_loss = 1.476, time/batch = 0.409\n",
      "497/25250 (epoch 0), train_loss = 1.505, time/batch = 0.431\n",
      "498/25250 (epoch 0), train_loss = 1.453, time/batch = 0.464\n",
      "499/25250 (epoch 0), train_loss = 1.453, time/batch = 0.377\n",
      "500/25250 (epoch 0), train_loss = 1.480, time/batch = 0.386\n",
      "501/25250 (epoch 0), train_loss = 1.504, time/batch = 0.402\n",
      "502/25250 (epoch 0), train_loss = 1.483, time/batch = 0.385\n",
      "503/25250 (epoch 0), train_loss = 1.504, time/batch = 0.407\n",
      "504/25250 (epoch 0), train_loss = 1.562, time/batch = 0.383\n",
      "505/25250 (epoch 1), train_loss = 1.840, time/batch = 0.419\n",
      "506/25250 (epoch 1), train_loss = 1.526, time/batch = 0.398\n",
      "507/25250 (epoch 1), train_loss = 1.562, time/batch = 0.386\n",
      "508/25250 (epoch 1), train_loss = 1.471, time/batch = 0.418\n",
      "509/25250 (epoch 1), train_loss = 1.546, time/batch = 0.394\n",
      "510/25250 (epoch 1), train_loss = 1.498, time/batch = 0.398\n",
      "511/25250 (epoch 1), train_loss = 1.522, time/batch = 0.394\n",
      "512/25250 (epoch 1), train_loss = 1.471, time/batch = 0.419\n",
      "513/25250 (epoch 1), train_loss = 1.476, time/batch = 0.373\n",
      "514/25250 (epoch 1), train_loss = 1.427, time/batch = 0.380\n",
      "515/25250 (epoch 1), train_loss = 1.511, time/batch = 0.382\n",
      "516/25250 (epoch 1), train_loss = 1.507, time/batch = 0.417\n",
      "517/25250 (epoch 1), train_loss = 1.460, time/batch = 0.421\n",
      "518/25250 (epoch 1), train_loss = 1.578, time/batch = 0.381\n",
      "519/25250 (epoch 1), train_loss = 1.633, time/batch = 0.407\n",
      "520/25250 (epoch 1), train_loss = 1.568, time/batch = 0.411\n",
      "521/25250 (epoch 1), train_loss = 1.584, time/batch = 0.396\n",
      "522/25250 (epoch 1), train_loss = 1.649, time/batch = 0.406\n",
      "523/25250 (epoch 1), train_loss = 1.549, time/batch = 0.432\n",
      "524/25250 (epoch 1), train_loss = 1.501, time/batch = 0.450\n",
      "525/25250 (epoch 1), train_loss = 1.442, time/batch = 0.405\n",
      "526/25250 (epoch 1), train_loss = 1.523, time/batch = 0.447\n",
      "527/25250 (epoch 1), train_loss = 1.482, time/batch = 0.482\n",
      "528/25250 (epoch 1), train_loss = 1.544, time/batch = 0.458\n",
      "529/25250 (epoch 1), train_loss = 1.510, time/batch = 0.474\n",
      "530/25250 (epoch 1), train_loss = 1.448, time/batch = 0.499\n",
      "531/25250 (epoch 1), train_loss = 1.490, time/batch = 0.446\n",
      "532/25250 (epoch 1), train_loss = 1.541, time/batch = 0.440\n",
      "533/25250 (epoch 1), train_loss = 1.537, time/batch = 0.387\n",
      "534/25250 (epoch 1), train_loss = 1.456, time/batch = 0.409\n",
      "535/25250 (epoch 1), train_loss = 1.479, time/batch = 0.428\n",
      "536/25250 (epoch 1), train_loss = 1.466, time/batch = 0.370\n",
      "537/25250 (epoch 1), train_loss = 1.449, time/batch = 0.371\n",
      "538/25250 (epoch 1), train_loss = 1.510, time/batch = 0.328\n",
      "539/25250 (epoch 1), train_loss = 1.527, time/batch = 0.361\n",
      "540/25250 (epoch 1), train_loss = 1.560, time/batch = 0.368\n",
      "541/25250 (epoch 1), train_loss = 1.515, time/batch = 0.337\n",
      "542/25250 (epoch 1), train_loss = 1.507, time/batch = 0.396\n",
      "543/25250 (epoch 1), train_loss = 1.492, time/batch = 0.371\n",
      "544/25250 (epoch 1), train_loss = 1.531, time/batch = 0.388\n",
      "545/25250 (epoch 1), train_loss = 1.535, time/batch = 0.355\n",
      "546/25250 (epoch 1), train_loss = 1.568, time/batch = 0.347\n",
      "547/25250 (epoch 1), train_loss = 1.560, time/batch = 0.357\n",
      "548/25250 (epoch 1), train_loss = 1.499, time/batch = 0.356\n",
      "549/25250 (epoch 1), train_loss = 1.400, time/batch = 0.358\n",
      "550/25250 (epoch 1), train_loss = 1.413, time/batch = 0.365\n",
      "551/25250 (epoch 1), train_loss = 1.439, time/batch = 0.374\n",
      "552/25250 (epoch 1), train_loss = 1.463, time/batch = 0.354\n",
      "553/25250 (epoch 1), train_loss = 1.459, time/batch = 0.356\n",
      "554/25250 (epoch 1), train_loss = 1.531, time/batch = 0.379\n",
      "555/25250 (epoch 1), train_loss = 1.498, time/batch = 0.389\n",
      "556/25250 (epoch 1), train_loss = 1.437, time/batch = 0.413\n",
      "557/25250 (epoch 1), train_loss = 1.477, time/batch = 0.403\n",
      "558/25250 (epoch 1), train_loss = 1.495, time/batch = 0.370\n",
      "559/25250 (epoch 1), train_loss = 1.427, time/batch = 0.392\n",
      "560/25250 (epoch 1), train_loss = 1.572, time/batch = 0.408\n",
      "561/25250 (epoch 1), train_loss = 1.522, time/batch = 0.407\n",
      "562/25250 (epoch 1), train_loss = 1.466, time/batch = 0.362\n",
      "563/25250 (epoch 1), train_loss = 1.486, time/batch = 0.410\n",
      "564/25250 (epoch 1), train_loss = 1.406, time/batch = 0.378\n",
      "565/25250 (epoch 1), train_loss = 1.526, time/batch = 0.370\n",
      "566/25250 (epoch 1), train_loss = 1.547, time/batch = 0.379\n",
      "567/25250 (epoch 1), train_loss = 1.495, time/batch = 0.395\n",
      "568/25250 (epoch 1), train_loss = 1.517, time/batch = 0.487\n",
      "569/25250 (epoch 1), train_loss = 1.498, time/batch = 0.465\n",
      "570/25250 (epoch 1), train_loss = 1.501, time/batch = 0.462\n",
      "571/25250 (epoch 1), train_loss = 1.553, time/batch = 0.388\n",
      "572/25250 (epoch 1), train_loss = 1.589, time/batch = 0.377\n",
      "573/25250 (epoch 1), train_loss = 1.595, time/batch = 0.395\n",
      "574/25250 (epoch 1), train_loss = 1.588, time/batch = 0.402\n",
      "575/25250 (epoch 1), train_loss = 1.557, time/batch = 0.402\n",
      "576/25250 (epoch 1), train_loss = 1.474, time/batch = 0.354\n",
      "577/25250 (epoch 1), train_loss = 1.612, time/batch = 0.383\n",
      "578/25250 (epoch 1), train_loss = 1.478, time/batch = 0.394\n",
      "579/25250 (epoch 1), train_loss = 1.473, time/batch = 0.420\n",
      "580/25250 (epoch 1), train_loss = 1.483, time/batch = 0.382\n",
      "581/25250 (epoch 1), train_loss = 1.483, time/batch = 0.356\n",
      "582/25250 (epoch 1), train_loss = 1.516, time/batch = 0.418\n",
      "583/25250 (epoch 1), train_loss = 1.438, time/batch = 0.353\n",
      "584/25250 (epoch 1), train_loss = 1.504, time/batch = 0.373\n",
      "585/25250 (epoch 1), train_loss = 1.543, time/batch = 0.407\n",
      "586/25250 (epoch 1), train_loss = 1.456, time/batch = 0.446\n",
      "587/25250 (epoch 1), train_loss = 1.517, time/batch = 0.389\n",
      "588/25250 (epoch 1), train_loss = 1.471, time/batch = 0.369\n",
      "589/25250 (epoch 1), train_loss = 1.479, time/batch = 0.368\n",
      "590/25250 (epoch 1), train_loss = 1.418, time/batch = 0.439\n",
      "591/25250 (epoch 1), train_loss = 1.449, time/batch = 0.487\n",
      "592/25250 (epoch 1), train_loss = 1.425, time/batch = 0.533\n",
      "593/25250 (epoch 1), train_loss = 1.484, time/batch = 0.426\n",
      "594/25250 (epoch 1), train_loss = 1.593, time/batch = 0.429\n",
      "595/25250 (epoch 1), train_loss = 1.532, time/batch = 0.430\n",
      "596/25250 (epoch 1), train_loss = 1.496, time/batch = 0.419\n",
      "597/25250 (epoch 1), train_loss = 1.441, time/batch = 0.384\n",
      "598/25250 (epoch 1), train_loss = 1.474, time/batch = 0.381\n",
      "599/25250 (epoch 1), train_loss = 1.437, time/batch = 0.370\n",
      "600/25250 (epoch 1), train_loss = 1.483, time/batch = 0.386\n",
      "601/25250 (epoch 1), train_loss = 1.474, time/batch = 0.377\n",
      "602/25250 (epoch 1), train_loss = 1.455, time/batch = 0.439\n",
      "603/25250 (epoch 1), train_loss = 1.521, time/batch = 0.479\n",
      "604/25250 (epoch 1), train_loss = 1.486, time/batch = 0.412\n",
      "605/25250 (epoch 1), train_loss = 1.533, time/batch = 0.365\n",
      "606/25250 (epoch 1), train_loss = 1.552, time/batch = 0.379\n",
      "607/25250 (epoch 1), train_loss = 1.552, time/batch = 0.359\n",
      "608/25250 (epoch 1), train_loss = 1.532, time/batch = 0.394\n",
      "609/25250 (epoch 1), train_loss = 1.449, time/batch = 0.394\n",
      "610/25250 (epoch 1), train_loss = 1.483, time/batch = 0.354\n",
      "611/25250 (epoch 1), train_loss = 1.419, time/batch = 0.368\n",
      "612/25250 (epoch 1), train_loss = 1.408, time/batch = 0.340\n",
      "613/25250 (epoch 1), train_loss = 1.428, time/batch = 0.359\n",
      "614/25250 (epoch 1), train_loss = 1.466, time/batch = 0.362\n",
      "615/25250 (epoch 1), train_loss = 1.390, time/batch = 0.357\n",
      "616/25250 (epoch 1), train_loss = 1.462, time/batch = 0.388\n",
      "617/25250 (epoch 1), train_loss = 1.553, time/batch = 0.356\n",
      "618/25250 (epoch 1), train_loss = 1.481, time/batch = 0.377\n",
      "619/25250 (epoch 1), train_loss = 1.432, time/batch = 0.386\n",
      "620/25250 (epoch 1), train_loss = 1.393, time/batch = 0.351\n",
      "621/25250 (epoch 1), train_loss = 1.454, time/batch = 0.381\n",
      "622/25250 (epoch 1), train_loss = 1.389, time/batch = 0.371\n",
      "623/25250 (epoch 1), train_loss = 1.405, time/batch = 0.410\n",
      "624/25250 (epoch 1), train_loss = 1.385, time/batch = 0.368\n",
      "625/25250 (epoch 1), train_loss = 1.443, time/batch = 0.370\n",
      "626/25250 (epoch 1), train_loss = 1.411, time/batch = 0.320\n",
      "627/25250 (epoch 1), train_loss = 1.460, time/batch = 0.335\n",
      "628/25250 (epoch 1), train_loss = 1.453, time/batch = 0.352\n",
      "629/25250 (epoch 1), train_loss = 1.412, time/batch = 0.376\n",
      "630/25250 (epoch 1), train_loss = 1.392, time/batch = 0.375\n",
      "631/25250 (epoch 1), train_loss = 1.290, time/batch = 0.350\n",
      "632/25250 (epoch 1), train_loss = 1.451, time/batch = 0.369\n",
      "633/25250 (epoch 1), train_loss = 1.453, time/batch = 0.353\n",
      "634/25250 (epoch 1), train_loss = 1.459, time/batch = 0.365\n",
      "635/25250 (epoch 1), train_loss = 1.436, time/batch = 0.384\n",
      "636/25250 (epoch 1), train_loss = 1.437, time/batch = 0.360\n",
      "637/25250 (epoch 1), train_loss = 1.421, time/batch = 0.361\n",
      "638/25250 (epoch 1), train_loss = 1.360, time/batch = 0.390\n",
      "639/25250 (epoch 1), train_loss = 1.392, time/batch = 0.526\n",
      "640/25250 (epoch 1), train_loss = 1.398, time/batch = 0.454\n",
      "641/25250 (epoch 1), train_loss = 1.383, time/batch = 0.383\n",
      "642/25250 (epoch 1), train_loss = 1.412, time/batch = 0.387\n",
      "643/25250 (epoch 1), train_loss = 1.458, time/batch = 0.523\n",
      "644/25250 (epoch 1), train_loss = 1.414, time/batch = 0.476\n",
      "645/25250 (epoch 1), train_loss = 1.481, time/batch = 0.448\n",
      "646/25250 (epoch 1), train_loss = 1.423, time/batch = 0.440\n",
      "647/25250 (epoch 1), train_loss = 1.428, time/batch = 0.416\n",
      "648/25250 (epoch 1), train_loss = 1.451, time/batch = 0.480\n",
      "649/25250 (epoch 1), train_loss = 1.441, time/batch = 0.423\n",
      "650/25250 (epoch 1), train_loss = 1.474, time/batch = 0.383\n",
      "651/25250 (epoch 1), train_loss = 1.443, time/batch = 0.391\n",
      "652/25250 (epoch 1), train_loss = 1.506, time/batch = 0.377\n",
      "653/25250 (epoch 1), train_loss = 1.455, time/batch = 0.373\n",
      "654/25250 (epoch 1), train_loss = 1.503, time/batch = 0.356\n",
      "655/25250 (epoch 1), train_loss = 1.409, time/batch = 0.377\n",
      "656/25250 (epoch 1), train_loss = 1.475, time/batch = 0.359\n",
      "657/25250 (epoch 1), train_loss = 1.495, time/batch = 0.367\n",
      "658/25250 (epoch 1), train_loss = 1.463, time/batch = 0.353\n",
      "659/25250 (epoch 1), train_loss = 1.388, time/batch = 0.354\n",
      "660/25250 (epoch 1), train_loss = 1.523, time/batch = 0.340\n",
      "661/25250 (epoch 1), train_loss = 1.438, time/batch = 0.339\n",
      "662/25250 (epoch 1), train_loss = 1.371, time/batch = 0.366\n",
      "663/25250 (epoch 1), train_loss = 1.424, time/batch = 0.367\n",
      "664/25250 (epoch 1), train_loss = 1.401, time/batch = 0.373\n",
      "665/25250 (epoch 1), train_loss = 1.484, time/batch = 0.358\n",
      "666/25250 (epoch 1), train_loss = 1.374, time/batch = 0.353\n",
      "667/25250 (epoch 1), train_loss = 1.475, time/batch = 0.363\n",
      "668/25250 (epoch 1), train_loss = 1.434, time/batch = 0.409\n",
      "669/25250 (epoch 1), train_loss = 1.407, time/batch = 0.388\n",
      "670/25250 (epoch 1), train_loss = 1.427, time/batch = 0.369\n",
      "671/25250 (epoch 1), train_loss = 1.534, time/batch = 0.354\n",
      "672/25250 (epoch 1), train_loss = 1.502, time/batch = 0.357\n",
      "673/25250 (epoch 1), train_loss = 1.438, time/batch = 0.356\n",
      "674/25250 (epoch 1), train_loss = 1.469, time/batch = 0.354\n",
      "675/25250 (epoch 1), train_loss = 1.412, time/batch = 0.372\n",
      "676/25250 (epoch 1), train_loss = 1.389, time/batch = 0.448\n",
      "677/25250 (epoch 1), train_loss = 1.356, time/batch = 0.373\n",
      "678/25250 (epoch 1), train_loss = 1.406, time/batch = 0.376\n",
      "679/25250 (epoch 1), train_loss = 1.415, time/batch = 0.377\n",
      "680/25250 (epoch 1), train_loss = 1.376, time/batch = 0.339\n",
      "681/25250 (epoch 1), train_loss = 1.413, time/batch = 0.347\n",
      "682/25250 (epoch 1), train_loss = 1.497, time/batch = 0.377\n",
      "683/25250 (epoch 1), train_loss = 1.442, time/batch = 0.347\n",
      "684/25250 (epoch 1), train_loss = 1.464, time/batch = 0.341\n",
      "685/25250 (epoch 1), train_loss = 1.344, time/batch = 0.354\n",
      "686/25250 (epoch 1), train_loss = 1.403, time/batch = 0.366\n",
      "687/25250 (epoch 1), train_loss = 1.417, time/batch = 0.367\n",
      "688/25250 (epoch 1), train_loss = 1.319, time/batch = 0.410\n",
      "689/25250 (epoch 1), train_loss = 1.363, time/batch = 0.374\n",
      "690/25250 (epoch 1), train_loss = 1.545, time/batch = 0.391\n",
      "691/25250 (epoch 1), train_loss = 1.430, time/batch = 0.378\n",
      "692/25250 (epoch 1), train_loss = 1.437, time/batch = 0.345\n",
      "693/25250 (epoch 1), train_loss = 1.472, time/batch = 0.335\n",
      "694/25250 (epoch 1), train_loss = 1.409, time/batch = 0.347\n",
      "695/25250 (epoch 1), train_loss = 1.506, time/batch = 0.349\n",
      "696/25250 (epoch 1), train_loss = 1.536, time/batch = 0.354\n",
      "697/25250 (epoch 1), train_loss = 1.492, time/batch = 0.388\n",
      "698/25250 (epoch 1), train_loss = 1.568, time/batch = 0.351\n",
      "699/25250 (epoch 1), train_loss = 1.487, time/batch = 0.361\n",
      "700/25250 (epoch 1), train_loss = 1.428, time/batch = 0.378\n",
      "701/25250 (epoch 1), train_loss = 1.344, time/batch = 0.405\n",
      "702/25250 (epoch 1), train_loss = 1.387, time/batch = 0.381\n",
      "703/25250 (epoch 1), train_loss = 1.358, time/batch = 0.346\n",
      "704/25250 (epoch 1), train_loss = 1.385, time/batch = 0.358\n",
      "705/25250 (epoch 1), train_loss = 1.406, time/batch = 0.376\n",
      "706/25250 (epoch 1), train_loss = 1.333, time/batch = 0.371\n",
      "707/25250 (epoch 1), train_loss = 1.342, time/batch = 0.368\n",
      "708/25250 (epoch 1), train_loss = 1.377, time/batch = 0.362\n",
      "709/25250 (epoch 1), train_loss = 1.386, time/batch = 0.350\n",
      "710/25250 (epoch 1), train_loss = 1.419, time/batch = 0.359\n",
      "711/25250 (epoch 1), train_loss = 1.425, time/batch = 0.380\n",
      "712/25250 (epoch 1), train_loss = 1.391, time/batch = 0.359\n",
      "713/25250 (epoch 1), train_loss = 1.376, time/batch = 0.361\n",
      "714/25250 (epoch 1), train_loss = 1.407, time/batch = 0.392\n",
      "715/25250 (epoch 1), train_loss = 1.448, time/batch = 0.431\n",
      "716/25250 (epoch 1), train_loss = 1.393, time/batch = 0.400\n",
      "717/25250 (epoch 1), train_loss = 1.376, time/batch = 0.354\n",
      "718/25250 (epoch 1), train_loss = 1.389, time/batch = 0.387\n",
      "719/25250 (epoch 1), train_loss = 1.334, time/batch = 0.373\n",
      "720/25250 (epoch 1), train_loss = 1.368, time/batch = 0.324\n",
      "721/25250 (epoch 1), train_loss = 1.390, time/batch = 0.339\n",
      "722/25250 (epoch 1), train_loss = 1.453, time/batch = 0.364\n",
      "723/25250 (epoch 1), train_loss = 1.447, time/batch = 0.415\n",
      "724/25250 (epoch 1), train_loss = 1.513, time/batch = 0.349\n",
      "725/25250 (epoch 1), train_loss = 1.415, time/batch = 0.359\n",
      "726/25250 (epoch 1), train_loss = 1.411, time/batch = 0.349\n",
      "727/25250 (epoch 1), train_loss = 1.436, time/batch = 0.353\n",
      "728/25250 (epoch 1), train_loss = 1.466, time/batch = 0.351\n",
      "729/25250 (epoch 1), train_loss = 1.370, time/batch = 0.340\n",
      "730/25250 (epoch 1), train_loss = 1.328, time/batch = 0.345\n",
      "731/25250 (epoch 1), train_loss = 1.474, time/batch = 0.366\n",
      "732/25250 (epoch 1), train_loss = 1.446, time/batch = 0.364\n",
      "733/25250 (epoch 1), train_loss = 1.357, time/batch = 0.377\n",
      "734/25250 (epoch 1), train_loss = 1.449, time/batch = 0.404\n",
      "735/25250 (epoch 1), train_loss = 1.375, time/batch = 0.384\n",
      "736/25250 (epoch 1), train_loss = 1.388, time/batch = 0.341\n",
      "737/25250 (epoch 1), train_loss = 1.440, time/batch = 0.356\n",
      "738/25250 (epoch 1), train_loss = 1.438, time/batch = 0.372\n",
      "739/25250 (epoch 1), train_loss = 1.412, time/batch = 0.384\n",
      "740/25250 (epoch 1), train_loss = 1.407, time/batch = 0.358\n",
      "741/25250 (epoch 1), train_loss = 1.391, time/batch = 0.371\n",
      "742/25250 (epoch 1), train_loss = 1.367, time/batch = 0.365\n",
      "743/25250 (epoch 1), train_loss = 1.360, time/batch = 0.399\n",
      "744/25250 (epoch 1), train_loss = 1.496, time/batch = 0.447\n",
      "745/25250 (epoch 1), train_loss = 1.390, time/batch = 0.349\n",
      "746/25250 (epoch 1), train_loss = 1.405, time/batch = 0.341\n",
      "747/25250 (epoch 1), train_loss = 1.391, time/batch = 0.342\n",
      "748/25250 (epoch 1), train_loss = 1.374, time/batch = 0.348\n",
      "749/25250 (epoch 1), train_loss = 1.372, time/batch = 0.360\n",
      "750/25250 (epoch 1), train_loss = 1.419, time/batch = 0.388\n",
      "751/25250 (epoch 1), train_loss = 1.337, time/batch = 0.341\n",
      "752/25250 (epoch 1), train_loss = 1.434, time/batch = 0.385\n",
      "753/25250 (epoch 1), train_loss = 1.443, time/batch = 0.386\n",
      "754/25250 (epoch 1), train_loss = 1.410, time/batch = 0.403\n",
      "755/25250 (epoch 1), train_loss = 1.413, time/batch = 0.372\n",
      "756/25250 (epoch 1), train_loss = 1.397, time/batch = 0.394\n",
      "757/25250 (epoch 1), train_loss = 1.390, time/batch = 0.369\n",
      "758/25250 (epoch 1), train_loss = 1.422, time/batch = 0.368\n",
      "759/25250 (epoch 1), train_loss = 1.386, time/batch = 0.370\n",
      "760/25250 (epoch 1), train_loss = 1.366, time/batch = 0.391\n",
      "761/25250 (epoch 1), train_loss = 1.398, time/batch = 0.381\n",
      "762/25250 (epoch 1), train_loss = 1.356, time/batch = 0.375\n",
      "763/25250 (epoch 1), train_loss = 1.381, time/batch = 0.386\n",
      "764/25250 (epoch 1), train_loss = 1.310, time/batch = 0.377\n",
      "765/25250 (epoch 1), train_loss = 1.354, time/batch = 0.360\n",
      "766/25250 (epoch 1), train_loss = 1.354, time/batch = 0.344\n",
      "767/25250 (epoch 1), train_loss = 1.316, time/batch = 0.392\n",
      "768/25250 (epoch 1), train_loss = 1.386, time/batch = 0.374\n",
      "769/25250 (epoch 1), train_loss = 1.334, time/batch = 0.359\n",
      "770/25250 (epoch 1), train_loss = 1.353, time/batch = 0.362\n",
      "771/25250 (epoch 1), train_loss = 1.325, time/batch = 0.380\n",
      "772/25250 (epoch 1), train_loss = 1.444, time/batch = 0.414\n",
      "773/25250 (epoch 1), train_loss = 1.428, time/batch = 0.377\n",
      "774/25250 (epoch 1), train_loss = 1.327, time/batch = 0.365\n",
      "775/25250 (epoch 1), train_loss = 1.342, time/batch = 0.368\n",
      "776/25250 (epoch 1), train_loss = 1.395, time/batch = 0.341\n",
      "777/25250 (epoch 1), train_loss = 1.387, time/batch = 0.388\n",
      "778/25250 (epoch 1), train_loss = 1.372, time/batch = 0.407\n",
      "779/25250 (epoch 1), train_loss = 1.344, time/batch = 0.380\n",
      "780/25250 (epoch 1), train_loss = 1.392, time/batch = 0.380\n",
      "781/25250 (epoch 1), train_loss = 1.426, time/batch = 0.386\n",
      "782/25250 (epoch 1), train_loss = 1.394, time/batch = 0.370\n",
      "783/25250 (epoch 1), train_loss = 1.382, time/batch = 0.369\n",
      "784/25250 (epoch 1), train_loss = 1.463, time/batch = 0.359\n",
      "785/25250 (epoch 1), train_loss = 1.452, time/batch = 0.362\n",
      "786/25250 (epoch 1), train_loss = 1.494, time/batch = 0.392\n",
      "787/25250 (epoch 1), train_loss = 1.343, time/batch = 0.384\n",
      "788/25250 (epoch 1), train_loss = 1.362, time/batch = 0.388\n",
      "789/25250 (epoch 1), train_loss = 1.379, time/batch = 0.431\n",
      "790/25250 (epoch 1), train_loss = 1.436, time/batch = 0.386\n",
      "791/25250 (epoch 1), train_loss = 1.356, time/batch = 0.383\n",
      "792/25250 (epoch 1), train_loss = 1.370, time/batch = 0.367\n",
      "793/25250 (epoch 1), train_loss = 1.348, time/batch = 0.355\n",
      "794/25250 (epoch 1), train_loss = 1.346, time/batch = 0.349\n",
      "795/25250 (epoch 1), train_loss = 1.430, time/batch = 0.353\n",
      "796/25250 (epoch 1), train_loss = 1.425, time/batch = 0.377\n",
      "797/25250 (epoch 1), train_loss = 1.392, time/batch = 0.409\n",
      "798/25250 (epoch 1), train_loss = 1.450, time/batch = 0.373\n",
      "799/25250 (epoch 1), train_loss = 1.451, time/batch = 0.361\n",
      "800/25250 (epoch 1), train_loss = 1.472, time/batch = 0.384\n",
      "801/25250 (epoch 1), train_loss = 1.441, time/batch = 0.354\n",
      "802/25250 (epoch 1), train_loss = 1.389, time/batch = 0.405\n",
      "803/25250 (epoch 1), train_loss = 1.400, time/batch = 0.366\n",
      "804/25250 (epoch 1), train_loss = 1.471, time/batch = 0.356\n",
      "805/25250 (epoch 1), train_loss = 1.465, time/batch = 0.365\n",
      "806/25250 (epoch 1), train_loss = 1.350, time/batch = 0.346\n",
      "807/25250 (epoch 1), train_loss = 1.259, time/batch = 0.374\n",
      "808/25250 (epoch 1), train_loss = 1.335, time/batch = 0.354\n",
      "809/25250 (epoch 1), train_loss = 1.324, time/batch = 0.382\n",
      "810/25250 (epoch 1), train_loss = 1.400, time/batch = 0.340\n",
      "811/25250 (epoch 1), train_loss = 1.347, time/batch = 0.377\n",
      "812/25250 (epoch 1), train_loss = 1.387, time/batch = 0.361\n",
      "813/25250 (epoch 1), train_loss = 1.385, time/batch = 0.379\n",
      "814/25250 (epoch 1), train_loss = 1.361, time/batch = 0.401\n",
      "815/25250 (epoch 1), train_loss = 1.385, time/batch = 0.376\n",
      "816/25250 (epoch 1), train_loss = 1.377, time/batch = 0.367\n",
      "817/25250 (epoch 1), train_loss = 1.330, time/batch = 0.416\n",
      "818/25250 (epoch 1), train_loss = 1.355, time/batch = 0.386\n",
      "819/25250 (epoch 1), train_loss = 1.365, time/batch = 0.384\n",
      "820/25250 (epoch 1), train_loss = 1.300, time/batch = 0.373\n",
      "821/25250 (epoch 1), train_loss = 1.397, time/batch = 0.373\n",
      "822/25250 (epoch 1), train_loss = 1.368, time/batch = 0.388\n",
      "823/25250 (epoch 1), train_loss = 1.402, time/batch = 0.372\n",
      "824/25250 (epoch 1), train_loss = 1.372, time/batch = 0.368\n",
      "825/25250 (epoch 1), train_loss = 1.384, time/batch = 0.373\n",
      "826/25250 (epoch 1), train_loss = 1.342, time/batch = 0.382\n",
      "827/25250 (epoch 1), train_loss = 1.382, time/batch = 0.365\n",
      "828/25250 (epoch 1), train_loss = 1.290, time/batch = 0.358\n",
      "829/25250 (epoch 1), train_loss = 1.392, time/batch = 0.368\n",
      "830/25250 (epoch 1), train_loss = 1.352, time/batch = 0.379\n",
      "831/25250 (epoch 1), train_loss = 1.432, time/batch = 0.333\n",
      "832/25250 (epoch 1), train_loss = 1.418, time/batch = 0.352\n",
      "833/25250 (epoch 1), train_loss = 1.448, time/batch = 0.356\n",
      "834/25250 (epoch 1), train_loss = 1.400, time/batch = 0.352\n",
      "835/25250 (epoch 1), train_loss = 1.359, time/batch = 0.358\n",
      "836/25250 (epoch 1), train_loss = 1.332, time/batch = 0.367\n",
      "837/25250 (epoch 1), train_loss = 1.382, time/batch = 0.339\n",
      "838/25250 (epoch 1), train_loss = 1.423, time/batch = 0.407\n",
      "839/25250 (epoch 1), train_loss = 1.374, time/batch = 0.414\n",
      "840/25250 (epoch 1), train_loss = 1.329, time/batch = 0.403\n",
      "841/25250 (epoch 1), train_loss = 1.341, time/batch = 0.411\n",
      "842/25250 (epoch 1), train_loss = 1.441, time/batch = 0.441\n",
      "843/25250 (epoch 1), train_loss = 1.370, time/batch = 0.448\n",
      "844/25250 (epoch 1), train_loss = 1.375, time/batch = 0.431\n",
      "845/25250 (epoch 1), train_loss = 1.419, time/batch = 0.409\n",
      "846/25250 (epoch 1), train_loss = 1.388, time/batch = 0.383\n",
      "847/25250 (epoch 1), train_loss = 1.382, time/batch = 0.377\n",
      "848/25250 (epoch 1), train_loss = 1.414, time/batch = 0.382\n",
      "849/25250 (epoch 1), train_loss = 1.344, time/batch = 0.362\n",
      "850/25250 (epoch 1), train_loss = 1.422, time/batch = 0.415\n",
      "851/25250 (epoch 1), train_loss = 1.416, time/batch = 0.342\n",
      "852/25250 (epoch 1), train_loss = 1.384, time/batch = 0.337\n",
      "853/25250 (epoch 1), train_loss = 1.345, time/batch = 0.353\n",
      "854/25250 (epoch 1), train_loss = 1.416, time/batch = 0.347\n",
      "855/25250 (epoch 1), train_loss = 1.355, time/batch = 0.366\n",
      "856/25250 (epoch 1), train_loss = 1.244, time/batch = 0.363\n",
      "857/25250 (epoch 1), train_loss = 1.286, time/batch = 0.364\n",
      "858/25250 (epoch 1), train_loss = 1.350, time/batch = 0.354\n",
      "859/25250 (epoch 1), train_loss = 1.333, time/batch = 0.367\n",
      "860/25250 (epoch 1), train_loss = 1.386, time/batch = 0.406\n",
      "861/25250 (epoch 1), train_loss = 1.277, time/batch = 0.416\n",
      "862/25250 (epoch 1), train_loss = 1.388, time/batch = 0.367\n",
      "863/25250 (epoch 1), train_loss = 1.341, time/batch = 0.380\n",
      "864/25250 (epoch 1), train_loss = 1.356, time/batch = 0.490\n",
      "865/25250 (epoch 1), train_loss = 1.336, time/batch = 0.472\n",
      "866/25250 (epoch 1), train_loss = 1.359, time/batch = 0.396\n",
      "867/25250 (epoch 1), train_loss = 1.327, time/batch = 0.401\n",
      "868/25250 (epoch 1), train_loss = 1.338, time/batch = 0.426\n",
      "869/25250 (epoch 1), train_loss = 1.359, time/batch = 0.381\n",
      "870/25250 (epoch 1), train_loss = 1.341, time/batch = 0.387\n",
      "871/25250 (epoch 1), train_loss = 1.323, time/batch = 0.372\n",
      "872/25250 (epoch 1), train_loss = 1.363, time/batch = 0.364\n",
      "873/25250 (epoch 1), train_loss = 1.314, time/batch = 0.363\n",
      "874/25250 (epoch 1), train_loss = 1.338, time/batch = 0.339\n",
      "875/25250 (epoch 1), train_loss = 1.322, time/batch = 0.345\n",
      "876/25250 (epoch 1), train_loss = 1.273, time/batch = 0.353\n",
      "877/25250 (epoch 1), train_loss = 1.286, time/batch = 0.431\n",
      "878/25250 (epoch 1), train_loss = 1.363, time/batch = 0.424\n",
      "879/25250 (epoch 1), train_loss = 1.332, time/batch = 0.375\n",
      "880/25250 (epoch 1), train_loss = 1.448, time/batch = 0.376\n",
      "881/25250 (epoch 1), train_loss = 1.375, time/batch = 0.379\n",
      "882/25250 (epoch 1), train_loss = 1.315, time/batch = 0.382\n",
      "883/25250 (epoch 1), train_loss = 1.327, time/batch = 0.342\n",
      "884/25250 (epoch 1), train_loss = 1.297, time/batch = 0.397\n",
      "885/25250 (epoch 1), train_loss = 1.278, time/batch = 0.366\n",
      "886/25250 (epoch 1), train_loss = 1.289, time/batch = 0.371\n",
      "887/25250 (epoch 1), train_loss = 1.269, time/batch = 0.376\n",
      "888/25250 (epoch 1), train_loss = 1.275, time/batch = 0.407\n",
      "889/25250 (epoch 1), train_loss = 1.241, time/batch = 0.382\n",
      "890/25250 (epoch 1), train_loss = 1.288, time/batch = 0.341\n",
      "891/25250 (epoch 1), train_loss = 1.309, time/batch = 0.394\n",
      "892/25250 (epoch 1), train_loss = 1.330, time/batch = 0.369\n",
      "893/25250 (epoch 1), train_loss = 1.398, time/batch = 0.376\n",
      "894/25250 (epoch 1), train_loss = 1.382, time/batch = 0.346\n",
      "895/25250 (epoch 1), train_loss = 1.314, time/batch = 0.368\n",
      "896/25250 (epoch 1), train_loss = 1.430, time/batch = 0.360\n",
      "897/25250 (epoch 1), train_loss = 1.280, time/batch = 0.389\n",
      "898/25250 (epoch 1), train_loss = 1.269, time/batch = 0.387\n",
      "899/25250 (epoch 1), train_loss = 1.405, time/batch = 0.384\n",
      "900/25250 (epoch 1), train_loss = 1.418, time/batch = 0.511\n",
      "901/25250 (epoch 1), train_loss = 1.370, time/batch = 0.489\n",
      "902/25250 (epoch 1), train_loss = 1.344, time/batch = 0.524\n",
      "903/25250 (epoch 1), train_loss = 1.353, time/batch = 0.461\n",
      "904/25250 (epoch 1), train_loss = 1.396, time/batch = 0.409\n",
      "905/25250 (epoch 1), train_loss = 1.341, time/batch = 0.465\n",
      "906/25250 (epoch 1), train_loss = 1.314, time/batch = 0.472\n",
      "907/25250 (epoch 1), train_loss = 1.347, time/batch = 0.430\n",
      "908/25250 (epoch 1), train_loss = 1.397, time/batch = 0.514\n",
      "909/25250 (epoch 1), train_loss = 1.428, time/batch = 0.469\n",
      "910/25250 (epoch 1), train_loss = 1.462, time/batch = 0.424\n",
      "911/25250 (epoch 1), train_loss = 1.337, time/batch = 0.349\n",
      "912/25250 (epoch 1), train_loss = 1.352, time/batch = 0.416\n",
      "913/25250 (epoch 1), train_loss = 1.333, time/batch = 0.363\n",
      "914/25250 (epoch 1), train_loss = 1.287, time/batch = 0.345\n",
      "915/25250 (epoch 1), train_loss = 1.407, time/batch = 0.404\n",
      "916/25250 (epoch 1), train_loss = 1.372, time/batch = 0.404\n",
      "917/25250 (epoch 1), train_loss = 1.310, time/batch = 0.446\n",
      "918/25250 (epoch 1), train_loss = 1.343, time/batch = 0.402\n",
      "919/25250 (epoch 1), train_loss = 1.323, time/batch = 0.382\n",
      "920/25250 (epoch 1), train_loss = 1.265, time/batch = 0.414\n",
      "921/25250 (epoch 1), train_loss = 1.257, time/batch = 0.455\n",
      "922/25250 (epoch 1), train_loss = 1.306, time/batch = 0.395\n",
      "923/25250 (epoch 1), train_loss = 1.251, time/batch = 0.396\n",
      "924/25250 (epoch 1), train_loss = 1.382, time/batch = 0.379\n",
      "925/25250 (epoch 1), train_loss = 1.319, time/batch = 0.406\n",
      "926/25250 (epoch 1), train_loss = 1.365, time/batch = 0.377\n",
      "927/25250 (epoch 1), train_loss = 1.311, time/batch = 0.371\n",
      "928/25250 (epoch 1), train_loss = 1.360, time/batch = 0.398\n",
      "929/25250 (epoch 1), train_loss = 1.285, time/batch = 0.406\n",
      "930/25250 (epoch 1), train_loss = 1.274, time/batch = 0.358\n",
      "931/25250 (epoch 1), train_loss = 1.328, time/batch = 0.352\n",
      "932/25250 (epoch 1), train_loss = 1.331, time/batch = 0.379\n",
      "933/25250 (epoch 1), train_loss = 1.328, time/batch = 0.416\n",
      "934/25250 (epoch 1), train_loss = 1.292, time/batch = 0.349\n",
      "935/25250 (epoch 1), train_loss = 1.342, time/batch = 0.358\n",
      "936/25250 (epoch 1), train_loss = 1.256, time/batch = 0.367\n",
      "937/25250 (epoch 1), train_loss = 1.292, time/batch = 0.447\n",
      "938/25250 (epoch 1), train_loss = 1.281, time/batch = 0.425\n",
      "939/25250 (epoch 1), train_loss = 1.342, time/batch = 0.444\n",
      "940/25250 (epoch 1), train_loss = 1.280, time/batch = 0.454\n",
      "941/25250 (epoch 1), train_loss = 1.277, time/batch = 0.450\n",
      "942/25250 (epoch 1), train_loss = 1.325, time/batch = 0.399\n",
      "943/25250 (epoch 1), train_loss = 1.301, time/batch = 0.416\n",
      "944/25250 (epoch 1), train_loss = 1.303, time/batch = 0.422\n",
      "945/25250 (epoch 1), train_loss = 1.273, time/batch = 0.377\n",
      "946/25250 (epoch 1), train_loss = 1.254, time/batch = 0.383\n",
      "947/25250 (epoch 1), train_loss = 1.295, time/batch = 0.368\n",
      "948/25250 (epoch 1), train_loss = 1.315, time/batch = 0.427\n",
      "949/25250 (epoch 1), train_loss = 1.356, time/batch = 0.487\n",
      "950/25250 (epoch 1), train_loss = 1.228, time/batch = 0.474\n",
      "951/25250 (epoch 1), train_loss = 1.272, time/batch = 0.482\n",
      "952/25250 (epoch 1), train_loss = 1.265, time/batch = 0.476\n",
      "953/25250 (epoch 1), train_loss = 1.270, time/batch = 0.494\n",
      "954/25250 (epoch 1), train_loss = 1.352, time/batch = 0.497\n",
      "955/25250 (epoch 1), train_loss = 1.330, time/batch = 0.417\n",
      "956/25250 (epoch 1), train_loss = 1.387, time/batch = 0.501\n",
      "957/25250 (epoch 1), train_loss = 1.292, time/batch = 0.453\n",
      "958/25250 (epoch 1), train_loss = 1.344, time/batch = 0.397\n",
      "959/25250 (epoch 1), train_loss = 1.406, time/batch = 0.373\n",
      "960/25250 (epoch 1), train_loss = 1.356, time/batch = 0.378\n",
      "961/25250 (epoch 1), train_loss = 1.382, time/batch = 0.387\n",
      "962/25250 (epoch 1), train_loss = 1.284, time/batch = 0.618\n",
      "963/25250 (epoch 1), train_loss = 1.316, time/batch = 0.428\n",
      "964/25250 (epoch 1), train_loss = 1.258, time/batch = 0.424\n",
      "965/25250 (epoch 1), train_loss = 1.310, time/batch = 0.388\n",
      "966/25250 (epoch 1), train_loss = 1.264, time/batch = 0.384\n",
      "967/25250 (epoch 1), train_loss = 1.329, time/batch = 0.371\n",
      "968/25250 (epoch 1), train_loss = 1.267, time/batch = 0.413\n",
      "969/25250 (epoch 1), train_loss = 1.321, time/batch = 0.440\n",
      "970/25250 (epoch 1), train_loss = 1.290, time/batch = 0.402\n",
      "971/25250 (epoch 1), train_loss = 1.391, time/batch = 0.385\n",
      "972/25250 (epoch 1), train_loss = 1.394, time/batch = 0.363\n",
      "973/25250 (epoch 1), train_loss = 1.362, time/batch = 0.366\n",
      "974/25250 (epoch 1), train_loss = 1.335, time/batch = 0.361\n",
      "975/25250 (epoch 1), train_loss = 1.358, time/batch = 0.383\n",
      "976/25250 (epoch 1), train_loss = 1.261, time/batch = 0.408\n",
      "977/25250 (epoch 1), train_loss = 1.332, time/batch = 0.445\n",
      "978/25250 (epoch 1), train_loss = 1.320, time/batch = 0.448\n",
      "979/25250 (epoch 1), train_loss = 1.262, time/batch = 0.379\n",
      "980/25250 (epoch 1), train_loss = 1.357, time/batch = 0.374\n",
      "981/25250 (epoch 1), train_loss = 1.312, time/batch = 0.401\n",
      "982/25250 (epoch 1), train_loss = 1.354, time/batch = 0.405\n",
      "983/25250 (epoch 1), train_loss = 1.322, time/batch = 0.350\n",
      "984/25250 (epoch 1), train_loss = 1.309, time/batch = 0.338\n",
      "985/25250 (epoch 1), train_loss = 1.282, time/batch = 0.350\n",
      "986/25250 (epoch 1), train_loss = 1.277, time/batch = 0.358\n",
      "987/25250 (epoch 1), train_loss = 1.321, time/batch = 0.435\n",
      "988/25250 (epoch 1), train_loss = 1.270, time/batch = 0.465\n",
      "989/25250 (epoch 1), train_loss = 1.340, time/batch = 0.394\n",
      "990/25250 (epoch 1), train_loss = 1.279, time/batch = 0.483\n",
      "991/25250 (epoch 1), train_loss = 1.334, time/batch = 0.368\n",
      "992/25250 (epoch 1), train_loss = 1.352, time/batch = 0.393\n",
      "993/25250 (epoch 1), train_loss = 1.258, time/batch = 0.346\n",
      "994/25250 (epoch 1), train_loss = 1.314, time/batch = 0.386\n",
      "995/25250 (epoch 1), train_loss = 1.378, time/batch = 0.341\n",
      "996/25250 (epoch 1), train_loss = 1.279, time/batch = 0.422\n",
      "997/25250 (epoch 1), train_loss = 1.343, time/batch = 0.443\n",
      "998/25250 (epoch 1), train_loss = 1.349, time/batch = 0.399\n",
      "999/25250 (epoch 1), train_loss = 1.342, time/batch = 0.392\n",
      "1000/25250 (epoch 1), train_loss = 1.311, time/batch = 0.383\n",
      "model saved to ./char-rnn-tensorflow/save/model.ckpt\n",
      "1001/25250 (epoch 1), train_loss = 1.291, time/batch = 0.385\n",
      "1002/25250 (epoch 1), train_loss = 1.310, time/batch = 0.355\n",
      "1003/25250 (epoch 1), train_loss = 1.257, time/batch = 0.347\n",
      "1004/25250 (epoch 1), train_loss = 1.259, time/batch = 0.341\n",
      "1005/25250 (epoch 1), train_loss = 1.300, time/batch = 0.354\n",
      "1006/25250 (epoch 1), train_loss = 1.298, time/batch = 0.411\n",
      "1007/25250 (epoch 1), train_loss = 1.266, time/batch = 0.342\n",
      "1008/25250 (epoch 1), train_loss = 1.284, time/batch = 0.403\n",
      "1009/25250 (epoch 1), train_loss = 1.383, time/batch = 0.352\n",
      "1010/25250 (epoch 2), train_loss = 1.664, time/batch = 0.356\n",
      "1011/25250 (epoch 2), train_loss = 1.319, time/batch = 0.356\n",
      "1012/25250 (epoch 2), train_loss = 1.344, time/batch = 0.357\n",
      "1013/25250 (epoch 2), train_loss = 1.305, time/batch = 0.411\n",
      "1014/25250 (epoch 2), train_loss = 1.358, time/batch = 0.356\n",
      "1015/25250 (epoch 2), train_loss = 1.312, time/batch = 0.355\n",
      "1016/25250 (epoch 2), train_loss = 1.310, time/batch = 0.364\n",
      "1017/25250 (epoch 2), train_loss = 1.272, time/batch = 0.417\n",
      "1018/25250 (epoch 2), train_loss = 1.286, time/batch = 0.397\n",
      "1019/25250 (epoch 2), train_loss = 1.244, time/batch = 0.371\n",
      "1020/25250 (epoch 2), train_loss = 1.324, time/batch = 0.369\n",
      "1021/25250 (epoch 2), train_loss = 1.293, time/batch = 0.391\n",
      "1022/25250 (epoch 2), train_loss = 1.253, time/batch = 0.572\n",
      "1023/25250 (epoch 2), train_loss = 1.371, time/batch = 0.567\n",
      "1024/25250 (epoch 2), train_loss = 1.405, time/batch = 0.557\n",
      "1025/25250 (epoch 2), train_loss = 1.361, time/batch = 0.435\n",
      "1026/25250 (epoch 2), train_loss = 1.385, time/batch = 0.386\n",
      "1027/25250 (epoch 2), train_loss = 1.422, time/batch = 0.410\n",
      "1028/25250 (epoch 2), train_loss = 1.333, time/batch = 0.411\n",
      "1029/25250 (epoch 2), train_loss = 1.308, time/batch = 0.377\n",
      "1030/25250 (epoch 2), train_loss = 1.256, time/batch = 0.442\n",
      "1031/25250 (epoch 2), train_loss = 1.291, time/batch = 0.413\n",
      "1032/25250 (epoch 2), train_loss = 1.257, time/batch = 0.397\n",
      "1033/25250 (epoch 2), train_loss = 1.336, time/batch = 0.497\n",
      "1034/25250 (epoch 2), train_loss = 1.292, time/batch = 0.426\n",
      "1035/25250 (epoch 2), train_loss = 1.228, time/batch = 0.391\n",
      "1036/25250 (epoch 2), train_loss = 1.281, time/batch = 0.447\n",
      "1037/25250 (epoch 2), train_loss = 1.340, time/batch = 0.476\n",
      "1038/25250 (epoch 2), train_loss = 1.345, time/batch = 0.435\n",
      "1039/25250 (epoch 2), train_loss = 1.256, time/batch = 0.513\n",
      "1040/25250 (epoch 2), train_loss = 1.266, time/batch = 0.460\n",
      "1041/25250 (epoch 2), train_loss = 1.246, time/batch = 0.460\n",
      "1042/25250 (epoch 2), train_loss = 1.225, time/batch = 0.413\n",
      "1043/25250 (epoch 2), train_loss = 1.285, time/batch = 0.409\n",
      "1044/25250 (epoch 2), train_loss = 1.285, time/batch = 0.385\n",
      "1045/25250 (epoch 2), train_loss = 1.359, time/batch = 0.417\n",
      "1046/25250 (epoch 2), train_loss = 1.310, time/batch = 0.389\n",
      "1047/25250 (epoch 2), train_loss = 1.286, time/batch = 0.397\n",
      "1048/25250 (epoch 2), train_loss = 1.247, time/batch = 0.460\n",
      "1049/25250 (epoch 2), train_loss = 1.284, time/batch = 0.468\n",
      "1050/25250 (epoch 2), train_loss = 1.265, time/batch = 0.495\n",
      "1051/25250 (epoch 2), train_loss = 1.353, time/batch = 0.432\n",
      "1052/25250 (epoch 2), train_loss = 1.335, time/batch = 0.501\n",
      "1053/25250 (epoch 2), train_loss = 1.296, time/batch = 0.410\n",
      "1054/25250 (epoch 2), train_loss = 1.171, time/batch = 0.389\n",
      "1055/25250 (epoch 2), train_loss = 1.201, time/batch = 0.363\n",
      "1056/25250 (epoch 2), train_loss = 1.239, time/batch = 0.369\n",
      "1057/25250 (epoch 2), train_loss = 1.288, time/batch = 0.357\n",
      "1058/25250 (epoch 2), train_loss = 1.263, time/batch = 0.357\n",
      "1059/25250 (epoch 2), train_loss = 1.349, time/batch = 0.405\n",
      "1060/25250 (epoch 2), train_loss = 1.311, time/batch = 0.402\n",
      "1061/25250 (epoch 2), train_loss = 1.257, time/batch = 0.400\n",
      "1062/25250 (epoch 2), train_loss = 1.264, time/batch = 0.389\n",
      "1063/25250 (epoch 2), train_loss = 1.331, time/batch = 0.465\n",
      "1064/25250 (epoch 2), train_loss = 1.234, time/batch = 0.381\n",
      "1065/25250 (epoch 2), train_loss = 1.386, time/batch = 0.356\n",
      "1066/25250 (epoch 2), train_loss = 1.318, time/batch = 0.377\n",
      "1067/25250 (epoch 2), train_loss = 1.295, time/batch = 0.587\n",
      "1068/25250 (epoch 2), train_loss = 1.282, time/batch = 0.467\n",
      "1069/25250 (epoch 2), train_loss = 1.231, time/batch = 0.410\n",
      "1070/25250 (epoch 2), train_loss = 1.347, time/batch = 0.396\n",
      "1071/25250 (epoch 2), train_loss = 1.380, time/batch = 0.372\n",
      "1072/25250 (epoch 2), train_loss = 1.315, time/batch = 0.364\n",
      "1073/25250 (epoch 2), train_loss = 1.336, time/batch = 0.398\n",
      "1074/25250 (epoch 2), train_loss = 1.317, time/batch = 0.392\n",
      "1075/25250 (epoch 2), train_loss = 1.310, time/batch = 0.383\n",
      "1076/25250 (epoch 2), train_loss = 1.381, time/batch = 0.378\n",
      "1077/25250 (epoch 2), train_loss = 1.397, time/batch = 0.377\n",
      "1078/25250 (epoch 2), train_loss = 1.392, time/batch = 0.457\n",
      "1079/25250 (epoch 2), train_loss = 1.404, time/batch = 0.424\n",
      "1080/25250 (epoch 2), train_loss = 1.358, time/batch = 0.381\n",
      "1081/25250 (epoch 2), train_loss = 1.290, time/batch = 0.413\n",
      "1082/25250 (epoch 2), train_loss = 1.409, time/batch = 0.424\n",
      "1083/25250 (epoch 2), train_loss = 1.290, time/batch = 0.379\n",
      "1084/25250 (epoch 2), train_loss = 1.290, time/batch = 0.380\n",
      "1085/25250 (epoch 2), train_loss = 1.303, time/batch = 0.402\n",
      "1086/25250 (epoch 2), train_loss = 1.307, time/batch = 0.386\n",
      "1087/25250 (epoch 2), train_loss = 1.332, time/batch = 0.401\n",
      "1088/25250 (epoch 2), train_loss = 1.264, time/batch = 0.454\n",
      "1089/25250 (epoch 2), train_loss = 1.327, time/batch = 0.428\n",
      "1090/25250 (epoch 2), train_loss = 1.356, time/batch = 0.416\n",
      "1091/25250 (epoch 2), train_loss = 1.276, time/batch = 0.402\n",
      "1092/25250 (epoch 2), train_loss = 1.343, time/batch = 0.392\n",
      "1093/25250 (epoch 2), train_loss = 1.310, time/batch = 0.373\n",
      "1094/25250 (epoch 2), train_loss = 1.298, time/batch = 0.427\n",
      "1095/25250 (epoch 2), train_loss = 1.243, time/batch = 0.447\n",
      "1096/25250 (epoch 2), train_loss = 1.275, time/batch = 0.402\n",
      "1097/25250 (epoch 2), train_loss = 1.239, time/batch = 0.416\n",
      "1098/25250 (epoch 2), train_loss = 1.346, time/batch = 0.413\n",
      "1099/25250 (epoch 2), train_loss = 1.417, time/batch = 0.427\n",
      "1100/25250 (epoch 2), train_loss = 1.333, time/batch = 0.454\n",
      "1101/25250 (epoch 2), train_loss = 1.276, time/batch = 0.454\n",
      "1102/25250 (epoch 2), train_loss = 1.271, time/batch = 0.531\n",
      "1103/25250 (epoch 2), train_loss = 1.317, time/batch = 0.412\n",
      "1104/25250 (epoch 2), train_loss = 1.268, time/batch = 0.367\n",
      "1105/25250 (epoch 2), train_loss = 1.332, time/batch = 0.358\n",
      "1106/25250 (epoch 2), train_loss = 1.292, time/batch = 0.353\n",
      "1107/25250 (epoch 2), train_loss = 1.261, time/batch = 0.355\n",
      "1108/25250 (epoch 2), train_loss = 1.328, time/batch = 0.387\n",
      "1109/25250 (epoch 2), train_loss = 1.268, time/batch = 0.342\n",
      "1110/25250 (epoch 2), train_loss = 1.355, time/batch = 0.418\n",
      "1111/25250 (epoch 2), train_loss = 1.373, time/batch = 0.428\n",
      "1112/25250 (epoch 2), train_loss = 1.372, time/batch = 0.457\n",
      "1113/25250 (epoch 2), train_loss = 1.362, time/batch = 0.400\n",
      "1114/25250 (epoch 2), train_loss = 1.282, time/batch = 0.375\n",
      "1115/25250 (epoch 2), train_loss = 1.309, time/batch = 0.370\n",
      "1116/25250 (epoch 2), train_loss = 1.247, time/batch = 0.352\n",
      "1117/25250 (epoch 2), train_loss = 1.219, time/batch = 0.348\n",
      "1118/25250 (epoch 2), train_loss = 1.266, time/batch = 0.360\n",
      "1119/25250 (epoch 2), train_loss = 1.292, time/batch = 0.436\n",
      "1120/25250 (epoch 2), train_loss = 1.221, time/batch = 0.462\n",
      "1121/25250 (epoch 2), train_loss = 1.298, time/batch = 0.384\n",
      "1122/25250 (epoch 2), train_loss = 1.348, time/batch = 0.356\n",
      "1123/25250 (epoch 2), train_loss = 1.302, time/batch = 0.372\n",
      "1124/25250 (epoch 2), train_loss = 1.255, time/batch = 0.392\n",
      "1125/25250 (epoch 2), train_loss = 1.224, time/batch = 0.383\n",
      "1126/25250 (epoch 2), train_loss = 1.280, time/batch = 0.388\n",
      "1127/25250 (epoch 2), train_loss = 1.251, time/batch = 0.420\n",
      "1128/25250 (epoch 2), train_loss = 1.247, time/batch = 0.419\n",
      "1129/25250 (epoch 2), train_loss = 1.229, time/batch = 0.406\n",
      "1130/25250 (epoch 2), train_loss = 1.268, time/batch = 0.406\n",
      "1131/25250 (epoch 2), train_loss = 1.252, time/batch = 0.383\n",
      "1132/25250 (epoch 2), train_loss = 1.302, time/batch = 0.418\n",
      "1133/25250 (epoch 2), train_loss = 1.312, time/batch = 0.503\n",
      "1134/25250 (epoch 2), train_loss = 1.261, time/batch = 0.535\n",
      "1135/25250 (epoch 2), train_loss = 1.253, time/batch = 0.445\n",
      "1136/25250 (epoch 2), train_loss = 1.139, time/batch = 0.411\n",
      "1137/25250 (epoch 2), train_loss = 1.309, time/batch = 0.376\n",
      "1138/25250 (epoch 2), train_loss = 1.307, time/batch = 0.435\n",
      "1139/25250 (epoch 2), train_loss = 1.302, time/batch = 0.425\n",
      "1140/25250 (epoch 2), train_loss = 1.272, time/batch = 0.524\n",
      "1141/25250 (epoch 2), train_loss = 1.286, time/batch = 0.368\n",
      "1142/25250 (epoch 2), train_loss = 1.271, time/batch = 0.410\n",
      "1143/25250 (epoch 2), train_loss = 1.201, time/batch = 0.361\n",
      "1144/25250 (epoch 2), train_loss = 1.222, time/batch = 0.398\n",
      "1145/25250 (epoch 2), train_loss = 1.246, time/batch = 0.369\n",
      "1146/25250 (epoch 2), train_loss = 1.235, time/batch = 0.417\n",
      "1147/25250 (epoch 2), train_loss = 1.267, time/batch = 0.431\n",
      "1148/25250 (epoch 2), train_loss = 1.299, time/batch = 0.461\n",
      "1149/25250 (epoch 2), train_loss = 1.263, time/batch = 0.406\n",
      "1150/25250 (epoch 2), train_loss = 1.330, time/batch = 0.386\n",
      "1151/25250 (epoch 2), train_loss = 1.262, time/batch = 0.379\n",
      "1152/25250 (epoch 2), train_loss = 1.266, time/batch = 0.411\n",
      "1153/25250 (epoch 2), train_loss = 1.267, time/batch = 0.432\n",
      "1154/25250 (epoch 2), train_loss = 1.287, time/batch = 0.389\n",
      "1155/25250 (epoch 2), train_loss = 1.331, time/batch = 0.436\n",
      "1156/25250 (epoch 2), train_loss = 1.308, time/batch = 0.408\n",
      "1157/25250 (epoch 2), train_loss = 1.351, time/batch = 0.356\n",
      "1158/25250 (epoch 2), train_loss = 1.305, time/batch = 0.466\n",
      "1159/25250 (epoch 2), train_loss = 1.346, time/batch = 0.447\n",
      "1160/25250 (epoch 2), train_loss = 1.241, time/batch = 0.419\n",
      "1161/25250 (epoch 2), train_loss = 1.333, time/batch = 0.541\n",
      "1162/25250 (epoch 2), train_loss = 1.329, time/batch = 0.469\n",
      "1163/25250 (epoch 2), train_loss = 1.292, time/batch = 0.429\n",
      "1164/25250 (epoch 2), train_loss = 1.236, time/batch = 0.405\n",
      "1165/25250 (epoch 2), train_loss = 1.360, time/batch = 0.445\n",
      "1166/25250 (epoch 2), train_loss = 1.281, time/batch = 0.466\n",
      "1167/25250 (epoch 2), train_loss = 1.223, time/batch = 0.402\n",
      "1168/25250 (epoch 2), train_loss = 1.280, time/batch = 0.418\n",
      "1169/25250 (epoch 2), train_loss = 1.234, time/batch = 0.480\n",
      "1170/25250 (epoch 2), train_loss = 1.339, time/batch = 0.515\n",
      "1171/25250 (epoch 2), train_loss = 1.242, time/batch = 0.473\n",
      "1172/25250 (epoch 2), train_loss = 1.309, time/batch = 0.396\n",
      "1173/25250 (epoch 2), train_loss = 1.287, time/batch = 0.379\n",
      "1174/25250 (epoch 2), train_loss = 1.247, time/batch = 0.456\n",
      "1175/25250 (epoch 2), train_loss = 1.297, time/batch = 0.365\n",
      "1176/25250 (epoch 2), train_loss = 1.384, time/batch = 0.384\n",
      "1177/25250 (epoch 2), train_loss = 1.350, time/batch = 0.355\n",
      "1178/25250 (epoch 2), train_loss = 1.298, time/batch = 0.365\n",
      "1179/25250 (epoch 2), train_loss = 1.313, time/batch = 0.360\n",
      "1180/25250 (epoch 2), train_loss = 1.250, time/batch = 0.400\n",
      "1181/25250 (epoch 2), train_loss = 1.212, time/batch = 0.382\n",
      "1182/25250 (epoch 2), train_loss = 1.172, time/batch = 0.380\n",
      "1183/25250 (epoch 2), train_loss = 1.224, time/batch = 0.379\n",
      "1184/25250 (epoch 2), train_loss = 1.259, time/batch = 0.409\n",
      "1185/25250 (epoch 2), train_loss = 1.219, time/batch = 0.501\n",
      "1186/25250 (epoch 2), train_loss = 1.244, time/batch = 0.451\n",
      "1187/25250 (epoch 2), train_loss = 1.327, time/batch = 0.441\n",
      "1188/25250 (epoch 2), train_loss = 1.300, time/batch = 0.404\n",
      "1189/25250 (epoch 2), train_loss = 1.327, time/batch = 0.328\n",
      "1190/25250 (epoch 2), train_loss = 1.233, time/batch = 0.386\n",
      "1191/25250 (epoch 2), train_loss = 1.268, time/batch = 0.396\n",
      "1192/25250 (epoch 2), train_loss = 1.282, time/batch = 0.362\n",
      "1193/25250 (epoch 2), train_loss = 1.188, time/batch = 0.388\n",
      "1194/25250 (epoch 2), train_loss = 1.223, time/batch = 0.419\n",
      "1195/25250 (epoch 2), train_loss = 1.401, time/batch = 0.373\n",
      "1196/25250 (epoch 2), train_loss = 1.294, time/batch = 0.381\n",
      "1197/25250 (epoch 2), train_loss = 1.306, time/batch = 0.341\n",
      "1198/25250 (epoch 2), train_loss = 1.313, time/batch = 0.379\n",
      "1199/25250 (epoch 2), train_loss = 1.290, time/batch = 0.560\n",
      "1200/25250 (epoch 2), train_loss = 1.360, time/batch = 0.483\n",
      "1201/25250 (epoch 2), train_loss = 1.377, time/batch = 0.396\n",
      "1202/25250 (epoch 2), train_loss = 1.364, time/batch = 0.419\n",
      "1203/25250 (epoch 2), train_loss = 1.401, time/batch = 0.398\n",
      "1204/25250 (epoch 2), train_loss = 1.324, time/batch = 0.361\n",
      "1205/25250 (epoch 2), train_loss = 1.276, time/batch = 0.407\n",
      "1206/25250 (epoch 2), train_loss = 1.195, time/batch = 0.428\n",
      "1207/25250 (epoch 2), train_loss = 1.251, time/batch = 0.394\n",
      "1208/25250 (epoch 2), train_loss = 1.197, time/batch = 0.379\n",
      "1209/25250 (epoch 2), train_loss = 1.221, time/batch = 0.353\n",
      "1210/25250 (epoch 2), train_loss = 1.231, time/batch = 0.348\n",
      "1211/25250 (epoch 2), train_loss = 1.158, time/batch = 0.402\n",
      "1212/25250 (epoch 2), train_loss = 1.159, time/batch = 0.416\n",
      "1213/25250 (epoch 2), train_loss = 1.201, time/batch = 0.391\n",
      "1214/25250 (epoch 2), train_loss = 1.226, time/batch = 0.377\n",
      "1215/25250 (epoch 2), train_loss = 1.261, time/batch = 0.348\n",
      "1216/25250 (epoch 2), train_loss = 1.269, time/batch = 0.352\n",
      "1217/25250 (epoch 2), train_loss = 1.235, time/batch = 0.383\n",
      "1218/25250 (epoch 2), train_loss = 1.224, time/batch = 0.383\n",
      "1219/25250 (epoch 2), train_loss = 1.259, time/batch = 0.357\n",
      "1220/25250 (epoch 2), train_loss = 1.270, time/batch = 0.339\n",
      "1221/25250 (epoch 2), train_loss = 1.224, time/batch = 0.347\n",
      "1222/25250 (epoch 2), train_loss = 1.229, time/batch = 0.397\n",
      "1223/25250 (epoch 2), train_loss = 1.241, time/batch = 0.355\n",
      "1224/25250 (epoch 2), train_loss = 1.186, time/batch = 0.381\n",
      "1225/25250 (epoch 2), train_loss = 1.197, time/batch = 0.353\n",
      "1226/25250 (epoch 2), train_loss = 1.223, time/batch = 0.355\n",
      "1227/25250 (epoch 2), train_loss = 1.259, time/batch = 0.357\n",
      "1228/25250 (epoch 2), train_loss = 1.243, time/batch = 0.372\n",
      "1229/25250 (epoch 2), train_loss = 1.336, time/batch = 0.377\n",
      "1230/25250 (epoch 2), train_loss = 1.258, time/batch = 0.400\n",
      "1231/25250 (epoch 2), train_loss = 1.266, time/batch = 0.389\n",
      "1232/25250 (epoch 2), train_loss = 1.272, time/batch = 0.361\n",
      "1233/25250 (epoch 2), train_loss = 1.270, time/batch = 0.351\n",
      "1234/25250 (epoch 2), train_loss = 1.195, time/batch = 0.360\n",
      "1235/25250 (epoch 2), train_loss = 1.184, time/batch = 0.357\n",
      "1236/25250 (epoch 2), train_loss = 1.313, time/batch = 0.449\n",
      "1237/25250 (epoch 2), train_loss = 1.290, time/batch = 0.373\n",
      "1238/25250 (epoch 2), train_loss = 1.196, time/batch = 0.353\n",
      "1239/25250 (epoch 2), train_loss = 1.290, time/batch = 0.410\n",
      "1240/25250 (epoch 2), train_loss = 1.203, time/batch = 0.391\n",
      "1241/25250 (epoch 2), train_loss = 1.236, time/batch = 0.400\n",
      "1242/25250 (epoch 2), train_loss = 1.278, time/batch = 0.403\n",
      "1243/25250 (epoch 2), train_loss = 1.283, time/batch = 0.391\n",
      "1244/25250 (epoch 2), train_loss = 1.249, time/batch = 0.352\n",
      "1245/25250 (epoch 2), train_loss = 1.267, time/batch = 0.350\n",
      "1246/25250 (epoch 2), train_loss = 1.247, time/batch = 0.335\n",
      "1247/25250 (epoch 2), train_loss = 1.215, time/batch = 0.352\n",
      "1248/25250 (epoch 2), train_loss = 1.216, time/batch = 0.354\n",
      "1249/25250 (epoch 2), train_loss = 1.324, time/batch = 0.374\n",
      "1250/25250 (epoch 2), train_loss = 1.226, time/batch = 0.551\n",
      "1251/25250 (epoch 2), train_loss = 1.237, time/batch = 0.439\n",
      "1252/25250 (epoch 2), train_loss = 1.245, time/batch = 0.379\n",
      "1253/25250 (epoch 2), train_loss = 1.237, time/batch = 0.376\n",
      "1254/25250 (epoch 2), train_loss = 1.237, time/batch = 0.383\n",
      "1255/25250 (epoch 2), train_loss = 1.277, time/batch = 0.366\n",
      "1256/25250 (epoch 2), train_loss = 1.229, time/batch = 0.365\n",
      "1257/25250 (epoch 2), train_loss = 1.305, time/batch = 0.365\n",
      "1258/25250 (epoch 2), train_loss = 1.318, time/batch = 0.357\n",
      "1259/25250 (epoch 2), train_loss = 1.268, time/batch = 0.369\n",
      "1260/25250 (epoch 2), train_loss = 1.274, time/batch = 0.378\n",
      "1261/25250 (epoch 2), train_loss = 1.267, time/batch = 0.344\n",
      "1262/25250 (epoch 2), train_loss = 1.256, time/batch = 0.361\n",
      "1263/25250 (epoch 2), train_loss = 1.300, time/batch = 0.338\n",
      "1264/25250 (epoch 2), train_loss = 1.244, time/batch = 0.352\n",
      "1265/25250 (epoch 2), train_loss = 1.214, time/batch = 0.363\n",
      "1266/25250 (epoch 2), train_loss = 1.248, time/batch = 0.369\n",
      "1267/25250 (epoch 2), train_loss = 1.227, time/batch = 0.363\n",
      "1268/25250 (epoch 2), train_loss = 1.245, time/batch = 0.371\n",
      "1269/25250 (epoch 2), train_loss = 1.176, time/batch = 0.398\n",
      "1270/25250 (epoch 2), train_loss = 1.236, time/batch = 0.373\n",
      "1271/25250 (epoch 2), train_loss = 1.242, time/batch = 0.385\n",
      "1272/25250 (epoch 2), train_loss = 1.193, time/batch = 0.366\n",
      "1273/25250 (epoch 2), train_loss = 1.268, time/batch = 0.474\n",
      "1274/25250 (epoch 2), train_loss = 1.214, time/batch = 0.377\n",
      "1275/25250 (epoch 2), train_loss = 1.229, time/batch = 0.377\n",
      "1276/25250 (epoch 2), train_loss = 1.194, time/batch = 0.352\n",
      "1277/25250 (epoch 2), train_loss = 1.318, time/batch = 0.340\n",
      "1278/25250 (epoch 2), train_loss = 1.284, time/batch = 0.390\n",
      "1279/25250 (epoch 2), train_loss = 1.202, time/batch = 0.367\n",
      "1280/25250 (epoch 2), train_loss = 1.225, time/batch = 0.354\n",
      "1281/25250 (epoch 2), train_loss = 1.246, time/batch = 0.354\n",
      "1282/25250 (epoch 2), train_loss = 1.246, time/batch = 0.358\n",
      "1283/25250 (epoch 2), train_loss = 1.224, time/batch = 0.356\n",
      "1284/25250 (epoch 2), train_loss = 1.218, time/batch = 0.394\n",
      "1285/25250 (epoch 2), train_loss = 1.259, time/batch = 0.411\n",
      "1286/25250 (epoch 2), train_loss = 1.287, time/batch = 0.383\n",
      "1287/25250 (epoch 2), train_loss = 1.282, time/batch = 0.352\n",
      "1288/25250 (epoch 2), train_loss = 1.268, time/batch = 0.414\n",
      "1289/25250 (epoch 2), train_loss = 1.363, time/batch = 0.359\n",
      "1290/25250 (epoch 2), train_loss = 1.335, time/batch = 0.377\n",
      "1291/25250 (epoch 2), train_loss = 1.366, time/batch = 0.394\n",
      "1292/25250 (epoch 2), train_loss = 1.226, time/batch = 0.363\n",
      "1293/25250 (epoch 2), train_loss = 1.232, time/batch = 0.507\n",
      "1294/25250 (epoch 2), train_loss = 1.252, time/batch = 0.481\n",
      "1295/25250 (epoch 2), train_loss = 1.333, time/batch = 0.383\n",
      "1296/25250 (epoch 2), train_loss = 1.253, time/batch = 0.451\n",
      "1297/25250 (epoch 2), train_loss = 1.243, time/batch = 0.421\n",
      "1298/25250 (epoch 2), train_loss = 1.221, time/batch = 0.366\n",
      "1299/25250 (epoch 2), train_loss = 1.229, time/batch = 0.358\n",
      "1300/25250 (epoch 2), train_loss = 1.303, time/batch = 0.352\n",
      "1301/25250 (epoch 2), train_loss = 1.303, time/batch = 0.380\n",
      "1302/25250 (epoch 2), train_loss = 1.265, time/batch = 0.387\n",
      "1303/25250 (epoch 2), train_loss = 1.325, time/batch = 0.362\n",
      "1304/25250 (epoch 2), train_loss = 1.345, time/batch = 0.390\n",
      "1305/25250 (epoch 2), train_loss = 1.343, time/batch = 0.358\n",
      "1306/25250 (epoch 2), train_loss = 1.334, time/batch = 0.339\n",
      "1307/25250 (epoch 2), train_loss = 1.291, time/batch = 0.333\n",
      "1308/25250 (epoch 2), train_loss = 1.290, time/batch = 0.386\n",
      "1309/25250 (epoch 2), train_loss = 1.362, time/batch = 0.411\n",
      "1310/25250 (epoch 2), train_loss = 1.341, time/batch = 0.411\n",
      "1311/25250 (epoch 2), train_loss = 1.221, time/batch = 0.357\n",
      "1312/25250 (epoch 2), train_loss = 1.160, time/batch = 0.361\n",
      "1313/25250 (epoch 2), train_loss = 1.212, time/batch = 0.389\n",
      "1314/25250 (epoch 2), train_loss = 1.213, time/batch = 0.390\n",
      "1315/25250 (epoch 2), train_loss = 1.291, time/batch = 0.402\n",
      "1316/25250 (epoch 2), train_loss = 1.235, time/batch = 0.370\n",
      "1317/25250 (epoch 2), train_loss = 1.263, time/batch = 0.389\n",
      "1318/25250 (epoch 2), train_loss = 1.266, time/batch = 0.370\n",
      "1319/25250 (epoch 2), train_loss = 1.238, time/batch = 0.381\n",
      "1320/25250 (epoch 2), train_loss = 1.287, time/batch = 0.373\n",
      "1321/25250 (epoch 2), train_loss = 1.279, time/batch = 0.399\n",
      "1322/25250 (epoch 2), train_loss = 1.234, time/batch = 0.430\n",
      "1323/25250 (epoch 2), train_loss = 1.260, time/batch = 0.506\n",
      "1324/25250 (epoch 2), train_loss = 1.272, time/batch = 0.520\n",
      "1325/25250 (epoch 2), train_loss = 1.203, time/batch = 0.461\n",
      "1326/25250 (epoch 2), train_loss = 1.291, time/batch = 0.517\n",
      "1327/25250 (epoch 2), train_loss = 1.283, time/batch = 0.474\n",
      "1328/25250 (epoch 2), train_loss = 1.289, time/batch = 0.453\n",
      "1329/25250 (epoch 2), train_loss = 1.276, time/batch = 0.424\n",
      "1330/25250 (epoch 2), train_loss = 1.288, time/batch = 0.428\n",
      "1331/25250 (epoch 2), train_loss = 1.248, time/batch = 0.400\n",
      "1332/25250 (epoch 2), train_loss = 1.293, time/batch = 0.379\n",
      "1333/25250 (epoch 2), train_loss = 1.199, time/batch = 0.375\n",
      "1334/25250 (epoch 2), train_loss = 1.279, time/batch = 0.373\n",
      "1335/25250 (epoch 2), train_loss = 1.240, time/batch = 0.499\n",
      "1336/25250 (epoch 2), train_loss = 1.330, time/batch = 0.427\n",
      "1337/25250 (epoch 2), train_loss = 1.306, time/batch = 0.425\n",
      "1338/25250 (epoch 2), train_loss = 1.320, time/batch = 0.392\n",
      "1339/25250 (epoch 2), train_loss = 1.290, time/batch = 0.407\n",
      "1340/25250 (epoch 2), train_loss = 1.248, time/batch = 0.414\n",
      "1341/25250 (epoch 2), train_loss = 1.219, time/batch = 0.396\n",
      "1342/25250 (epoch 2), train_loss = 1.288, time/batch = 0.416\n",
      "1343/25250 (epoch 2), train_loss = 1.316, time/batch = 0.466\n",
      "1344/25250 (epoch 2), train_loss = 1.270, time/batch = 0.427\n",
      "1345/25250 (epoch 2), train_loss = 1.226, time/batch = 0.422\n",
      "1346/25250 (epoch 2), train_loss = 1.231, time/batch = 0.394\n",
      "1347/25250 (epoch 2), train_loss = 1.336, time/batch = 0.400\n",
      "1348/25250 (epoch 2), train_loss = 1.263, time/batch = 0.379\n",
      "1349/25250 (epoch 2), train_loss = 1.264, time/batch = 0.402\n",
      "1350/25250 (epoch 2), train_loss = 1.311, time/batch = 0.376\n",
      "1351/25250 (epoch 2), train_loss = 1.273, time/batch = 0.367\n",
      "1352/25250 (epoch 2), train_loss = 1.267, time/batch = 0.378\n",
      "1353/25250 (epoch 2), train_loss = 1.265, time/batch = 0.382\n",
      "1354/25250 (epoch 2), train_loss = 1.237, time/batch = 0.385\n",
      "1355/25250 (epoch 2), train_loss = 1.313, time/batch = 0.387\n",
      "1356/25250 (epoch 2), train_loss = 1.297, time/batch = 0.377\n",
      "1357/25250 (epoch 2), train_loss = 1.262, time/batch = 0.376\n",
      "1358/25250 (epoch 2), train_loss = 1.210, time/batch = 0.388\n",
      "1359/25250 (epoch 2), train_loss = 1.288, time/batch = 0.381\n",
      "1360/25250 (epoch 2), train_loss = 1.217, time/batch = 0.386\n",
      "1361/25250 (epoch 2), train_loss = 1.125, time/batch = 0.404\n",
      "1362/25250 (epoch 2), train_loss = 1.166, time/batch = 0.414\n",
      "1363/25250 (epoch 2), train_loss = 1.211, time/batch = 0.395\n",
      "1364/25250 (epoch 2), train_loss = 1.196, time/batch = 0.402\n",
      "1365/25250 (epoch 2), train_loss = 1.268, time/batch = 0.421\n",
      "1366/25250 (epoch 2), train_loss = 1.169, time/batch = 0.436\n",
      "1367/25250 (epoch 2), train_loss = 1.262, time/batch = 0.433\n",
      "1368/25250 (epoch 2), train_loss = 1.210, time/batch = 0.417\n",
      "1369/25250 (epoch 2), train_loss = 1.230, time/batch = 0.401\n",
      "1370/25250 (epoch 2), train_loss = 1.216, time/batch = 0.375\n",
      "1371/25250 (epoch 2), train_loss = 1.227, time/batch = 0.448\n",
      "1372/25250 (epoch 2), train_loss = 1.218, time/batch = 0.419\n",
      "1373/25250 (epoch 2), train_loss = 1.202, time/batch = 0.396\n",
      "1374/25250 (epoch 2), train_loss = 1.244, time/batch = 0.378\n",
      "1375/25250 (epoch 2), train_loss = 1.232, time/batch = 0.404\n",
      "1376/25250 (epoch 2), train_loss = 1.216, time/batch = 0.452\n",
      "1377/25250 (epoch 2), train_loss = 1.248, time/batch = 0.419\n",
      "1378/25250 (epoch 2), train_loss = 1.189, time/batch = 0.459\n",
      "1379/25250 (epoch 2), train_loss = 1.211, time/batch = 0.417\n",
      "1380/25250 (epoch 2), train_loss = 1.207, time/batch = 0.384\n",
      "1381/25250 (epoch 2), train_loss = 1.157, time/batch = 0.365\n",
      "1382/25250 (epoch 2), train_loss = 1.166, time/batch = 0.406\n",
      "1383/25250 (epoch 2), train_loss = 1.265, time/batch = 0.370\n",
      "1384/25250 (epoch 2), train_loss = 1.246, time/batch = 0.354\n",
      "1385/25250 (epoch 2), train_loss = 1.336, time/batch = 0.394\n",
      "1386/25250 (epoch 2), train_loss = 1.260, time/batch = 0.379\n",
      "1387/25250 (epoch 2), train_loss = 1.220, time/batch = 0.346\n",
      "1388/25250 (epoch 2), train_loss = 1.234, time/batch = 0.389\n",
      "1389/25250 (epoch 2), train_loss = 1.203, time/batch = 0.366\n",
      "1390/25250 (epoch 2), train_loss = 1.181, time/batch = 0.344\n",
      "1391/25250 (epoch 2), train_loss = 1.197, time/batch = 0.346\n",
      "1392/25250 (epoch 2), train_loss = 1.144, time/batch = 0.353\n",
      "1393/25250 (epoch 2), train_loss = 1.159, time/batch = 0.401\n",
      "1394/25250 (epoch 2), train_loss = 1.144, time/batch = 0.418\n",
      "1395/25250 (epoch 2), train_loss = 1.178, time/batch = 0.402\n",
      "1396/25250 (epoch 2), train_loss = 1.188, time/batch = 0.352\n",
      "1397/25250 (epoch 2), train_loss = 1.206, time/batch = 0.363\n",
      "1398/25250 (epoch 2), train_loss = 1.273, time/batch = 0.363\n",
      "1399/25250 (epoch 2), train_loss = 1.277, time/batch = 0.354\n",
      "1400/25250 (epoch 2), train_loss = 1.210, time/batch = 0.373\n",
      "1401/25250 (epoch 2), train_loss = 1.317, time/batch = 0.379\n",
      "1402/25250 (epoch 2), train_loss = 1.179, time/batch = 0.396\n",
      "1403/25250 (epoch 2), train_loss = 1.174, time/batch = 0.378\n",
      "1404/25250 (epoch 2), train_loss = 1.295, time/batch = 0.354\n",
      "1405/25250 (epoch 2), train_loss = 1.298, time/batch = 0.414\n",
      "1406/25250 (epoch 2), train_loss = 1.254, time/batch = 0.391\n",
      "1407/25250 (epoch 2), train_loss = 1.236, time/batch = 0.363\n",
      "1408/25250 (epoch 2), train_loss = 1.248, time/batch = 0.378\n",
      "1409/25250 (epoch 2), train_loss = 1.273, time/batch = 0.361\n",
      "1410/25250 (epoch 2), train_loss = 1.219, time/batch = 0.378\n",
      "1411/25250 (epoch 2), train_loss = 1.196, time/batch = 0.378\n",
      "1412/25250 (epoch 2), train_loss = 1.238, time/batch = 0.356\n",
      "1413/25250 (epoch 2), train_loss = 1.268, time/batch = 0.405\n",
      "1414/25250 (epoch 2), train_loss = 1.282, time/batch = 0.353\n",
      "1415/25250 (epoch 2), train_loss = 1.333, time/batch = 0.424\n",
      "1416/25250 (epoch 2), train_loss = 1.230, time/batch = 0.425\n",
      "1417/25250 (epoch 2), train_loss = 1.235, time/batch = 0.379\n",
      "1418/25250 (epoch 2), train_loss = 1.214, time/batch = 0.453\n",
      "1419/25250 (epoch 2), train_loss = 1.191, time/batch = 0.374\n",
      "1420/25250 (epoch 2), train_loss = 1.289, time/batch = 0.359\n",
      "1421/25250 (epoch 2), train_loss = 1.275, time/batch = 0.380\n",
      "1422/25250 (epoch 2), train_loss = 1.199, time/batch = 0.357\n",
      "1423/25250 (epoch 2), train_loss = 1.231, time/batch = 0.390\n",
      "1424/25250 (epoch 2), train_loss = 1.205, time/batch = 0.363\n",
      "1425/25250 (epoch 2), train_loss = 1.171, time/batch = 0.360\n",
      "1426/25250 (epoch 2), train_loss = 1.161, time/batch = 0.382\n",
      "1427/25250 (epoch 2), train_loss = 1.197, time/batch = 0.350\n",
      "1428/25250 (epoch 2), train_loss = 1.157, time/batch = 0.354\n",
      "1429/25250 (epoch 2), train_loss = 1.283, time/batch = 0.360\n",
      "1430/25250 (epoch 2), train_loss = 1.232, time/batch = 0.353\n",
      "1431/25250 (epoch 2), train_loss = 1.260, time/batch = 0.356\n",
      "1432/25250 (epoch 2), train_loss = 1.230, time/batch = 0.434\n",
      "1433/25250 (epoch 2), train_loss = 1.252, time/batch = 0.378\n",
      "1434/25250 (epoch 2), train_loss = 1.176, time/batch = 0.374\n",
      "1435/25250 (epoch 2), train_loss = 1.164, time/batch = 0.358\n",
      "1436/25250 (epoch 2), train_loss = 1.214, time/batch = 0.369\n",
      "1437/25250 (epoch 2), train_loss = 1.194, time/batch = 0.381\n",
      "1438/25250 (epoch 2), train_loss = 1.204, time/batch = 0.404\n",
      "1439/25250 (epoch 2), train_loss = 1.161, time/batch = 0.378\n",
      "1440/25250 (epoch 2), train_loss = 1.222, time/batch = 0.410\n",
      "1441/25250 (epoch 2), train_loss = 1.129, time/batch = 0.419\n",
      "1442/25250 (epoch 2), train_loss = 1.167, time/batch = 0.388\n",
      "1443/25250 (epoch 2), train_loss = 1.137, time/batch = 0.419\n",
      "1444/25250 (epoch 2), train_loss = 1.210, time/batch = 0.409\n",
      "1445/25250 (epoch 2), train_loss = 1.144, time/batch = 0.414\n",
      "1446/25250 (epoch 2), train_loss = 1.151, time/batch = 0.474\n",
      "1447/25250 (epoch 2), train_loss = 1.181, time/batch = 0.464\n",
      "1448/25250 (epoch 2), train_loss = 1.166, time/batch = 0.456\n",
      "1449/25250 (epoch 2), train_loss = 1.163, time/batch = 0.386\n",
      "1450/25250 (epoch 2), train_loss = 1.144, time/batch = 0.417\n",
      "1451/25250 (epoch 2), train_loss = 1.130, time/batch = 0.530\n",
      "1452/25250 (epoch 2), train_loss = 1.171, time/batch = 0.526\n",
      "1453/25250 (epoch 2), train_loss = 1.194, time/batch = 0.376\n",
      "1454/25250 (epoch 2), train_loss = 1.222, time/batch = 0.384\n",
      "1455/25250 (epoch 2), train_loss = 1.103, time/batch = 0.377\n",
      "1456/25250 (epoch 2), train_loss = 1.153, time/batch = 0.376\n",
      "1457/25250 (epoch 2), train_loss = 1.141, time/batch = 0.358\n",
      "1458/25250 (epoch 2), train_loss = 1.144, time/batch = 0.352\n",
      "1459/25250 (epoch 2), train_loss = 1.228, time/batch = 0.356\n",
      "1460/25250 (epoch 2), train_loss = 1.217, time/batch = 0.357\n",
      "1461/25250 (epoch 2), train_loss = 1.256, time/batch = 0.353\n",
      "1462/25250 (epoch 2), train_loss = 1.177, time/batch = 0.519\n",
      "1463/25250 (epoch 2), train_loss = 1.234, time/batch = 0.433\n",
      "1464/25250 (epoch 2), train_loss = 1.293, time/batch = 0.401\n",
      "1465/25250 (epoch 2), train_loss = 1.248, time/batch = 0.536\n",
      "1466/25250 (epoch 2), train_loss = 1.269, time/batch = 0.462\n",
      "1467/25250 (epoch 2), train_loss = 1.186, time/batch = 0.400\n",
      "1468/25250 (epoch 2), train_loss = 1.216, time/batch = 0.363\n",
      "1469/25250 (epoch 2), train_loss = 1.168, time/batch = 0.358\n",
      "1470/25250 (epoch 2), train_loss = 1.196, time/batch = 0.352\n",
      "1471/25250 (epoch 2), train_loss = 1.158, time/batch = 0.358\n",
      "1472/25250 (epoch 2), train_loss = 1.205, time/batch = 0.486\n",
      "1473/25250 (epoch 2), train_loss = 1.148, time/batch = 0.476\n",
      "1474/25250 (epoch 2), train_loss = 1.188, time/batch = 0.386\n",
      "1475/25250 (epoch 2), train_loss = 1.184, time/batch = 0.361\n",
      "1476/25250 (epoch 2), train_loss = 1.290, time/batch = 0.511\n",
      "1477/25250 (epoch 2), train_loss = 1.268, time/batch = 0.517\n",
      "1478/25250 (epoch 2), train_loss = 1.243, time/batch = 0.399\n",
      "1479/25250 (epoch 2), train_loss = 1.234, time/batch = 0.400\n",
      "1480/25250 (epoch 2), train_loss = 1.249, time/batch = 0.408\n",
      "1481/25250 (epoch 2), train_loss = 1.146, time/batch = 0.378\n",
      "1482/25250 (epoch 2), train_loss = 1.229, time/batch = 0.359\n",
      "1483/25250 (epoch 2), train_loss = 1.219, time/batch = 0.348\n",
      "1484/25250 (epoch 2), train_loss = 1.154, time/batch = 0.373\n",
      "1485/25250 (epoch 2), train_loss = 1.236, time/batch = 0.459\n",
      "1486/25250 (epoch 2), train_loss = 1.189, time/batch = 0.437\n",
      "1487/25250 (epoch 2), train_loss = 1.246, time/batch = 0.370\n",
      "1488/25250 (epoch 2), train_loss = 1.211, time/batch = 0.378\n",
      "1489/25250 (epoch 2), train_loss = 1.221, time/batch = 0.429\n",
      "1490/25250 (epoch 2), train_loss = 1.170, time/batch = 0.492\n",
      "1491/25250 (epoch 2), train_loss = 1.173, time/batch = 0.406\n",
      "1492/25250 (epoch 2), train_loss = 1.223, time/batch = 0.472\n",
      "1493/25250 (epoch 2), train_loss = 1.183, time/batch = 0.435\n",
      "1494/25250 (epoch 2), train_loss = 1.240, time/batch = 0.469\n",
      "1495/25250 (epoch 2), train_loss = 1.191, time/batch = 0.506\n",
      "1496/25250 (epoch 2), train_loss = 1.253, time/batch = 0.514\n",
      "1497/25250 (epoch 2), train_loss = 1.270, time/batch = 0.478\n",
      "1498/25250 (epoch 2), train_loss = 1.174, time/batch = 0.479\n",
      "1499/25250 (epoch 2), train_loss = 1.222, time/batch = 0.462\n",
      "1500/25250 (epoch 2), train_loss = 1.282, time/batch = 0.467\n",
      "1501/25250 (epoch 2), train_loss = 1.185, time/batch = 0.483\n",
      "1502/25250 (epoch 2), train_loss = 1.253, time/batch = 0.439\n",
      "1503/25250 (epoch 2), train_loss = 1.241, time/batch = 0.420\n",
      "1504/25250 (epoch 2), train_loss = 1.243, time/batch = 0.406\n",
      "1505/25250 (epoch 2), train_loss = 1.219, time/batch = 0.388\n",
      "1506/25250 (epoch 2), train_loss = 1.219, time/batch = 0.384\n",
      "1507/25250 (epoch 2), train_loss = 1.224, time/batch = 0.408\n",
      "1508/25250 (epoch 2), train_loss = 1.163, time/batch = 0.432\n",
      "1509/25250 (epoch 2), train_loss = 1.167, time/batch = 0.375\n",
      "1510/25250 (epoch 2), train_loss = 1.213, time/batch = 0.402\n",
      "1511/25250 (epoch 2), train_loss = 1.202, time/batch = 0.394\n",
      "1512/25250 (epoch 2), train_loss = 1.171, time/batch = 0.405\n",
      "1513/25250 (epoch 2), train_loss = 1.195, time/batch = 0.377\n",
      "1514/25250 (epoch 2), train_loss = 1.293, time/batch = 0.383\n",
      "1515/25250 (epoch 3), train_loss = 1.574, time/batch = 0.359\n",
      "1516/25250 (epoch 3), train_loss = 1.219, time/batch = 0.401\n",
      "1517/25250 (epoch 3), train_loss = 1.232, time/batch = 0.433\n",
      "1518/25250 (epoch 3), train_loss = 1.209, time/batch = 0.440\n",
      "1519/25250 (epoch 3), train_loss = 1.266, time/batch = 0.408\n",
      "1520/25250 (epoch 3), train_loss = 1.229, time/batch = 0.357\n",
      "1521/25250 (epoch 3), train_loss = 1.226, time/batch = 0.385\n",
      "1522/25250 (epoch 3), train_loss = 1.182, time/batch = 0.402\n",
      "1523/25250 (epoch 3), train_loss = 1.184, time/batch = 0.351\n",
      "1524/25250 (epoch 3), train_loss = 1.154, time/batch = 0.354\n",
      "1525/25250 (epoch 3), train_loss = 1.222, time/batch = 0.367\n",
      "1526/25250 (epoch 3), train_loss = 1.192, time/batch = 0.404\n",
      "1527/25250 (epoch 3), train_loss = 1.150, time/batch = 0.386\n",
      "1528/25250 (epoch 3), train_loss = 1.247, time/batch = 0.379\n",
      "1529/25250 (epoch 3), train_loss = 1.285, time/batch = 0.358\n",
      "1530/25250 (epoch 3), train_loss = 1.264, time/batch = 0.358\n",
      "1531/25250 (epoch 3), train_loss = 1.268, time/batch = 0.366\n",
      "1532/25250 (epoch 3), train_loss = 1.317, time/batch = 0.362\n",
      "1533/25250 (epoch 3), train_loss = 1.221, time/batch = 0.428\n",
      "1534/25250 (epoch 3), train_loss = 1.184, time/batch = 0.360\n",
      "1535/25250 (epoch 3), train_loss = 1.141, time/batch = 0.384\n",
      "1536/25250 (epoch 3), train_loss = 1.184, time/batch = 0.368\n",
      "1537/25250 (epoch 3), train_loss = 1.140, time/batch = 0.355\n",
      "1538/25250 (epoch 3), train_loss = 1.228, time/batch = 0.358\n",
      "1539/25250 (epoch 3), train_loss = 1.190, time/batch = 0.395\n",
      "1540/25250 (epoch 3), train_loss = 1.131, time/batch = 0.355\n",
      "1541/25250 (epoch 3), train_loss = 1.166, time/batch = 0.353\n",
      "1542/25250 (epoch 3), train_loss = 1.233, time/batch = 0.354\n",
      "1543/25250 (epoch 3), train_loss = 1.251, time/batch = 0.353\n",
      "1544/25250 (epoch 3), train_loss = 1.154, time/batch = 0.360\n",
      "1545/25250 (epoch 3), train_loss = 1.167, time/batch = 0.371\n",
      "1546/25250 (epoch 3), train_loss = 1.135, time/batch = 0.369\n",
      "1547/25250 (epoch 3), train_loss = 1.112, time/batch = 0.366\n",
      "1548/25250 (epoch 3), train_loss = 1.178, time/batch = 0.426\n",
      "1549/25250 (epoch 3), train_loss = 1.169, time/batch = 0.458\n",
      "1550/25250 (epoch 3), train_loss = 1.262, time/batch = 0.410\n",
      "1551/25250 (epoch 3), train_loss = 1.202, time/batch = 0.376\n",
      "1552/25250 (epoch 3), train_loss = 1.170, time/batch = 0.365\n",
      "1553/25250 (epoch 3), train_loss = 1.137, time/batch = 0.376\n",
      "1554/25250 (epoch 3), train_loss = 1.176, time/batch = 0.435\n",
      "1555/25250 (epoch 3), train_loss = 1.137, time/batch = 0.412\n",
      "1556/25250 (epoch 3), train_loss = 1.248, time/batch = 0.363\n",
      "1557/25250 (epoch 3), train_loss = 1.219, time/batch = 0.387\n",
      "1558/25250 (epoch 3), train_loss = 1.209, time/batch = 0.377\n",
      "1559/25250 (epoch 3), train_loss = 1.071, time/batch = 0.390\n",
      "1560/25250 (epoch 3), train_loss = 1.107, time/batch = 0.368\n",
      "1561/25250 (epoch 3), train_loss = 1.140, time/batch = 0.340\n",
      "1562/25250 (epoch 3), train_loss = 1.198, time/batch = 0.350\n",
      "1563/25250 (epoch 3), train_loss = 1.173, time/batch = 0.395\n",
      "1564/25250 (epoch 3), train_loss = 1.268, time/batch = 0.409\n",
      "1565/25250 (epoch 3), train_loss = 1.222, time/batch = 0.358\n",
      "1566/25250 (epoch 3), train_loss = 1.176, time/batch = 0.362\n",
      "1567/25250 (epoch 3), train_loss = 1.164, time/batch = 0.441\n",
      "1568/25250 (epoch 3), train_loss = 1.250, time/batch = 0.423\n",
      "1569/25250 (epoch 3), train_loss = 1.143, time/batch = 0.393\n",
      "1570/25250 (epoch 3), train_loss = 1.296, time/batch = 0.384\n",
      "1571/25250 (epoch 3), train_loss = 1.223, time/batch = 0.344\n",
      "1572/25250 (epoch 3), train_loss = 1.205, time/batch = 0.367\n",
      "1573/25250 (epoch 3), train_loss = 1.177, time/batch = 0.378\n",
      "1574/25250 (epoch 3), train_loss = 1.153, time/batch = 0.434\n",
      "1575/25250 (epoch 3), train_loss = 1.259, time/batch = 0.399\n",
      "1576/25250 (epoch 3), train_loss = 1.285, time/batch = 0.402\n",
      "1577/25250 (epoch 3), train_loss = 1.218, time/batch = 0.374\n",
      "1578/25250 (epoch 3), train_loss = 1.240, time/batch = 0.379\n",
      "1579/25250 (epoch 3), train_loss = 1.228, time/batch = 0.367\n",
      "1580/25250 (epoch 3), train_loss = 1.211, time/batch = 0.329\n",
      "1581/25250 (epoch 3), train_loss = 1.296, time/batch = 0.339\n",
      "1582/25250 (epoch 3), train_loss = 1.283, time/batch = 0.358\n",
      "1583/25250 (epoch 3), train_loss = 1.281, time/batch = 0.378\n",
      "1584/25250 (epoch 3), train_loss = 1.309, time/batch = 0.387\n",
      "1585/25250 (epoch 3), train_loss = 1.263, time/batch = 0.376\n",
      "1586/25250 (epoch 3), train_loss = 1.204, time/batch = 0.369\n",
      "1587/25250 (epoch 3), train_loss = 1.307, time/batch = 0.370\n",
      "1588/25250 (epoch 3), train_loss = 1.206, time/batch = 0.366\n",
      "1589/25250 (epoch 3), train_loss = 1.207, time/batch = 0.368\n",
      "1590/25250 (epoch 3), train_loss = 1.208, time/batch = 0.357\n",
      "1591/25250 (epoch 3), train_loss = 1.216, time/batch = 0.401\n",
      "1592/25250 (epoch 3), train_loss = 1.250, time/batch = 0.401\n",
      "1593/25250 (epoch 3), train_loss = 1.190, time/batch = 0.390\n",
      "1594/25250 (epoch 3), train_loss = 1.227, time/batch = 0.375\n",
      "1595/25250 (epoch 3), train_loss = 1.258, time/batch = 0.399\n",
      "1596/25250 (epoch 3), train_loss = 1.180, time/batch = 0.377\n",
      "1597/25250 (epoch 3), train_loss = 1.248, time/batch = 0.395\n",
      "1598/25250 (epoch 3), train_loss = 1.236, time/batch = 0.377\n",
      "1599/25250 (epoch 3), train_loss = 1.218, time/batch = 0.415\n",
      "1600/25250 (epoch 3), train_loss = 1.163, time/batch = 0.414\n",
      "1601/25250 (epoch 3), train_loss = 1.193, time/batch = 0.473\n",
      "1602/25250 (epoch 3), train_loss = 1.151, time/batch = 0.403\n",
      "1603/25250 (epoch 3), train_loss = 1.276, time/batch = 0.414\n",
      "1604/25250 (epoch 3), train_loss = 1.339, time/batch = 0.416\n",
      "1605/25250 (epoch 3), train_loss = 1.242, time/batch = 0.408\n",
      "1606/25250 (epoch 3), train_loss = 1.168, time/batch = 0.365\n",
      "1607/25250 (epoch 3), train_loss = 1.199, time/batch = 0.388\n",
      "1608/25250 (epoch 3), train_loss = 1.233, time/batch = 0.359\n",
      "1609/25250 (epoch 3), train_loss = 1.186, time/batch = 0.372\n",
      "1610/25250 (epoch 3), train_loss = 1.251, time/batch = 0.357\n",
      "1611/25250 (epoch 3), train_loss = 1.204, time/batch = 0.345\n",
      "1612/25250 (epoch 3), train_loss = 1.161, time/batch = 0.368\n",
      "1613/25250 (epoch 3), train_loss = 1.221, time/batch = 0.391\n",
      "1614/25250 (epoch 3), train_loss = 1.157, time/batch = 0.360\n",
      "1615/25250 (epoch 3), train_loss = 1.264, time/batch = 0.391\n",
      "1616/25250 (epoch 3), train_loss = 1.273, time/batch = 0.384\n",
      "1617/25250 (epoch 3), train_loss = 1.274, time/batch = 0.358\n",
      "1618/25250 (epoch 3), train_loss = 1.273, time/batch = 0.399\n",
      "1619/25250 (epoch 3), train_loss = 1.187, time/batch = 0.382\n",
      "1620/25250 (epoch 3), train_loss = 1.212, time/batch = 0.364\n",
      "1621/25250 (epoch 3), train_loss = 1.160, time/batch = 0.470\n",
      "1622/25250 (epoch 3), train_loss = 1.114, time/batch = 0.467\n",
      "1623/25250 (epoch 3), train_loss = 1.173, time/batch = 0.451\n",
      "1624/25250 (epoch 3), train_loss = 1.188, time/batch = 0.434\n",
      "1625/25250 (epoch 3), train_loss = 1.132, time/batch = 0.415\n",
      "1626/25250 (epoch 3), train_loss = 1.215, time/batch = 0.449\n",
      "1627/25250 (epoch 3), train_loss = 1.260, time/batch = 0.433\n",
      "1628/25250 (epoch 3), train_loss = 1.209, time/batch = 0.382\n",
      "1629/25250 (epoch 3), train_loss = 1.172, time/batch = 0.366\n",
      "1630/25250 (epoch 3), train_loss = 1.146, time/batch = 0.354\n",
      "1631/25250 (epoch 3), train_loss = 1.184, time/batch = 0.385\n",
      "1632/25250 (epoch 3), train_loss = 1.173, time/batch = 0.357\n",
      "1633/25250 (epoch 3), train_loss = 1.157, time/batch = 0.385\n",
      "1634/25250 (epoch 3), train_loss = 1.155, time/batch = 0.364\n",
      "1635/25250 (epoch 3), train_loss = 1.184, time/batch = 0.386\n",
      "1636/25250 (epoch 3), train_loss = 1.174, time/batch = 0.375\n",
      "1637/25250 (epoch 3), train_loss = 1.217, time/batch = 0.339\n",
      "1638/25250 (epoch 3), train_loss = 1.233, time/batch = 0.349\n",
      "1639/25250 (epoch 3), train_loss = 1.181, time/batch = 0.369\n",
      "1640/25250 (epoch 3), train_loss = 1.193, time/batch = 0.390\n",
      "1641/25250 (epoch 3), train_loss = 1.075, time/batch = 0.373\n",
      "1642/25250 (epoch 3), train_loss = 1.238, time/batch = 0.379\n",
      "1643/25250 (epoch 3), train_loss = 1.223, time/batch = 0.356\n",
      "1644/25250 (epoch 3), train_loss = 1.217, time/batch = 0.351\n",
      "1645/25250 (epoch 3), train_loss = 1.181, time/batch = 0.357\n",
      "1646/25250 (epoch 3), train_loss = 1.194, time/batch = 0.431\n",
      "1647/25250 (epoch 3), train_loss = 1.188, time/batch = 0.357\n",
      "1648/25250 (epoch 3), train_loss = 1.128, time/batch = 0.351\n",
      "1649/25250 (epoch 3), train_loss = 1.136, time/batch = 0.354\n",
      "1650/25250 (epoch 3), train_loss = 1.179, time/batch = 0.398\n",
      "1651/25250 (epoch 3), train_loss = 1.158, time/batch = 0.363\n",
      "1652/25250 (epoch 3), train_loss = 1.181, time/batch = 0.383\n",
      "1653/25250 (epoch 3), train_loss = 1.223, time/batch = 0.409\n",
      "1654/25250 (epoch 3), train_loss = 1.185, time/batch = 0.419\n",
      "1655/25250 (epoch 3), train_loss = 1.250, time/batch = 0.408\n",
      "1656/25250 (epoch 3), train_loss = 1.198, time/batch = 0.399\n",
      "1657/25250 (epoch 3), train_loss = 1.193, time/batch = 0.388\n",
      "1658/25250 (epoch 3), train_loss = 1.181, time/batch = 0.401\n",
      "1659/25250 (epoch 3), train_loss = 1.213, time/batch = 0.391\n",
      "1660/25250 (epoch 3), train_loss = 1.260, time/batch = 0.385\n",
      "1661/25250 (epoch 3), train_loss = 1.226, time/batch = 0.412\n",
      "1662/25250 (epoch 3), train_loss = 1.261, time/batch = 0.360\n",
      "1663/25250 (epoch 3), train_loss = 1.222, time/batch = 0.382\n",
      "1664/25250 (epoch 3), train_loss = 1.263, time/batch = 0.397\n",
      "1665/25250 (epoch 3), train_loss = 1.163, time/batch = 0.410\n",
      "1666/25250 (epoch 3), train_loss = 1.267, time/batch = 0.385\n",
      "1667/25250 (epoch 3), train_loss = 1.230, time/batch = 0.370\n",
      "1668/25250 (epoch 3), train_loss = 1.215, time/batch = 0.372\n",
      "1669/25250 (epoch 3), train_loss = 1.155, time/batch = 0.370\n",
      "1670/25250 (epoch 3), train_loss = 1.267, time/batch = 0.360\n",
      "1671/25250 (epoch 3), train_loss = 1.198, time/batch = 0.383\n",
      "1672/25250 (epoch 3), train_loss = 1.148, time/batch = 0.374\n",
      "1673/25250 (epoch 3), train_loss = 1.195, time/batch = 0.390\n",
      "1674/25250 (epoch 3), train_loss = 1.154, time/batch = 0.384\n",
      "1675/25250 (epoch 3), train_loss = 1.265, time/batch = 0.368\n",
      "1676/25250 (epoch 3), train_loss = 1.168, time/batch = 0.365\n",
      "1677/25250 (epoch 3), train_loss = 1.214, time/batch = 0.368\n",
      "1678/25250 (epoch 3), train_loss = 1.207, time/batch = 0.365\n",
      "1679/25250 (epoch 3), train_loss = 1.162, time/batch = 0.369\n",
      "1680/25250 (epoch 3), train_loss = 1.223, time/batch = 0.440\n",
      "1681/25250 (epoch 3), train_loss = 1.307, time/batch = 0.387\n",
      "1682/25250 (epoch 3), train_loss = 1.275, time/batch = 0.351\n",
      "1683/25250 (epoch 3), train_loss = 1.225, time/batch = 0.357\n",
      "1684/25250 (epoch 3), train_loss = 1.233, time/batch = 0.400\n",
      "1685/25250 (epoch 3), train_loss = 1.155, time/batch = 0.375\n",
      "1686/25250 (epoch 3), train_loss = 1.119, time/batch = 0.409\n",
      "1687/25250 (epoch 3), train_loss = 1.076, time/batch = 0.387\n",
      "1688/25250 (epoch 3), train_loss = 1.126, time/batch = 0.385\n",
      "1689/25250 (epoch 3), train_loss = 1.165, time/batch = 0.349\n",
      "1690/25250 (epoch 3), train_loss = 1.132, time/batch = 0.350\n",
      "1691/25250 (epoch 3), train_loss = 1.148, time/batch = 0.354\n",
      "1692/25250 (epoch 3), train_loss = 1.235, time/batch = 0.396\n",
      "1693/25250 (epoch 3), train_loss = 1.219, time/batch = 0.444\n",
      "1694/25250 (epoch 3), train_loss = 1.249, time/batch = 0.414\n",
      "1695/25250 (epoch 3), train_loss = 1.169, time/batch = 0.373\n",
      "1696/25250 (epoch 3), train_loss = 1.201, time/batch = 0.461\n",
      "1697/25250 (epoch 3), train_loss = 1.209, time/batch = 0.353\n",
      "1698/25250 (epoch 3), train_loss = 1.109, time/batch = 0.376\n",
      "1699/25250 (epoch 3), train_loss = 1.150, time/batch = 0.344\n",
      "1700/25250 (epoch 3), train_loss = 1.313, time/batch = 0.409\n",
      "1701/25250 (epoch 3), train_loss = 1.222, time/batch = 0.422\n",
      "1702/25250 (epoch 3), train_loss = 1.230, time/batch = 0.401\n",
      "1703/25250 (epoch 3), train_loss = 1.211, time/batch = 0.408\n",
      "1704/25250 (epoch 3), train_loss = 1.207, time/batch = 0.398\n",
      "1705/25250 (epoch 3), train_loss = 1.280, time/batch = 0.379\n",
      "1706/25250 (epoch 3), train_loss = 1.289, time/batch = 0.386\n",
      "1707/25250 (epoch 3), train_loss = 1.288, time/batch = 0.379\n",
      "1708/25250 (epoch 3), train_loss = 1.308, time/batch = 0.374\n",
      "1709/25250 (epoch 3), train_loss = 1.245, time/batch = 0.372\n",
      "1710/25250 (epoch 3), train_loss = 1.189, time/batch = 0.362\n",
      "1711/25250 (epoch 3), train_loss = 1.126, time/batch = 0.363\n",
      "1712/25250 (epoch 3), train_loss = 1.179, time/batch = 0.399\n",
      "1713/25250 (epoch 3), train_loss = 1.119, time/batch = 0.428\n",
      "1714/25250 (epoch 3), train_loss = 1.130, time/batch = 0.381\n",
      "1715/25250 (epoch 3), train_loss = 1.141, time/batch = 0.392\n",
      "1716/25250 (epoch 3), train_loss = 1.065, time/batch = 0.400\n",
      "1717/25250 (epoch 3), train_loss = 1.069, time/batch = 0.376\n",
      "1718/25250 (epoch 3), train_loss = 1.104, time/batch = 0.342\n",
      "1719/25250 (epoch 3), train_loss = 1.136, time/batch = 0.341\n",
      "1720/25250 (epoch 3), train_loss = 1.163, time/batch = 0.352\n",
      "1721/25250 (epoch 3), train_loss = 1.186, time/batch = 0.458\n",
      "1722/25250 (epoch 3), train_loss = 1.154, time/batch = 0.421\n",
      "1723/25250 (epoch 3), train_loss = 1.141, time/batch = 0.375\n",
      "1724/25250 (epoch 3), train_loss = 1.181, time/batch = 0.382\n",
      "1725/25250 (epoch 3), train_loss = 1.169, time/batch = 0.423\n",
      "1726/25250 (epoch 3), train_loss = 1.132, time/batch = 0.382\n",
      "1727/25250 (epoch 3), train_loss = 1.143, time/batch = 0.396\n",
      "1728/25250 (epoch 3), train_loss = 1.154, time/batch = 0.424\n",
      "1729/25250 (epoch 3), train_loss = 1.113, time/batch = 0.418\n",
      "1730/25250 (epoch 3), train_loss = 1.115, time/batch = 0.427\n",
      "1731/25250 (epoch 3), train_loss = 1.130, time/batch = 0.395\n",
      "1732/25250 (epoch 3), train_loss = 1.155, time/batch = 0.375\n",
      "1733/25250 (epoch 3), train_loss = 1.132, time/batch = 0.373\n",
      "1734/25250 (epoch 3), train_loss = 1.238, time/batch = 0.369\n",
      "1735/25250 (epoch 3), train_loss = 1.179, time/batch = 0.396\n",
      "1736/25250 (epoch 3), train_loss = 1.200, time/batch = 0.389\n",
      "1737/25250 (epoch 3), train_loss = 1.192, time/batch = 0.378\n",
      "1738/25250 (epoch 3), train_loss = 1.160, time/batch = 0.430\n",
      "1739/25250 (epoch 3), train_loss = 1.107, time/batch = 0.480\n",
      "1740/25250 (epoch 3), train_loss = 1.101, time/batch = 0.403\n",
      "1741/25250 (epoch 3), train_loss = 1.219, time/batch = 0.406\n",
      "1742/25250 (epoch 3), train_loss = 1.205, time/batch = 0.385\n",
      "1743/25250 (epoch 3), train_loss = 1.122, time/batch = 0.443\n",
      "1744/25250 (epoch 3), train_loss = 1.195, time/batch = 0.378\n",
      "1745/25250 (epoch 3), train_loss = 1.112, time/batch = 0.411\n",
      "1746/25250 (epoch 3), train_loss = 1.151, time/batch = 0.446\n",
      "1747/25250 (epoch 3), train_loss = 1.179, time/batch = 0.406\n",
      "1748/25250 (epoch 3), train_loss = 1.210, time/batch = 0.364\n",
      "1749/25250 (epoch 3), train_loss = 1.160, time/batch = 0.401\n",
      "1750/25250 (epoch 3), train_loss = 1.175, time/batch = 0.380\n",
      "1751/25250 (epoch 3), train_loss = 1.172, time/batch = 0.367\n",
      "1752/25250 (epoch 3), train_loss = 1.134, time/batch = 0.397\n",
      "1753/25250 (epoch 3), train_loss = 1.133, time/batch = 0.370\n",
      "1754/25250 (epoch 3), train_loss = 1.235, time/batch = 0.374\n",
      "1755/25250 (epoch 3), train_loss = 1.133, time/batch = 0.439\n",
      "1756/25250 (epoch 3), train_loss = 1.150, time/batch = 0.443\n",
      "1757/25250 (epoch 3), train_loss = 1.171, time/batch = 0.448\n",
      "1758/25250 (epoch 3), train_loss = 1.159, time/batch = 0.441\n",
      "1759/25250 (epoch 3), train_loss = 1.170, time/batch = 0.406\n",
      "1760/25250 (epoch 3), train_loss = 1.194, time/batch = 0.377\n",
      "1761/25250 (epoch 3), train_loss = 1.154, time/batch = 0.383\n",
      "1762/25250 (epoch 3), train_loss = 1.233, time/batch = 0.430\n",
      "1763/25250 (epoch 3), train_loss = 1.244, time/batch = 0.367\n",
      "1764/25250 (epoch 3), train_loss = 1.188, time/batch = 0.374\n",
      "1765/25250 (epoch 3), train_loss = 1.192, time/batch = 0.391\n",
      "1766/25250 (epoch 3), train_loss = 1.199, time/batch = 0.384\n",
      "1767/25250 (epoch 3), train_loss = 1.181, time/batch = 0.373\n",
      "1768/25250 (epoch 3), train_loss = 1.219, time/batch = 0.406\n",
      "1769/25250 (epoch 3), train_loss = 1.171, time/batch = 0.382\n",
      "1770/25250 (epoch 3), train_loss = 1.137, time/batch = 0.364\n",
      "1771/25250 (epoch 3), train_loss = 1.164, time/batch = 0.422\n",
      "1772/25250 (epoch 3), train_loss = 1.153, time/batch = 0.371\n",
      "1773/25250 (epoch 3), train_loss = 1.174, time/batch = 0.467\n",
      "1774/25250 (epoch 3), train_loss = 1.112, time/batch = 0.416\n",
      "1775/25250 (epoch 3), train_loss = 1.178, time/batch = 0.418\n",
      "1776/25250 (epoch 3), train_loss = 1.176, time/batch = 0.389\n",
      "1777/25250 (epoch 3), train_loss = 1.132, time/batch = 0.385\n",
      "1778/25250 (epoch 3), train_loss = 1.203, time/batch = 0.389\n",
      "1779/25250 (epoch 3), train_loss = 1.147, time/batch = 0.394\n",
      "1780/25250 (epoch 3), train_loss = 1.162, time/batch = 0.368\n",
      "1781/25250 (epoch 3), train_loss = 1.124, time/batch = 0.406\n",
      "1782/25250 (epoch 3), train_loss = 1.244, time/batch = 0.425\n",
      "1783/25250 (epoch 3), train_loss = 1.201, time/batch = 0.456\n",
      "1784/25250 (epoch 3), train_loss = 1.141, time/batch = 0.419\n",
      "1785/25250 (epoch 3), train_loss = 1.159, time/batch = 0.437\n",
      "1786/25250 (epoch 3), train_loss = 1.169, time/batch = 0.382\n",
      "1787/25250 (epoch 3), train_loss = 1.172, time/batch = 0.392\n",
      "1788/25250 (epoch 3), train_loss = 1.150, time/batch = 0.420\n",
      "1789/25250 (epoch 3), train_loss = 1.148, time/batch = 0.378\n",
      "1790/25250 (epoch 3), train_loss = 1.175, time/batch = 0.407\n",
      "1791/25250 (epoch 3), train_loss = 1.208, time/batch = 0.383\n",
      "1792/25250 (epoch 3), train_loss = 1.211, time/batch = 0.378\n",
      "1793/25250 (epoch 3), train_loss = 1.197, time/batch = 0.441\n",
      "1794/25250 (epoch 3), train_loss = 1.305, time/batch = 0.412\n",
      "1795/25250 (epoch 3), train_loss = 1.257, time/batch = 0.427\n",
      "1796/25250 (epoch 3), train_loss = 1.292, time/batch = 0.395\n",
      "1797/25250 (epoch 3), train_loss = 1.155, time/batch = 0.369\n",
      "1798/25250 (epoch 3), train_loss = 1.165, time/batch = 0.436\n",
      "1799/25250 (epoch 3), train_loss = 1.185, time/batch = 0.363\n",
      "1800/25250 (epoch 3), train_loss = 1.277, time/batch = 0.365\n",
      "1801/25250 (epoch 3), train_loss = 1.202, time/batch = 0.418\n",
      "1802/25250 (epoch 3), train_loss = 1.174, time/batch = 0.369\n",
      "1803/25250 (epoch 3), train_loss = 1.160, time/batch = 0.395\n",
      "1804/25250 (epoch 3), train_loss = 1.180, time/batch = 0.379\n",
      "1805/25250 (epoch 3), train_loss = 1.240, time/batch = 0.408\n",
      "1806/25250 (epoch 3), train_loss = 1.236, time/batch = 0.379\n",
      "1807/25250 (epoch 3), train_loss = 1.196, time/batch = 0.376\n",
      "1808/25250 (epoch 3), train_loss = 1.256, time/batch = 0.416\n",
      "1809/25250 (epoch 3), train_loss = 1.275, time/batch = 0.417\n",
      "1810/25250 (epoch 3), train_loss = 1.264, time/batch = 0.392\n",
      "1811/25250 (epoch 3), train_loss = 1.261, time/batch = 0.348\n",
      "1812/25250 (epoch 3), train_loss = 1.236, time/batch = 0.379\n",
      "1813/25250 (epoch 3), train_loss = 1.225, time/batch = 0.377\n",
      "1814/25250 (epoch 3), train_loss = 1.303, time/batch = 0.406\n",
      "1815/25250 (epoch 3), train_loss = 1.272, time/batch = 0.397\n",
      "1816/25250 (epoch 3), train_loss = 1.151, time/batch = 0.399\n",
      "1817/25250 (epoch 3), train_loss = 1.104, time/batch = 0.383\n",
      "1818/25250 (epoch 3), train_loss = 1.149, time/batch = 0.413\n",
      "1819/25250 (epoch 3), train_loss = 1.156, time/batch = 0.400\n",
      "1820/25250 (epoch 3), train_loss = 1.235, time/batch = 0.422\n",
      "1821/25250 (epoch 3), train_loss = 1.184, time/batch = 0.439\n",
      "1822/25250 (epoch 3), train_loss = 1.200, time/batch = 0.360\n",
      "1823/25250 (epoch 3), train_loss = 1.196, time/batch = 0.370\n",
      "1824/25250 (epoch 3), train_loss = 1.166, time/batch = 0.346\n",
      "1825/25250 (epoch 3), train_loss = 1.235, time/batch = 0.370\n",
      "1826/25250 (epoch 3), train_loss = 1.213, time/batch = 0.371\n",
      "1827/25250 (epoch 3), train_loss = 1.175, time/batch = 0.375\n",
      "1828/25250 (epoch 3), train_loss = 1.202, time/batch = 0.372\n",
      "1829/25250 (epoch 3), train_loss = 1.227, time/batch = 0.361\n",
      "1830/25250 (epoch 3), train_loss = 1.143, time/batch = 0.445\n",
      "1831/25250 (epoch 3), train_loss = 1.236, time/batch = 0.447\n",
      "1832/25250 (epoch 3), train_loss = 1.231, time/batch = 0.402\n",
      "1833/25250 (epoch 3), train_loss = 1.222, time/batch = 0.399\n",
      "1834/25250 (epoch 3), train_loss = 1.221, time/batch = 0.397\n",
      "1835/25250 (epoch 3), train_loss = 1.224, time/batch = 0.377\n",
      "1836/25250 (epoch 3), train_loss = 1.190, time/batch = 0.393\n",
      "1837/25250 (epoch 3), train_loss = 1.234, time/batch = 0.365\n",
      "1838/25250 (epoch 3), train_loss = 1.143, time/batch = 0.376\n",
      "1839/25250 (epoch 3), train_loss = 1.221, time/batch = 0.395\n",
      "1840/25250 (epoch 3), train_loss = 1.177, time/batch = 0.359\n",
      "1841/25250 (epoch 3), train_loss = 1.259, time/batch = 0.375\n",
      "1842/25250 (epoch 3), train_loss = 1.243, time/batch = 0.368\n",
      "1843/25250 (epoch 3), train_loss = 1.243, time/batch = 0.385\n",
      "1844/25250 (epoch 3), train_loss = 1.228, time/batch = 0.426\n",
      "1845/25250 (epoch 3), train_loss = 1.182, time/batch = 0.390\n",
      "1846/25250 (epoch 3), train_loss = 1.144, time/batch = 0.365\n",
      "1847/25250 (epoch 3), train_loss = 1.224, time/batch = 0.362\n",
      "1848/25250 (epoch 3), train_loss = 1.252, time/batch = 0.379\n",
      "1849/25250 (epoch 3), train_loss = 1.211, time/batch = 0.380\n",
      "1850/25250 (epoch 3), train_loss = 1.164, time/batch = 0.382\n",
      "1851/25250 (epoch 3), train_loss = 1.168, time/batch = 0.384\n",
      "1852/25250 (epoch 3), train_loss = 1.267, time/batch = 0.384\n",
      "1853/25250 (epoch 3), train_loss = 1.196, time/batch = 0.361\n",
      "1854/25250 (epoch 3), train_loss = 1.199, time/batch = 0.399\n",
      "1855/25250 (epoch 3), train_loss = 1.241, time/batch = 0.392\n",
      "1856/25250 (epoch 3), train_loss = 1.205, time/batch = 0.433\n",
      "1857/25250 (epoch 3), train_loss = 1.198, time/batch = 0.370\n",
      "1858/25250 (epoch 3), train_loss = 1.182, time/batch = 0.362\n",
      "1859/25250 (epoch 3), train_loss = 1.177, time/batch = 0.375\n",
      "1860/25250 (epoch 3), train_loss = 1.251, time/batch = 0.404\n",
      "1861/25250 (epoch 3), train_loss = 1.224, time/batch = 0.425\n",
      "1862/25250 (epoch 3), train_loss = 1.190, time/batch = 0.405\n",
      "1863/25250 (epoch 3), train_loss = 1.130, time/batch = 0.417\n",
      "1864/25250 (epoch 3), train_loss = 1.205, time/batch = 0.394\n",
      "1865/25250 (epoch 3), train_loss = 1.136, time/batch = 0.372\n",
      "1866/25250 (epoch 3), train_loss = 1.056, time/batch = 0.327\n",
      "1867/25250 (epoch 3), train_loss = 1.102, time/batch = 0.350\n",
      "1868/25250 (epoch 3), train_loss = 1.146, time/batch = 0.369\n",
      "1869/25250 (epoch 3), train_loss = 1.124, time/batch = 0.370\n",
      "1870/25250 (epoch 3), train_loss = 1.202, time/batch = 0.394\n",
      "1871/25250 (epoch 3), train_loss = 1.106, time/batch = 0.427\n",
      "1872/25250 (epoch 3), train_loss = 1.187, time/batch = 0.424\n",
      "1873/25250 (epoch 3), train_loss = 1.141, time/batch = 0.393\n",
      "1874/25250 (epoch 3), train_loss = 1.156, time/batch = 0.361\n",
      "1875/25250 (epoch 3), train_loss = 1.142, time/batch = 0.398\n",
      "1876/25250 (epoch 3), train_loss = 1.144, time/batch = 0.360\n",
      "1877/25250 (epoch 3), train_loss = 1.148, time/batch = 0.387\n",
      "1878/25250 (epoch 3), train_loss = 1.135, time/batch = 0.384\n",
      "1879/25250 (epoch 3), train_loss = 1.178, time/batch = 0.404\n",
      "1880/25250 (epoch 3), train_loss = 1.160, time/batch = 0.475\n",
      "1881/25250 (epoch 3), train_loss = 1.151, time/batch = 0.421\n",
      "1882/25250 (epoch 3), train_loss = 1.173, time/batch = 0.402\n",
      "1883/25250 (epoch 3), train_loss = 1.119, time/batch = 0.392\n",
      "1884/25250 (epoch 3), train_loss = 1.137, time/batch = 0.385\n",
      "1885/25250 (epoch 3), train_loss = 1.132, time/batch = 0.382\n",
      "1886/25250 (epoch 3), train_loss = 1.097, time/batch = 0.387\n",
      "1887/25250 (epoch 3), train_loss = 1.094, time/batch = 0.411\n",
      "1888/25250 (epoch 3), train_loss = 1.210, time/batch = 0.391\n",
      "1889/25250 (epoch 3), train_loss = 1.188, time/batch = 0.377\n",
      "1890/25250 (epoch 3), train_loss = 1.265, time/batch = 0.345\n",
      "1891/25250 (epoch 3), train_loss = 1.186, time/batch = 0.411\n",
      "1892/25250 (epoch 3), train_loss = 1.148, time/batch = 0.518\n",
      "1893/25250 (epoch 3), train_loss = 1.171, time/batch = 0.394\n",
      "1894/25250 (epoch 3), train_loss = 1.147, time/batch = 0.429\n",
      "1895/25250 (epoch 3), train_loss = 1.117, time/batch = 0.409\n",
      "1896/25250 (epoch 3), train_loss = 1.145, time/batch = 0.452\n",
      "1897/25250 (epoch 3), train_loss = 1.077, time/batch = 0.356\n",
      "1898/25250 (epoch 3), train_loss = 1.094, time/batch = 0.350\n",
      "1899/25250 (epoch 3), train_loss = 1.079, time/batch = 0.379\n",
      "1900/25250 (epoch 3), train_loss = 1.104, time/batch = 0.396\n",
      "1901/25250 (epoch 3), train_loss = 1.107, time/batch = 0.354\n",
      "1902/25250 (epoch 3), train_loss = 1.138, time/batch = 0.363\n",
      "1903/25250 (epoch 3), train_loss = 1.201, time/batch = 0.366\n",
      "1904/25250 (epoch 3), train_loss = 1.213, time/batch = 0.387\n",
      "1905/25250 (epoch 3), train_loss = 1.140, time/batch = 0.366\n",
      "1906/25250 (epoch 3), train_loss = 1.254, time/batch = 0.423\n",
      "1907/25250 (epoch 3), train_loss = 1.119, time/batch = 0.396\n",
      "1908/25250 (epoch 3), train_loss = 1.115, time/batch = 0.380\n",
      "1909/25250 (epoch 3), train_loss = 1.233, time/batch = 0.427\n",
      "1910/25250 (epoch 3), train_loss = 1.229, time/batch = 0.386\n",
      "1911/25250 (epoch 3), train_loss = 1.183, time/batch = 0.371\n",
      "1912/25250 (epoch 3), train_loss = 1.170, time/batch = 0.411\n",
      "1913/25250 (epoch 3), train_loss = 1.184, time/batch = 0.396\n",
      "1914/25250 (epoch 3), train_loss = 1.193, time/batch = 0.371\n",
      "1915/25250 (epoch 3), train_loss = 1.156, time/batch = 0.426\n",
      "1916/25250 (epoch 3), train_loss = 1.124, time/batch = 0.479\n",
      "1917/25250 (epoch 3), train_loss = 1.174, time/batch = 0.491\n",
      "1918/25250 (epoch 3), train_loss = 1.190, time/batch = 0.427\n",
      "1919/25250 (epoch 3), train_loss = 1.187, time/batch = 0.422\n",
      "1920/25250 (epoch 3), train_loss = 1.265, time/batch = 0.398\n",
      "1921/25250 (epoch 3), train_loss = 1.172, time/batch = 0.428\n",
      "1922/25250 (epoch 3), train_loss = 1.164, time/batch = 0.389\n",
      "1923/25250 (epoch 3), train_loss = 1.146, time/batch = 0.364\n",
      "1924/25250 (epoch 3), train_loss = 1.140, time/batch = 0.384\n",
      "1925/25250 (epoch 3), train_loss = 1.218, time/batch = 0.407\n",
      "1926/25250 (epoch 3), train_loss = 1.214, time/batch = 0.401\n",
      "1927/25250 (epoch 3), train_loss = 1.131, time/batch = 0.410\n",
      "1928/25250 (epoch 3), train_loss = 1.160, time/batch = 0.400\n",
      "1929/25250 (epoch 3), train_loss = 1.139, time/batch = 0.378\n",
      "1930/25250 (epoch 3), train_loss = 1.120, time/batch = 0.425\n",
      "1931/25250 (epoch 3), train_loss = 1.101, time/batch = 0.415\n",
      "1932/25250 (epoch 3), train_loss = 1.144, time/batch = 0.411\n",
      "1933/25250 (epoch 3), train_loss = 1.111, time/batch = 0.368\n",
      "1934/25250 (epoch 3), train_loss = 1.220, time/batch = 0.426\n",
      "1935/25250 (epoch 3), train_loss = 1.179, time/batch = 0.430\n",
      "1936/25250 (epoch 3), train_loss = 1.199, time/batch = 0.476\n",
      "1937/25250 (epoch 3), train_loss = 1.178, time/batch = 0.462\n",
      "1938/25250 (epoch 3), train_loss = 1.189, time/batch = 0.462\n",
      "1939/25250 (epoch 3), train_loss = 1.119, time/batch = 0.386\n",
      "1940/25250 (epoch 3), train_loss = 1.113, time/batch = 0.381\n",
      "1941/25250 (epoch 3), train_loss = 1.152, time/batch = 0.360\n",
      "1942/25250 (epoch 3), train_loss = 1.123, time/batch = 0.360\n",
      "1943/25250 (epoch 3), train_loss = 1.129, time/batch = 0.370\n",
      "1944/25250 (epoch 3), train_loss = 1.082, time/batch = 0.415\n",
      "1945/25250 (epoch 3), train_loss = 1.146, time/batch = 0.405\n",
      "1946/25250 (epoch 3), train_loss = 1.061, time/batch = 0.342\n",
      "1947/25250 (epoch 3), train_loss = 1.098, time/batch = 0.356\n",
      "1948/25250 (epoch 3), train_loss = 1.067, time/batch = 0.388\n",
      "1949/25250 (epoch 3), train_loss = 1.130, time/batch = 0.344\n",
      "1950/25250 (epoch 3), train_loss = 1.061, time/batch = 0.387\n",
      "1951/25250 (epoch 3), train_loss = 1.076, time/batch = 0.416\n",
      "1952/25250 (epoch 3), train_loss = 1.098, time/batch = 0.337\n",
      "1953/25250 (epoch 3), train_loss = 1.087, time/batch = 0.340\n",
      "1954/25250 (epoch 3), train_loss = 1.076, time/batch = 0.394\n",
      "1955/25250 (epoch 3), train_loss = 1.070, time/batch = 0.432\n",
      "1956/25250 (epoch 3), train_loss = 1.063, time/batch = 0.388\n",
      "1957/25250 (epoch 3), train_loss = 1.094, time/batch = 0.387\n",
      "1958/25250 (epoch 3), train_loss = 1.106, time/batch = 0.394\n",
      "1959/25250 (epoch 3), train_loss = 1.133, time/batch = 0.584\n",
      "1960/25250 (epoch 3), train_loss = 1.035, time/batch = 0.487\n",
      "1961/25250 (epoch 3), train_loss = 1.085, time/batch = 0.366\n",
      "1962/25250 (epoch 3), train_loss = 1.068, time/batch = 0.447\n",
      "1963/25250 (epoch 3), train_loss = 1.077, time/batch = 0.378\n",
      "1964/25250 (epoch 3), train_loss = 1.165, time/batch = 0.376\n",
      "1965/25250 (epoch 3), train_loss = 1.150, time/batch = 0.388\n",
      "1966/25250 (epoch 3), train_loss = 1.179, time/batch = 0.367\n",
      "1967/25250 (epoch 3), train_loss = 1.122, time/batch = 0.355\n",
      "1968/25250 (epoch 3), train_loss = 1.172, time/batch = 0.379\n",
      "1969/25250 (epoch 3), train_loss = 1.219, time/batch = 0.377\n",
      "1970/25250 (epoch 3), train_loss = 1.184, time/batch = 0.374\n",
      "1971/25250 (epoch 3), train_loss = 1.206, time/batch = 0.358\n",
      "1972/25250 (epoch 3), train_loss = 1.128, time/batch = 0.355\n",
      "1973/25250 (epoch 3), train_loss = 1.158, time/batch = 0.425\n",
      "1974/25250 (epoch 3), train_loss = 1.115, time/batch = 0.392\n",
      "1975/25250 (epoch 3), train_loss = 1.134, time/batch = 0.418\n",
      "1976/25250 (epoch 3), train_loss = 1.093, time/batch = 0.416\n",
      "1977/25250 (epoch 3), train_loss = 1.135, time/batch = 0.436\n",
      "1978/25250 (epoch 3), train_loss = 1.091, time/batch = 0.464\n",
      "1979/25250 (epoch 3), train_loss = 1.117, time/batch = 0.496\n",
      "1980/25250 (epoch 3), train_loss = 1.128, time/batch = 0.482\n",
      "1981/25250 (epoch 3), train_loss = 1.224, time/batch = 0.466\n",
      "1982/25250 (epoch 3), train_loss = 1.191, time/batch = 0.467\n",
      "1983/25250 (epoch 3), train_loss = 1.172, time/batch = 0.486\n",
      "1984/25250 (epoch 3), train_loss = 1.181, time/batch = 0.503\n",
      "1985/25250 (epoch 3), train_loss = 1.190, time/batch = 0.432\n",
      "1986/25250 (epoch 3), train_loss = 1.074, time/batch = 0.385\n",
      "1987/25250 (epoch 3), train_loss = 1.167, time/batch = 0.363\n",
      "1988/25250 (epoch 3), train_loss = 1.168, time/batch = 0.341\n",
      "1989/25250 (epoch 3), train_loss = 1.098, time/batch = 0.354\n",
      "1990/25250 (epoch 3), train_loss = 1.172, time/batch = 0.364\n",
      "1991/25250 (epoch 3), train_loss = 1.130, time/batch = 0.361\n",
      "1992/25250 (epoch 3), train_loss = 1.192, time/batch = 0.362\n",
      "1993/25250 (epoch 3), train_loss = 1.144, time/batch = 0.388\n",
      "1994/25250 (epoch 3), train_loss = 1.154, time/batch = 0.373\n",
      "1995/25250 (epoch 3), train_loss = 1.094, time/batch = 0.370\n",
      "1996/25250 (epoch 3), train_loss = 1.115, time/batch = 0.345\n",
      "1997/25250 (epoch 3), train_loss = 1.165, time/batch = 0.619\n",
      "1998/25250 (epoch 3), train_loss = 1.134, time/batch = 0.495\n",
      "1999/25250 (epoch 3), train_loss = 1.180, time/batch = 0.395\n",
      "2000/25250 (epoch 3), train_loss = 1.150, time/batch = 0.390\n",
      "model saved to ./char-rnn-tensorflow/save/model.ckpt\n",
      "2001/25250 (epoch 3), train_loss = 1.202, time/batch = 0.331\n",
      "2002/25250 (epoch 3), train_loss = 1.219, time/batch = 0.426\n",
      "2003/25250 (epoch 3), train_loss = 1.128, time/batch = 0.396\n",
      "2004/25250 (epoch 3), train_loss = 1.163, time/batch = 0.483\n",
      "2005/25250 (epoch 3), train_loss = 1.230, time/batch = 0.471\n",
      "2006/25250 (epoch 3), train_loss = 1.128, time/batch = 0.409\n",
      "2007/25250 (epoch 3), train_loss = 1.194, time/batch = 0.422\n",
      "2008/25250 (epoch 3), train_loss = 1.171, time/batch = 0.424\n",
      "2009/25250 (epoch 3), train_loss = 1.186, time/batch = 0.419\n",
      "2010/25250 (epoch 3), train_loss = 1.159, time/batch = 0.372\n",
      "2011/25250 (epoch 3), train_loss = 1.164, time/batch = 0.347\n",
      "2012/25250 (epoch 3), train_loss = 1.171, time/batch = 0.386\n",
      "2013/25250 (epoch 3), train_loss = 1.105, time/batch = 0.365\n",
      "2014/25250 (epoch 3), train_loss = 1.109, time/batch = 0.365\n",
      "2015/25250 (epoch 3), train_loss = 1.161, time/batch = 0.369\n",
      "2016/25250 (epoch 3), train_loss = 1.145, time/batch = 0.367\n",
      "2017/25250 (epoch 3), train_loss = 1.109, time/batch = 0.360\n",
      "2018/25250 (epoch 3), train_loss = 1.148, time/batch = 0.368\n",
      "2019/25250 (epoch 3), train_loss = 1.242, time/batch = 0.365\n",
      "2020/25250 (epoch 4), train_loss = 1.530, time/batch = 0.347\n",
      "2021/25250 (epoch 4), train_loss = 1.160, time/batch = 0.353\n",
      "2022/25250 (epoch 4), train_loss = 1.165, time/batch = 0.408\n",
      "2023/25250 (epoch 4), train_loss = 1.145, time/batch = 0.371\n",
      "2024/25250 (epoch 4), train_loss = 1.209, time/batch = 0.352\n",
      "2025/25250 (epoch 4), train_loss = 1.169, time/batch = 0.373\n",
      "2026/25250 (epoch 4), train_loss = 1.168, time/batch = 0.364\n",
      "2027/25250 (epoch 4), train_loss = 1.131, time/batch = 0.369\n",
      "2028/25250 (epoch 4), train_loss = 1.130, time/batch = 0.417\n",
      "2029/25250 (epoch 4), train_loss = 1.105, time/batch = 0.386\n",
      "2030/25250 (epoch 4), train_loss = 1.164, time/batch = 0.365\n",
      "2031/25250 (epoch 4), train_loss = 1.123, time/batch = 0.370\n",
      "2032/25250 (epoch 4), train_loss = 1.086, time/batch = 0.380\n",
      "2033/25250 (epoch 4), train_loss = 1.171, time/batch = 0.387\n",
      "2034/25250 (epoch 4), train_loss = 1.210, time/batch = 0.381\n",
      "2035/25250 (epoch 4), train_loss = 1.203, time/batch = 0.420\n",
      "2036/25250 (epoch 4), train_loss = 1.205, time/batch = 0.380\n",
      "2037/25250 (epoch 4), train_loss = 1.267, time/batch = 0.414\n",
      "2038/25250 (epoch 4), train_loss = 1.159, time/batch = 0.420\n",
      "2039/25250 (epoch 4), train_loss = 1.115, time/batch = 0.391\n",
      "2040/25250 (epoch 4), train_loss = 1.070, time/batch = 0.426\n",
      "2041/25250 (epoch 4), train_loss = 1.124, time/batch = 0.400\n",
      "2042/25250 (epoch 4), train_loss = 1.072, time/batch = 0.364\n",
      "2043/25250 (epoch 4), train_loss = 1.159, time/batch = 0.382\n",
      "2044/25250 (epoch 4), train_loss = 1.129, time/batch = 0.516\n",
      "2045/25250 (epoch 4), train_loss = 1.080, time/batch = 0.557\n",
      "2046/25250 (epoch 4), train_loss = 1.097, time/batch = 0.467\n",
      "2047/25250 (epoch 4), train_loss = 1.168, time/batch = 0.437\n",
      "2048/25250 (epoch 4), train_loss = 1.185, time/batch = 0.368\n",
      "2049/25250 (epoch 4), train_loss = 1.088, time/batch = 0.391\n",
      "2050/25250 (epoch 4), train_loss = 1.099, time/batch = 0.382\n",
      "2051/25250 (epoch 4), train_loss = 1.070, time/batch = 0.440\n",
      "2052/25250 (epoch 4), train_loss = 1.050, time/batch = 0.383\n",
      "2053/25250 (epoch 4), train_loss = 1.121, time/batch = 0.403\n",
      "2054/25250 (epoch 4), train_loss = 1.101, time/batch = 0.373\n",
      "2055/25250 (epoch 4), train_loss = 1.203, time/batch = 0.365\n",
      "2056/25250 (epoch 4), train_loss = 1.142, time/batch = 0.341\n",
      "2057/25250 (epoch 4), train_loss = 1.107, time/batch = 0.349\n",
      "2058/25250 (epoch 4), train_loss = 1.080, time/batch = 0.389\n",
      "2059/25250 (epoch 4), train_loss = 1.111, time/batch = 0.376\n",
      "2060/25250 (epoch 4), train_loss = 1.075, time/batch = 0.365\n",
      "2061/25250 (epoch 4), train_loss = 1.179, time/batch = 0.378\n",
      "2062/25250 (epoch 4), train_loss = 1.162, time/batch = 0.349\n",
      "2063/25250 (epoch 4), train_loss = 1.155, time/batch = 0.365\n",
      "2064/25250 (epoch 4), train_loss = 1.014, time/batch = 0.373\n",
      "2065/25250 (epoch 4), train_loss = 1.057, time/batch = 0.350\n",
      "2066/25250 (epoch 4), train_loss = 1.086, time/batch = 0.391\n",
      "2067/25250 (epoch 4), train_loss = 1.145, time/batch = 0.382\n",
      "2068/25250 (epoch 4), train_loss = 1.120, time/batch = 0.370\n",
      "2069/25250 (epoch 4), train_loss = 1.218, time/batch = 0.353\n",
      "2070/25250 (epoch 4), train_loss = 1.167, time/batch = 0.356\n",
      "2071/25250 (epoch 4), train_loss = 1.133, time/batch = 0.393\n",
      "2072/25250 (epoch 4), train_loss = 1.105, time/batch = 0.359\n",
      "2073/25250 (epoch 4), train_loss = 1.197, time/batch = 0.397\n",
      "2074/25250 (epoch 4), train_loss = 1.087, time/batch = 0.359\n",
      "2075/25250 (epoch 4), train_loss = 1.245, time/batch = 0.375\n",
      "2076/25250 (epoch 4), train_loss = 1.163, time/batch = 0.360\n",
      "2077/25250 (epoch 4), train_loss = 1.155, time/batch = 0.369\n",
      "2078/25250 (epoch 4), train_loss = 1.122, time/batch = 0.375\n",
      "2079/25250 (epoch 4), train_loss = 1.106, time/batch = 0.356\n",
      "2080/25250 (epoch 4), train_loss = 1.198, time/batch = 0.379\n",
      "2081/25250 (epoch 4), train_loss = 1.218, time/batch = 0.389\n",
      "2082/25250 (epoch 4), train_loss = 1.174, time/batch = 0.433\n",
      "2083/25250 (epoch 4), train_loss = 1.179, time/batch = 0.451\n",
      "2084/25250 (epoch 4), train_loss = 1.174, time/batch = 0.406\n",
      "2085/25250 (epoch 4), train_loss = 1.152, time/batch = 0.376\n",
      "2086/25250 (epoch 4), train_loss = 1.227, time/batch = 0.388\n",
      "2087/25250 (epoch 4), train_loss = 1.204, time/batch = 0.353\n",
      "2088/25250 (epoch 4), train_loss = 1.215, time/batch = 0.372\n",
      "2089/25250 (epoch 4), train_loss = 1.239, time/batch = 0.378\n",
      "2090/25250 (epoch 4), train_loss = 1.199, time/batch = 0.359\n",
      "2091/25250 (epoch 4), train_loss = 1.147, time/batch = 0.394\n",
      "2092/25250 (epoch 4), train_loss = 1.243, time/batch = 0.357\n",
      "2093/25250 (epoch 4), train_loss = 1.158, time/batch = 0.378\n",
      "2094/25250 (epoch 4), train_loss = 1.159, time/batch = 0.420\n",
      "2095/25250 (epoch 4), train_loss = 1.150, time/batch = 0.389\n",
      "2096/25250 (epoch 4), train_loss = 1.162, time/batch = 0.356\n",
      "2097/25250 (epoch 4), train_loss = 1.197, time/batch = 0.360\n",
      "2098/25250 (epoch 4), train_loss = 1.131, time/batch = 0.357\n",
      "2099/25250 (epoch 4), train_loss = 1.164, time/batch = 0.378\n",
      "2100/25250 (epoch 4), train_loss = 1.196, time/batch = 0.374\n",
      "2101/25250 (epoch 4), train_loss = 1.117, time/batch = 0.381\n",
      "2102/25250 (epoch 4), train_loss = 1.187, time/batch = 0.379\n",
      "2103/25250 (epoch 4), train_loss = 1.186, time/batch = 0.373\n",
      "2104/25250 (epoch 4), train_loss = 1.165, time/batch = 0.374\n",
      "2105/25250 (epoch 4), train_loss = 1.110, time/batch = 0.371\n",
      "2106/25250 (epoch 4), train_loss = 1.141, time/batch = 0.371\n",
      "2107/25250 (epoch 4), train_loss = 1.097, time/batch = 0.359\n",
      "2108/25250 (epoch 4), train_loss = 1.223, time/batch = 0.370\n",
      "2109/25250 (epoch 4), train_loss = 1.278, time/batch = 0.355\n",
      "2110/25250 (epoch 4), train_loss = 1.175, time/batch = 0.400\n",
      "2111/25250 (epoch 4), train_loss = 1.098, time/batch = 0.421\n",
      "2112/25250 (epoch 4), train_loss = 1.150, time/batch = 0.423\n",
      "2113/25250 (epoch 4), train_loss = 1.178, time/batch = 0.367\n",
      "2114/25250 (epoch 4), train_loss = 1.135, time/batch = 0.359\n",
      "2115/25250 (epoch 4), train_loss = 1.201, time/batch = 0.390\n",
      "2116/25250 (epoch 4), train_loss = 1.151, time/batch = 0.408\n",
      "2117/25250 (epoch 4), train_loss = 1.111, time/batch = 0.392\n",
      "2118/25250 (epoch 4), train_loss = 1.160, time/batch = 0.449\n",
      "2119/25250 (epoch 4), train_loss = 1.091, time/batch = 0.409\n",
      "2120/25250 (epoch 4), train_loss = 1.209, time/batch = 0.350\n",
      "2121/25250 (epoch 4), train_loss = 1.204, time/batch = 0.391\n",
      "2122/25250 (epoch 4), train_loss = 1.207, time/batch = 0.390\n",
      "2123/25250 (epoch 4), train_loss = 1.219, time/batch = 0.400\n",
      "2124/25250 (epoch 4), train_loss = 1.134, time/batch = 0.356\n",
      "2125/25250 (epoch 4), train_loss = 1.155, time/batch = 0.343\n",
      "2126/25250 (epoch 4), train_loss = 1.104, time/batch = 0.370\n",
      "2127/25250 (epoch 4), train_loss = 1.051, time/batch = 0.348\n",
      "2128/25250 (epoch 4), train_loss = 1.118, time/batch = 0.378\n",
      "2129/25250 (epoch 4), train_loss = 1.128, time/batch = 0.357\n",
      "2130/25250 (epoch 4), train_loss = 1.085, time/batch = 0.348\n",
      "2131/25250 (epoch 4), train_loss = 1.162, time/batch = 0.417\n",
      "2132/25250 (epoch 4), train_loss = 1.210, time/batch = 0.410\n",
      "2133/25250 (epoch 4), train_loss = 1.151, time/batch = 0.393\n",
      "2134/25250 (epoch 4), train_loss = 1.125, time/batch = 0.362\n",
      "2135/25250 (epoch 4), train_loss = 1.099, time/batch = 0.411\n",
      "2136/25250 (epoch 4), train_loss = 1.127, time/batch = 0.359\n",
      "2137/25250 (epoch 4), train_loss = 1.121, time/batch = 0.356\n",
      "2138/25250 (epoch 4), train_loss = 1.103, time/batch = 0.373\n",
      "2139/25250 (epoch 4), train_loss = 1.113, time/batch = 0.350\n",
      "2140/25250 (epoch 4), train_loss = 1.128, time/batch = 0.378\n",
      "2141/25250 (epoch 4), train_loss = 1.117, time/batch = 0.384\n",
      "2142/25250 (epoch 4), train_loss = 1.164, time/batch = 0.368\n",
      "2143/25250 (epoch 4), train_loss = 1.175, time/batch = 0.353\n",
      "2144/25250 (epoch 4), train_loss = 1.129, time/batch = 0.351\n",
      "2145/25250 (epoch 4), train_loss = 1.154, time/batch = 0.361\n",
      "2146/25250 (epoch 4), train_loss = 1.032, time/batch = 0.374\n",
      "2147/25250 (epoch 4), train_loss = 1.195, time/batch = 0.440\n",
      "2148/25250 (epoch 4), train_loss = 1.168, time/batch = 0.390\n",
      "2149/25250 (epoch 4), train_loss = 1.161, time/batch = 0.387\n",
      "2150/25250 (epoch 4), train_loss = 1.122, time/batch = 0.366\n",
      "2151/25250 (epoch 4), train_loss = 1.137, time/batch = 0.388\n",
      "2152/25250 (epoch 4), train_loss = 1.136, time/batch = 0.359\n",
      "2153/25250 (epoch 4), train_loss = 1.083, time/batch = 0.394\n",
      "2154/25250 (epoch 4), train_loss = 1.086, time/batch = 0.408\n",
      "2155/25250 (epoch 4), train_loss = 1.131, time/batch = 0.527\n",
      "2156/25250 (epoch 4), train_loss = 1.118, time/batch = 0.515\n",
      "2157/25250 (epoch 4), train_loss = 1.131, time/batch = 0.402\n",
      "2158/25250 (epoch 4), train_loss = 1.172, time/batch = 0.359\n",
      "2159/25250 (epoch 4), train_loss = 1.135, time/batch = 0.365\n",
      "2160/25250 (epoch 4), train_loss = 1.195, time/batch = 0.383\n",
      "2161/25250 (epoch 4), train_loss = 1.152, time/batch = 0.388\n",
      "2162/25250 (epoch 4), train_loss = 1.149, time/batch = 0.391\n",
      "2163/25250 (epoch 4), train_loss = 1.121, time/batch = 0.402\n",
      "2164/25250 (epoch 4), train_loss = 1.168, time/batch = 0.401\n",
      "2165/25250 (epoch 4), train_loss = 1.217, time/batch = 0.384\n",
      "2166/25250 (epoch 4), train_loss = 1.175, time/batch = 0.427\n",
      "2167/25250 (epoch 4), train_loss = 1.207, time/batch = 0.425\n",
      "2168/25250 (epoch 4), train_loss = 1.168, time/batch = 0.411\n",
      "2169/25250 (epoch 4), train_loss = 1.209, time/batch = 0.505\n",
      "2170/25250 (epoch 4), train_loss = 1.104, time/batch = 0.520\n",
      "2171/25250 (epoch 4), train_loss = 1.217, time/batch = 0.396\n",
      "2172/25250 (epoch 4), train_loss = 1.176, time/batch = 0.426\n",
      "2173/25250 (epoch 4), train_loss = 1.158, time/batch = 0.371\n",
      "2174/25250 (epoch 4), train_loss = 1.103, time/batch = 0.352\n",
      "2175/25250 (epoch 4), train_loss = 1.204, time/batch = 0.391\n",
      "2176/25250 (epoch 4), train_loss = 1.141, time/batch = 0.550\n",
      "2177/25250 (epoch 4), train_loss = 1.100, time/batch = 0.409\n",
      "2178/25250 (epoch 4), train_loss = 1.141, time/batch = 0.530\n",
      "2179/25250 (epoch 4), train_loss = 1.105, time/batch = 0.487\n",
      "2180/25250 (epoch 4), train_loss = 1.222, time/batch = 0.421\n",
      "2181/25250 (epoch 4), train_loss = 1.115, time/batch = 0.426\n",
      "2182/25250 (epoch 4), train_loss = 1.161, time/batch = 0.400\n",
      "2183/25250 (epoch 4), train_loss = 1.153, time/batch = 0.387\n",
      "2184/25250 (epoch 4), train_loss = 1.113, time/batch = 0.389\n",
      "2185/25250 (epoch 4), train_loss = 1.182, time/batch = 0.376\n",
      "2186/25250 (epoch 4), train_loss = 1.253, time/batch = 0.376\n",
      "2187/25250 (epoch 4), train_loss = 1.221, time/batch = 0.386\n",
      "2188/25250 (epoch 4), train_loss = 1.178, time/batch = 0.390\n",
      "2189/25250 (epoch 4), train_loss = 1.182, time/batch = 0.429\n",
      "2190/25250 (epoch 4), train_loss = 1.095, time/batch = 0.478\n",
      "2191/25250 (epoch 4), train_loss = 1.062, time/batch = 0.428\n",
      "2192/25250 (epoch 4), train_loss = 1.018, time/batch = 0.456\n",
      "2193/25250 (epoch 4), train_loss = 1.072, time/batch = 0.420\n",
      "2194/25250 (epoch 4), train_loss = 1.110, time/batch = 0.397\n",
      "2195/25250 (epoch 4), train_loss = 1.087, time/batch = 0.405\n",
      "2196/25250 (epoch 4), train_loss = 1.093, time/batch = 0.422\n",
      "2197/25250 (epoch 4), train_loss = 1.178, time/batch = 0.418\n",
      "2198/25250 (epoch 4), train_loss = 1.164, time/batch = 0.442\n",
      "2199/25250 (epoch 4), train_loss = 1.195, time/batch = 0.381\n",
      "2200/25250 (epoch 4), train_loss = 1.127, time/batch = 0.416\n",
      "2201/25250 (epoch 4), train_loss = 1.159, time/batch = 0.381\n",
      "2202/25250 (epoch 4), train_loss = 1.161, time/batch = 0.388\n",
      "2203/25250 (epoch 4), train_loss = 1.062, time/batch = 0.367\n",
      "2204/25250 (epoch 4), train_loss = 1.102, time/batch = 0.364\n",
      "2205/25250 (epoch 4), train_loss = 1.255, time/batch = 0.357\n",
      "2206/25250 (epoch 4), train_loss = 1.173, time/batch = 0.398\n",
      "2207/25250 (epoch 4), train_loss = 1.183, time/batch = 0.402\n",
      "2208/25250 (epoch 4), train_loss = 1.152, time/batch = 0.424\n",
      "2209/25250 (epoch 4), train_loss = 1.163, time/batch = 0.417\n",
      "2210/25250 (epoch 4), train_loss = 1.220, time/batch = 0.408\n",
      "2211/25250 (epoch 4), train_loss = 1.236, time/batch = 0.339\n",
      "2212/25250 (epoch 4), train_loss = 1.249, time/batch = 0.359\n",
      "2213/25250 (epoch 4), train_loss = 1.262, time/batch = 0.365\n",
      "2214/25250 (epoch 4), train_loss = 1.207, time/batch = 0.436\n",
      "2215/25250 (epoch 4), train_loss = 1.135, time/batch = 0.383\n",
      "2216/25250 (epoch 4), train_loss = 1.083, time/batch = 0.380\n",
      "2217/25250 (epoch 4), train_loss = 1.134, time/batch = 0.371\n",
      "2218/25250 (epoch 4), train_loss = 1.070, time/batch = 0.367\n",
      "2219/25250 (epoch 4), train_loss = 1.075, time/batch = 0.373\n",
      "2220/25250 (epoch 4), train_loss = 1.085, time/batch = 0.380\n",
      "2221/25250 (epoch 4), train_loss = 1.006, time/batch = 0.407\n",
      "2222/25250 (epoch 4), train_loss = 1.016, time/batch = 0.371\n",
      "2223/25250 (epoch 4), train_loss = 1.050, time/batch = 0.420\n",
      "2224/25250 (epoch 4), train_loss = 1.086, time/batch = 0.431\n",
      "2225/25250 (epoch 4), train_loss = 1.108, time/batch = 0.426\n",
      "2226/25250 (epoch 4), train_loss = 1.131, time/batch = 0.466\n",
      "2227/25250 (epoch 4), train_loss = 1.099, time/batch = 0.416\n",
      "2228/25250 (epoch 4), train_loss = 1.084, time/batch = 0.442\n",
      "2229/25250 (epoch 4), train_loss = 1.133, time/batch = 0.441\n",
      "2230/25250 (epoch 4), train_loss = 1.112, time/batch = 0.423\n",
      "2231/25250 (epoch 4), train_loss = 1.073, time/batch = 0.421\n",
      "2232/25250 (epoch 4), train_loss = 1.098, time/batch = 0.446\n",
      "2233/25250 (epoch 4), train_loss = 1.096, time/batch = 0.397\n",
      "2234/25250 (epoch 4), train_loss = 1.060, time/batch = 0.415\n",
      "2235/25250 (epoch 4), train_loss = 1.061, time/batch = 0.514\n",
      "2236/25250 (epoch 4), train_loss = 1.075, time/batch = 0.513\n",
      "2237/25250 (epoch 4), train_loss = 1.096, time/batch = 0.495\n",
      "2238/25250 (epoch 4), train_loss = 1.068, time/batch = 0.534\n",
      "2239/25250 (epoch 4), train_loss = 1.173, time/batch = 0.520\n",
      "2240/25250 (epoch 4), train_loss = 1.126, time/batch = 0.515\n",
      "2241/25250 (epoch 4), train_loss = 1.153, time/batch = 0.513\n",
      "2242/25250 (epoch 4), train_loss = 1.140, time/batch = 0.457\n",
      "2243/25250 (epoch 4), train_loss = 1.087, time/batch = 0.542\n",
      "2244/25250 (epoch 4), train_loss = 1.050, time/batch = 0.423\n",
      "2245/25250 (epoch 4), train_loss = 1.051, time/batch = 0.426\n",
      "2246/25250 (epoch 4), train_loss = 1.158, time/batch = 0.427\n",
      "2247/25250 (epoch 4), train_loss = 1.151, time/batch = 0.455\n",
      "2248/25250 (epoch 4), train_loss = 1.074, time/batch = 0.416\n",
      "2249/25250 (epoch 4), train_loss = 1.141, time/batch = 0.389\n",
      "2250/25250 (epoch 4), train_loss = 1.060, time/batch = 0.358\n",
      "2251/25250 (epoch 4), train_loss = 1.103, time/batch = 0.357\n",
      "2252/25250 (epoch 4), train_loss = 1.114, time/batch = 0.366\n",
      "2253/25250 (epoch 4), train_loss = 1.163, time/batch = 0.371\n",
      "2254/25250 (epoch 4), train_loss = 1.111, time/batch = 0.431\n",
      "2255/25250 (epoch 4), train_loss = 1.116, time/batch = 0.443\n",
      "2256/25250 (epoch 4), train_loss = 1.118, time/batch = 0.390\n",
      "2257/25250 (epoch 4), train_loss = 1.085, time/batch = 0.394\n",
      "2258/25250 (epoch 4), train_loss = 1.082, time/batch = 0.409\n",
      "2259/25250 (epoch 4), train_loss = 1.181, time/batch = 0.396\n",
      "2260/25250 (epoch 4), train_loss = 1.076, time/batch = 0.400\n",
      "2261/25250 (epoch 4), train_loss = 1.093, time/batch = 0.391\n",
      "2262/25250 (epoch 4), train_loss = 1.117, time/batch = 0.443\n",
      "2263/25250 (epoch 4), train_loss = 1.113, time/batch = 0.377\n",
      "2264/25250 (epoch 4), train_loss = 1.127, time/batch = 0.407\n",
      "2265/25250 (epoch 4), train_loss = 1.142, time/batch = 0.365\n",
      "2266/25250 (epoch 4), train_loss = 1.104, time/batch = 0.523\n",
      "2267/25250 (epoch 4), train_loss = 1.177, time/batch = 0.429\n",
      "2268/25250 (epoch 4), train_loss = 1.194, time/batch = 0.409\n",
      "2269/25250 (epoch 4), train_loss = 1.136, time/batch = 0.387\n",
      "2270/25250 (epoch 4), train_loss = 1.142, time/batch = 0.548\n",
      "2271/25250 (epoch 4), train_loss = 1.156, time/batch = 0.373\n",
      "2272/25250 (epoch 4), train_loss = 1.136, time/batch = 0.378\n",
      "2273/25250 (epoch 4), train_loss = 1.167, time/batch = 0.546\n",
      "2274/25250 (epoch 4), train_loss = 1.130, time/batch = 0.435\n",
      "2275/25250 (epoch 4), train_loss = 1.093, time/batch = 0.562\n",
      "2276/25250 (epoch 4), train_loss = 1.118, time/batch = 0.445\n",
      "2277/25250 (epoch 4), train_loss = 1.108, time/batch = 0.393\n",
      "2278/25250 (epoch 4), train_loss = 1.129, time/batch = 0.411\n",
      "2279/25250 (epoch 4), train_loss = 1.072, time/batch = 0.367\n",
      "2280/25250 (epoch 4), train_loss = 1.140, time/batch = 0.351\n",
      "2281/25250 (epoch 4), train_loss = 1.132, time/batch = 0.372\n",
      "2282/25250 (epoch 4), train_loss = 1.086, time/batch = 0.350\n",
      "2283/25250 (epoch 4), train_loss = 1.154, time/batch = 0.349\n",
      "2284/25250 (epoch 4), train_loss = 1.103, time/batch = 0.398\n",
      "2285/25250 (epoch 4), train_loss = 1.120, time/batch = 0.359\n",
      "2286/25250 (epoch 4), train_loss = 1.073, time/batch = 0.364\n",
      "2287/25250 (epoch 4), train_loss = 1.194, time/batch = 0.360\n",
      "2288/25250 (epoch 4), train_loss = 1.146, time/batch = 0.370\n",
      "2289/25250 (epoch 4), train_loss = 1.099, time/batch = 0.416\n",
      "2290/25250 (epoch 4), train_loss = 1.118, time/batch = 0.440\n",
      "2291/25250 (epoch 4), train_loss = 1.117, time/batch = 0.392\n",
      "2292/25250 (epoch 4), train_loss = 1.117, time/batch = 0.371\n",
      "2293/25250 (epoch 4), train_loss = 1.102, time/batch = 0.449\n",
      "2294/25250 (epoch 4), train_loss = 1.102, time/batch = 0.403\n",
      "2295/25250 (epoch 4), train_loss = 1.123, time/batch = 0.496\n",
      "2296/25250 (epoch 4), train_loss = 1.159, time/batch = 0.540\n",
      "2297/25250 (epoch 4), train_loss = 1.165, time/batch = 0.469\n",
      "2298/25250 (epoch 4), train_loss = 1.154, time/batch = 0.457\n",
      "2299/25250 (epoch 4), train_loss = 1.261, time/batch = 0.466\n",
      "2300/25250 (epoch 4), train_loss = 1.209, time/batch = 0.441\n",
      "2301/25250 (epoch 4), train_loss = 1.237, time/batch = 0.385\n",
      "2302/25250 (epoch 4), train_loss = 1.103, time/batch = 0.472\n",
      "2303/25250 (epoch 4), train_loss = 1.119, time/batch = 0.445\n",
      "2304/25250 (epoch 4), train_loss = 1.139, time/batch = 0.413\n",
      "2305/25250 (epoch 4), train_loss = 1.239, time/batch = 0.397\n",
      "2306/25250 (epoch 4), train_loss = 1.160, time/batch = 0.366\n",
      "2307/25250 (epoch 4), train_loss = 1.134, time/batch = 0.385\n",
      "2308/25250 (epoch 4), train_loss = 1.112, time/batch = 0.389\n",
      "2309/25250 (epoch 4), train_loss = 1.141, time/batch = 0.438\n",
      "2310/25250 (epoch 4), train_loss = 1.199, time/batch = 0.394\n",
      "2311/25250 (epoch 4), train_loss = 1.188, time/batch = 0.365\n",
      "2312/25250 (epoch 4), train_loss = 1.148, time/batch = 0.408\n",
      "2313/25250 (epoch 4), train_loss = 1.214, time/batch = 0.354\n",
      "2314/25250 (epoch 4), train_loss = 1.226, time/batch = 0.358\n",
      "2315/25250 (epoch 4), train_loss = 1.217, time/batch = 0.506\n",
      "2316/25250 (epoch 4), train_loss = 1.207, time/batch = 0.418\n",
      "2317/25250 (epoch 4), train_loss = 1.187, time/batch = 0.382\n",
      "2318/25250 (epoch 4), train_loss = 1.177, time/batch = 0.377\n",
      "2319/25250 (epoch 4), train_loss = 1.262, time/batch = 0.527\n",
      "2320/25250 (epoch 4), train_loss = 1.224, time/batch = 0.391\n",
      "2321/25250 (epoch 4), train_loss = 1.100, time/batch = 0.383\n",
      "2322/25250 (epoch 4), train_loss = 1.065, time/batch = 0.394\n",
      "2323/25250 (epoch 4), train_loss = 1.104, time/batch = 0.452\n",
      "2324/25250 (epoch 4), train_loss = 1.117, time/batch = 0.441\n",
      "2325/25250 (epoch 4), train_loss = 1.198, time/batch = 0.423\n",
      "2326/25250 (epoch 4), train_loss = 1.143, time/batch = 0.431\n",
      "2327/25250 (epoch 4), train_loss = 1.154, time/batch = 0.366\n",
      "2328/25250 (epoch 4), train_loss = 1.147, time/batch = 0.397\n",
      "2329/25250 (epoch 4), train_loss = 1.125, time/batch = 0.385\n",
      "2330/25250 (epoch 4), train_loss = 1.189, time/batch = 0.373\n",
      "2331/25250 (epoch 4), train_loss = 1.164, time/batch = 0.356\n",
      "2332/25250 (epoch 4), train_loss = 1.134, time/batch = 0.370\n",
      "2333/25250 (epoch 4), train_loss = 1.160, time/batch = 0.359\n",
      "2334/25250 (epoch 4), train_loss = 1.190, time/batch = 0.403\n",
      "2335/25250 (epoch 4), train_loss = 1.106, time/batch = 0.389\n",
      "2336/25250 (epoch 4), train_loss = 1.201, time/batch = 0.381\n",
      "2337/25250 (epoch 4), train_loss = 1.192, time/batch = 0.373\n",
      "2338/25250 (epoch 4), train_loss = 1.180, time/batch = 0.381\n",
      "2339/25250 (epoch 4), train_loss = 1.170, time/batch = 0.348\n",
      "2340/25250 (epoch 4), train_loss = 1.177, time/batch = 0.421\n",
      "2341/25250 (epoch 4), train_loss = 1.151, time/batch = 0.357\n",
      "2342/25250 (epoch 4), train_loss = 1.190, time/batch = 0.364\n",
      "2343/25250 (epoch 4), train_loss = 1.102, time/batch = 0.372\n",
      "2344/25250 (epoch 4), train_loss = 1.177, time/batch = 0.411\n",
      "2345/25250 (epoch 4), train_loss = 1.138, time/batch = 0.369\n",
      "2346/25250 (epoch 4), train_loss = 1.210, time/batch = 0.387\n",
      "2347/25250 (epoch 4), train_loss = 1.199, time/batch = 0.402\n",
      "2348/25250 (epoch 4), train_loss = 1.194, time/batch = 0.385\n",
      "2349/25250 (epoch 4), train_loss = 1.182, time/batch = 0.397\n",
      "2350/25250 (epoch 4), train_loss = 1.133, time/batch = 0.401\n",
      "2351/25250 (epoch 4), train_loss = 1.097, time/batch = 0.413\n",
      "2352/25250 (epoch 4), train_loss = 1.183, time/batch = 0.404\n",
      "2353/25250 (epoch 4), train_loss = 1.209, time/batch = 0.403\n",
      "2354/25250 (epoch 4), train_loss = 1.167, time/batch = 0.413\n",
      "2355/25250 (epoch 4), train_loss = 1.120, time/batch = 0.367\n",
      "2356/25250 (epoch 4), train_loss = 1.120, time/batch = 0.365\n",
      "2357/25250 (epoch 4), train_loss = 1.220, time/batch = 0.406\n",
      "2358/25250 (epoch 4), train_loss = 1.151, time/batch = 0.414\n",
      "2359/25250 (epoch 4), train_loss = 1.148, time/batch = 0.454\n",
      "2360/25250 (epoch 4), train_loss = 1.193, time/batch = 0.415\n",
      "2361/25250 (epoch 4), train_loss = 1.164, time/batch = 0.394\n",
      "2362/25250 (epoch 4), train_loss = 1.151, time/batch = 0.384\n",
      "2363/25250 (epoch 4), train_loss = 1.130, time/batch = 0.372\n",
      "2364/25250 (epoch 4), train_loss = 1.130, time/batch = 0.393\n",
      "2365/25250 (epoch 4), train_loss = 1.205, time/batch = 0.403\n",
      "2366/25250 (epoch 4), train_loss = 1.177, time/batch = 0.386\n",
      "2367/25250 (epoch 4), train_loss = 1.137, time/batch = 0.377\n",
      "2368/25250 (epoch 4), train_loss = 1.075, time/batch = 0.363\n",
      "2369/25250 (epoch 4), train_loss = 1.145, time/batch = 0.355\n",
      "2370/25250 (epoch 4), train_loss = 1.078, time/batch = 0.367\n",
      "2371/25250 (epoch 4), train_loss = 1.015, time/batch = 0.380\n",
      "2372/25250 (epoch 4), train_loss = 1.059, time/batch = 0.391\n",
      "2373/25250 (epoch 4), train_loss = 1.100, time/batch = 0.505\n",
      "2374/25250 (epoch 4), train_loss = 1.074, time/batch = 0.432\n",
      "2375/25250 (epoch 4), train_loss = 1.162, time/batch = 0.440\n",
      "2376/25250 (epoch 4), train_loss = 1.064, time/batch = 0.538\n",
      "2377/25250 (epoch 4), train_loss = 1.138, time/batch = 0.574\n",
      "2378/25250 (epoch 4), train_loss = 1.096, time/batch = 0.410\n",
      "2379/25250 (epoch 4), train_loss = 1.105, time/batch = 0.369\n",
      "2380/25250 (epoch 4), train_loss = 1.099, time/batch = 0.378\n",
      "2381/25250 (epoch 4), train_loss = 1.093, time/batch = 0.418\n",
      "2382/25250 (epoch 4), train_loss = 1.111, time/batch = 0.404\n",
      "2383/25250 (epoch 4), train_loss = 1.090, time/batch = 0.369\n",
      "2384/25250 (epoch 4), train_loss = 1.137, time/batch = 0.381\n",
      "2385/25250 (epoch 4), train_loss = 1.116, time/batch = 0.457\n",
      "2386/25250 (epoch 4), train_loss = 1.108, time/batch = 0.373\n",
      "2387/25250 (epoch 4), train_loss = 1.117, time/batch = 0.421\n",
      "2388/25250 (epoch 4), train_loss = 1.076, time/batch = 0.377\n",
      "2389/25250 (epoch 4), train_loss = 1.091, time/batch = 0.361\n",
      "2390/25250 (epoch 4), train_loss = 1.087, time/batch = 0.381\n",
      "2391/25250 (epoch 4), train_loss = 1.056, time/batch = 0.388\n",
      "2392/25250 (epoch 4), train_loss = 1.051, time/batch = 0.354\n",
      "2393/25250 (epoch 4), train_loss = 1.173, time/batch = 0.467\n",
      "2394/25250 (epoch 4), train_loss = 1.151, time/batch = 0.467\n",
      "2395/25250 (epoch 4), train_loss = 1.220, time/batch = 0.377\n",
      "2396/25250 (epoch 4), train_loss = 1.147, time/batch = 0.380\n",
      "2397/25250 (epoch 4), train_loss = 1.111, time/batch = 0.518\n",
      "2398/25250 (epoch 4), train_loss = 1.123, time/batch = 0.352\n",
      "2399/25250 (epoch 4), train_loss = 1.110, time/batch = 0.482\n",
      "2400/25250 (epoch 4), train_loss = 1.073, time/batch = 0.384\n",
      "2401/25250 (epoch 4), train_loss = 1.109, time/batch = 0.423\n",
      "2402/25250 (epoch 4), train_loss = 1.030, time/batch = 0.438\n",
      "2403/25250 (epoch 4), train_loss = 1.049, time/batch = 0.514\n",
      "2404/25250 (epoch 4), train_loss = 1.040, time/batch = 0.475\n",
      "2405/25250 (epoch 4), train_loss = 1.056, time/batch = 0.528\n",
      "2406/25250 (epoch 4), train_loss = 1.053, time/batch = 0.389\n",
      "2407/25250 (epoch 4), train_loss = 1.095, time/batch = 0.374\n",
      "2408/25250 (epoch 4), train_loss = 1.156, time/batch = 0.381\n",
      "2409/25250 (epoch 4), train_loss = 1.172, time/batch = 0.380\n",
      "2410/25250 (epoch 4), train_loss = 1.094, time/batch = 0.367\n",
      "2411/25250 (epoch 4), train_loss = 1.209, time/batch = 0.382\n",
      "2412/25250 (epoch 4), train_loss = 1.080, time/batch = 0.536\n",
      "2413/25250 (epoch 4), train_loss = 1.075, time/batch = 0.443\n",
      "2414/25250 (epoch 4), train_loss = 1.186, time/batch = 0.501\n",
      "2415/25250 (epoch 4), train_loss = 1.176, time/batch = 0.465\n",
      "2416/25250 (epoch 4), train_loss = 1.131, time/batch = 0.454\n",
      "2417/25250 (epoch 4), train_loss = 1.124, time/batch = 0.471\n",
      "2418/25250 (epoch 4), train_loss = 1.134, time/batch = 0.517\n",
      "2419/25250 (epoch 4), train_loss = 1.142, time/batch = 0.516\n",
      "2420/25250 (epoch 4), train_loss = 1.110, time/batch = 0.519\n",
      "2421/25250 (epoch 4), train_loss = 1.074, time/batch = 0.387\n",
      "2422/25250 (epoch 4), train_loss = 1.132, time/batch = 0.426\n",
      "2423/25250 (epoch 4), train_loss = 1.141, time/batch = 0.449\n",
      "2424/25250 (epoch 4), train_loss = 1.133, time/batch = 0.416\n",
      "2425/25250 (epoch 4), train_loss = 1.221, time/batch = 0.474\n",
      "2426/25250 (epoch 4), train_loss = 1.129, time/batch = 0.564\n",
      "2427/25250 (epoch 4), train_loss = 1.120, time/batch = 0.403\n",
      "2428/25250 (epoch 4), train_loss = 1.092, time/batch = 0.390\n",
      "2429/25250 (epoch 4), train_loss = 1.103, time/batch = 0.402\n",
      "2430/25250 (epoch 4), train_loss = 1.168, time/batch = 0.495\n",
      "2431/25250 (epoch 4), train_loss = 1.170, time/batch = 0.378\n",
      "2432/25250 (epoch 4), train_loss = 1.081, time/batch = 0.370\n",
      "2433/25250 (epoch 4), train_loss = 1.115, time/batch = 0.381\n",
      "2434/25250 (epoch 4), train_loss = 1.087, time/batch = 0.386\n",
      "2435/25250 (epoch 4), train_loss = 1.080, time/batch = 0.370\n",
      "2436/25250 (epoch 4), train_loss = 1.060, time/batch = 0.362\n",
      "2437/25250 (epoch 4), train_loss = 1.101, time/batch = 0.378\n",
      "2438/25250 (epoch 4), train_loss = 1.075, time/batch = 0.388\n",
      "2439/25250 (epoch 4), train_loss = 1.174, time/batch = 0.367\n",
      "2440/25250 (epoch 4), train_loss = 1.145, time/batch = 0.402\n",
      "2441/25250 (epoch 4), train_loss = 1.155, time/batch = 0.403\n",
      "2442/25250 (epoch 4), train_loss = 1.138, time/batch = 0.378\n",
      "2443/25250 (epoch 4), train_loss = 1.149, time/batch = 0.371\n",
      "2444/25250 (epoch 4), train_loss = 1.076, time/batch = 0.417\n",
      "2445/25250 (epoch 4), train_loss = 1.083, time/batch = 0.420\n",
      "2446/25250 (epoch 4), train_loss = 1.113, time/batch = 0.404\n",
      "2447/25250 (epoch 4), train_loss = 1.080, time/batch = 0.397\n",
      "2448/25250 (epoch 4), train_loss = 1.079, time/batch = 0.424\n",
      "2449/25250 (epoch 4), train_loss = 1.030, time/batch = 0.443\n",
      "2450/25250 (epoch 4), train_loss = 1.096, time/batch = 0.404\n",
      "2451/25250 (epoch 4), train_loss = 1.008, time/batch = 0.383\n",
      "2452/25250 (epoch 4), train_loss = 1.055, time/batch = 0.406\n",
      "2453/25250 (epoch 4), train_loss = 1.019, time/batch = 0.406\n",
      "2454/25250 (epoch 4), train_loss = 1.076, time/batch = 0.400\n",
      "2455/25250 (epoch 4), train_loss = 1.011, time/batch = 0.380\n",
      "2456/25250 (epoch 4), train_loss = 1.026, time/batch = 0.389\n",
      "2457/25250 (epoch 4), train_loss = 1.046, time/batch = 0.410\n",
      "2458/25250 (epoch 4), train_loss = 1.034, time/batch = 0.433\n",
      "2459/25250 (epoch 4), train_loss = 1.020, time/batch = 0.504\n",
      "2460/25250 (epoch 4), train_loss = 1.020, time/batch = 0.480\n",
      "2461/25250 (epoch 4), train_loss = 1.015, time/batch = 0.381\n",
      "2462/25250 (epoch 4), train_loss = 1.045, time/batch = 0.375\n",
      "2463/25250 (epoch 4), train_loss = 1.048, time/batch = 0.401\n",
      "2464/25250 (epoch 4), train_loss = 1.080, time/batch = 0.357\n",
      "2465/25250 (epoch 4), train_loss = 0.992, time/batch = 0.355\n",
      "2466/25250 (epoch 4), train_loss = 1.041, time/batch = 0.359\n",
      "2467/25250 (epoch 4), train_loss = 1.030, time/batch = 0.349\n",
      "2468/25250 (epoch 4), train_loss = 1.038, time/batch = 0.381\n",
      "2469/25250 (epoch 4), train_loss = 1.124, time/batch = 0.383\n",
      "2470/25250 (epoch 4), train_loss = 1.106, time/batch = 0.372\n",
      "2471/25250 (epoch 4), train_loss = 1.136, time/batch = 0.437\n",
      "2472/25250 (epoch 4), train_loss = 1.085, time/batch = 0.388\n",
      "2473/25250 (epoch 4), train_loss = 1.136, time/batch = 0.362\n",
      "2474/25250 (epoch 4), train_loss = 1.171, time/batch = 0.397\n",
      "2475/25250 (epoch 4), train_loss = 1.135, time/batch = 0.410\n",
      "2476/25250 (epoch 4), train_loss = 1.157, time/batch = 0.423\n",
      "2477/25250 (epoch 4), train_loss = 1.092, time/batch = 0.388\n",
      "2478/25250 (epoch 4), train_loss = 1.113, time/batch = 0.394\n",
      "2479/25250 (epoch 4), train_loss = 1.082, time/batch = 0.400\n",
      "2480/25250 (epoch 4), train_loss = 1.092, time/batch = 0.370\n",
      "2481/25250 (epoch 4), train_loss = 1.043, time/batch = 0.457\n",
      "2482/25250 (epoch 4), train_loss = 1.087, time/batch = 0.360\n",
      "2483/25250 (epoch 4), train_loss = 1.051, time/batch = 0.383\n",
      "2484/25250 (epoch 4), train_loss = 1.073, time/batch = 0.360\n",
      "2485/25250 (epoch 4), train_loss = 1.092, time/batch = 0.367\n",
      "2486/25250 (epoch 4), train_loss = 1.176, time/batch = 0.386\n",
      "2487/25250 (epoch 4), train_loss = 1.139, time/batch = 0.429\n",
      "2488/25250 (epoch 4), train_loss = 1.124, time/batch = 0.444\n",
      "2489/25250 (epoch 4), train_loss = 1.140, time/batch = 0.378\n",
      "2490/25250 (epoch 4), train_loss = 1.150, time/batch = 0.394\n",
      "2491/25250 (epoch 4), train_loss = 1.027, time/batch = 0.396\n",
      "2492/25250 (epoch 4), train_loss = 1.130, time/batch = 0.398\n",
      "2493/25250 (epoch 4), train_loss = 1.130, time/batch = 0.412\n",
      "2494/25250 (epoch 4), train_loss = 1.058, time/batch = 0.538\n",
      "2495/25250 (epoch 4), train_loss = 1.121, time/batch = 0.467\n",
      "2496/25250 (epoch 4), train_loss = 1.088, time/batch = 0.445\n",
      "2497/25250 (epoch 4), train_loss = 1.154, time/batch = 0.367\n",
      "2498/25250 (epoch 4), train_loss = 1.101, time/batch = 0.395\n",
      "2499/25250 (epoch 4), train_loss = 1.111, time/batch = 0.388\n",
      "2500/25250 (epoch 4), train_loss = 1.047, time/batch = 0.400\n",
      "2501/25250 (epoch 4), train_loss = 1.086, time/batch = 0.389\n",
      "2502/25250 (epoch 4), train_loss = 1.129, time/batch = 0.375\n",
      "2503/25250 (epoch 4), train_loss = 1.103, time/batch = 0.383\n",
      "2504/25250 (epoch 4), train_loss = 1.142, time/batch = 0.381\n",
      "2505/25250 (epoch 4), train_loss = 1.113, time/batch = 0.355\n",
      "2506/25250 (epoch 4), train_loss = 1.166, time/batch = 0.368\n",
      "2507/25250 (epoch 4), train_loss = 1.182, time/batch = 0.532\n",
      "2508/25250 (epoch 4), train_loss = 1.101, time/batch = 0.453\n",
      "2509/25250 (epoch 4), train_loss = 1.123, time/batch = 0.492\n",
      "2510/25250 (epoch 4), train_loss = 1.197, time/batch = 0.395\n",
      "2511/25250 (epoch 4), train_loss = 1.086, time/batch = 0.438\n",
      "2512/25250 (epoch 4), train_loss = 1.155, time/batch = 0.507\n",
      "2513/25250 (epoch 4), train_loss = 1.129, time/batch = 0.403\n",
      "2514/25250 (epoch 4), train_loss = 1.148, time/batch = 0.427\n",
      "2515/25250 (epoch 4), train_loss = 1.116, time/batch = 0.422\n",
      "2516/25250 (epoch 4), train_loss = 1.128, time/batch = 0.446\n",
      "2517/25250 (epoch 4), train_loss = 1.133, time/batch = 0.363\n",
      "2518/25250 (epoch 4), train_loss = 1.063, time/batch = 0.407\n",
      "2519/25250 (epoch 4), train_loss = 1.071, time/batch = 0.407\n",
      "2520/25250 (epoch 4), train_loss = 1.124, time/batch = 0.398\n",
      "2521/25250 (epoch 4), train_loss = 1.107, time/batch = 0.411\n",
      "2522/25250 (epoch 4), train_loss = 1.069, time/batch = 0.372\n",
      "2523/25250 (epoch 4), train_loss = 1.105, time/batch = 0.404\n",
      "2524/25250 (epoch 4), train_loss = 1.204, time/batch = 0.430\n",
      "2525/25250 (epoch 5), train_loss = 1.497, time/batch = 0.421\n",
      "2526/25250 (epoch 5), train_loss = 1.134, time/batch = 0.473\n",
      "2527/25250 (epoch 5), train_loss = 1.120, time/batch = 0.425\n",
      "2528/25250 (epoch 5), train_loss = 1.103, time/batch = 0.465\n",
      "2529/25250 (epoch 5), train_loss = 1.167, time/batch = 0.481\n",
      "2530/25250 (epoch 5), train_loss = 1.126, time/batch = 0.434\n",
      "2531/25250 (epoch 5), train_loss = 1.129, time/batch = 0.412\n",
      "2532/25250 (epoch 5), train_loss = 1.099, time/batch = 0.359\n",
      "2533/25250 (epoch 5), train_loss = 1.097, time/batch = 0.358\n",
      "2534/25250 (epoch 5), train_loss = 1.069, time/batch = 0.369\n",
      "2535/25250 (epoch 5), train_loss = 1.126, time/batch = 0.364\n",
      "2536/25250 (epoch 5), train_loss = 1.074, time/batch = 0.355\n",
      "2537/25250 (epoch 5), train_loss = 1.043, time/batch = 0.373\n",
      "2538/25250 (epoch 5), train_loss = 1.122, time/batch = 0.361\n",
      "2539/25250 (epoch 5), train_loss = 1.166, time/batch = 0.371\n",
      "2540/25250 (epoch 5), train_loss = 1.157, time/batch = 0.367\n",
      "2541/25250 (epoch 5), train_loss = 1.162, time/batch = 0.366\n",
      "2542/25250 (epoch 5), train_loss = 1.228, time/batch = 0.394\n",
      "2543/25250 (epoch 5), train_loss = 1.112, time/batch = 0.407\n",
      "2544/25250 (epoch 5), train_loss = 1.076, time/batch = 0.371\n",
      "2545/25250 (epoch 5), train_loss = 1.024, time/batch = 0.416\n",
      "2546/25250 (epoch 5), train_loss = 1.084, time/batch = 0.407\n",
      "2547/25250 (epoch 5), train_loss = 1.023, time/batch = 0.352\n",
      "2548/25250 (epoch 5), train_loss = 1.107, time/batch = 0.352\n",
      "2549/25250 (epoch 5), train_loss = 1.081, time/batch = 0.372\n",
      "2550/25250 (epoch 5), train_loss = 1.045, time/batch = 0.422\n",
      "2551/25250 (epoch 5), train_loss = 1.050, time/batch = 0.382\n",
      "2552/25250 (epoch 5), train_loss = 1.126, time/batch = 0.377\n",
      "2553/25250 (epoch 5), train_loss = 1.139, time/batch = 0.361\n",
      "2554/25250 (epoch 5), train_loss = 1.045, time/batch = 0.378\n",
      "2555/25250 (epoch 5), train_loss = 1.055, time/batch = 0.374\n",
      "2556/25250 (epoch 5), train_loss = 1.025, time/batch = 0.360\n",
      "2557/25250 (epoch 5), train_loss = 1.011, time/batch = 0.376\n",
      "2558/25250 (epoch 5), train_loss = 1.088, time/batch = 0.389\n",
      "2559/25250 (epoch 5), train_loss = 1.063, time/batch = 0.413\n",
      "2560/25250 (epoch 5), train_loss = 1.163, time/batch = 0.380\n",
      "2561/25250 (epoch 5), train_loss = 1.105, time/batch = 0.373\n",
      "2562/25250 (epoch 5), train_loss = 1.066, time/batch = 0.373\n",
      "2563/25250 (epoch 5), train_loss = 1.041, time/batch = 0.397\n",
      "2564/25250 (epoch 5), train_loss = 1.062, time/batch = 0.437\n",
      "2565/25250 (epoch 5), train_loss = 1.040, time/batch = 0.405\n",
      "2566/25250 (epoch 5), train_loss = 1.135, time/batch = 0.387\n",
      "2567/25250 (epoch 5), train_loss = 1.129, time/batch = 0.380\n",
      "2568/25250 (epoch 5), train_loss = 1.121, time/batch = 0.389\n",
      "2569/25250 (epoch 5), train_loss = 0.972, time/batch = 0.510\n",
      "2570/25250 (epoch 5), train_loss = 1.016, time/batch = 0.478\n",
      "2571/25250 (epoch 5), train_loss = 1.047, time/batch = 0.566\n",
      "2572/25250 (epoch 5), train_loss = 1.113, time/batch = 0.524\n",
      "2573/25250 (epoch 5), train_loss = 1.088, time/batch = 0.450\n",
      "2574/25250 (epoch 5), train_loss = 1.190, time/batch = 0.455\n",
      "2575/25250 (epoch 5), train_loss = 1.133, time/batch = 0.487\n",
      "2576/25250 (epoch 5), train_loss = 1.105, time/batch = 0.447\n",
      "2577/25250 (epoch 5), train_loss = 1.067, time/batch = 0.440\n",
      "2578/25250 (epoch 5), train_loss = 1.159, time/batch = 0.450\n",
      "2579/25250 (epoch 5), train_loss = 1.048, time/batch = 0.519\n",
      "2580/25250 (epoch 5), train_loss = 1.204, time/batch = 0.469\n",
      "2581/25250 (epoch 5), train_loss = 1.119, time/batch = 0.439\n",
      "2582/25250 (epoch 5), train_loss = 1.116, time/batch = 0.465\n",
      "2583/25250 (epoch 5), train_loss = 1.094, time/batch = 0.495\n",
      "2584/25250 (epoch 5), train_loss = 1.067, time/batch = 0.558\n",
      "2585/25250 (epoch 5), train_loss = 1.155, time/batch = 0.536\n",
      "2586/25250 (epoch 5), train_loss = 1.177, time/batch = 0.494\n",
      "2587/25250 (epoch 5), train_loss = 1.136, time/batch = 0.417\n",
      "2588/25250 (epoch 5), train_loss = 1.134, time/batch = 0.431\n",
      "2589/25250 (epoch 5), train_loss = 1.140, time/batch = 0.379\n",
      "2590/25250 (epoch 5), train_loss = 1.113, time/batch = 0.362\n",
      "2591/25250 (epoch 5), train_loss = 1.186, time/batch = 0.412\n",
      "2592/25250 (epoch 5), train_loss = 1.149, time/batch = 0.411\n",
      "2593/25250 (epoch 5), train_loss = 1.171, time/batch = 0.380\n",
      "2594/25250 (epoch 5), train_loss = 1.191, time/batch = 0.371\n",
      "2595/25250 (epoch 5), train_loss = 1.158, time/batch = 0.433\n",
      "2596/25250 (epoch 5), train_loss = 1.103, time/batch = 0.443\n",
      "2597/25250 (epoch 5), train_loss = 1.206, time/batch = 0.403\n",
      "2598/25250 (epoch 5), train_loss = 1.120, time/batch = 0.381\n",
      "2599/25250 (epoch 5), train_loss = 1.122, time/batch = 0.398\n",
      "2600/25250 (epoch 5), train_loss = 1.114, time/batch = 0.382\n",
      "2601/25250 (epoch 5), train_loss = 1.132, time/batch = 0.469\n",
      "2602/25250 (epoch 5), train_loss = 1.155, time/batch = 0.413\n",
      "2603/25250 (epoch 5), train_loss = 1.088, time/batch = 0.423\n",
      "2604/25250 (epoch 5), train_loss = 1.122, time/batch = 0.465\n",
      "2605/25250 (epoch 5), train_loss = 1.151, time/batch = 0.417\n",
      "2606/25250 (epoch 5), train_loss = 1.076, time/batch = 0.455\n",
      "2607/25250 (epoch 5), train_loss = 1.146, time/batch = 0.440\n",
      "2608/25250 (epoch 5), train_loss = 1.153, time/batch = 0.387\n",
      "2609/25250 (epoch 5), train_loss = 1.121, time/batch = 0.353\n",
      "2610/25250 (epoch 5), train_loss = 1.069, time/batch = 0.404\n",
      "2611/25250 (epoch 5), train_loss = 1.101, time/batch = 0.417\n",
      "2612/25250 (epoch 5), train_loss = 1.064, time/batch = 0.412\n",
      "2613/25250 (epoch 5), train_loss = 1.187, time/batch = 0.395\n",
      "2614/25250 (epoch 5), train_loss = 1.234, time/batch = 0.368\n",
      "2615/25250 (epoch 5), train_loss = 1.127, time/batch = 0.385\n",
      "2616/25250 (epoch 5), train_loss = 1.050, time/batch = 0.361\n",
      "2617/25250 (epoch 5), train_loss = 1.111, time/batch = 0.388\n",
      "2618/25250 (epoch 5), train_loss = 1.141, time/batch = 0.341\n",
      "2619/25250 (epoch 5), train_loss = 1.097, time/batch = 0.385\n",
      "2620/25250 (epoch 5), train_loss = 1.165, time/batch = 0.392\n",
      "2621/25250 (epoch 5), train_loss = 1.112, time/batch = 0.387\n",
      "2622/25250 (epoch 5), train_loss = 1.084, time/batch = 0.393\n",
      "2623/25250 (epoch 5), train_loss = 1.124, time/batch = 0.399\n",
      "2624/25250 (epoch 5), train_loss = 1.055, time/batch = 0.405\n",
      "2625/25250 (epoch 5), train_loss = 1.169, time/batch = 0.389\n",
      "2626/25250 (epoch 5), train_loss = 1.157, time/batch = 0.417\n",
      "2627/25250 (epoch 5), train_loss = 1.163, time/batch = 0.397\n",
      "2628/25250 (epoch 5), train_loss = 1.178, time/batch = 0.471\n",
      "2629/25250 (epoch 5), train_loss = 1.093, time/batch = 0.506\n",
      "2630/25250 (epoch 5), train_loss = 1.113, time/batch = 0.528\n",
      "2631/25250 (epoch 5), train_loss = 1.065, time/batch = 0.525\n",
      "2632/25250 (epoch 5), train_loss = 1.010, time/batch = 0.453\n",
      "2633/25250 (epoch 5), train_loss = 1.087, time/batch = 0.425\n",
      "2634/25250 (epoch 5), train_loss = 1.091, time/batch = 0.499\n",
      "2635/25250 (epoch 5), train_loss = 1.053, time/batch = 0.474\n",
      "2636/25250 (epoch 5), train_loss = 1.125, time/batch = 0.513\n",
      "2637/25250 (epoch 5), train_loss = 1.173, time/batch = 0.484\n",
      "2638/25250 (epoch 5), train_loss = 1.107, time/batch = 0.421\n",
      "2639/25250 (epoch 5), train_loss = 1.090, time/batch = 0.384\n",
      "2640/25250 (epoch 5), train_loss = 1.059, time/batch = 0.364\n",
      "2641/25250 (epoch 5), train_loss = 1.082, time/batch = 0.372\n",
      "2642/25250 (epoch 5), train_loss = 1.088, time/batch = 0.388\n",
      "2643/25250 (epoch 5), train_loss = 1.066, time/batch = 0.430\n",
      "2644/25250 (epoch 5), train_loss = 1.078, time/batch = 0.404\n",
      "2645/25250 (epoch 5), train_loss = 1.088, time/batch = 0.439\n",
      "2646/25250 (epoch 5), train_loss = 1.075, time/batch = 0.402\n",
      "2647/25250 (epoch 5), train_loss = 1.126, time/batch = 0.415\n",
      "2648/25250 (epoch 5), train_loss = 1.139, time/batch = 0.367\n",
      "2649/25250 (epoch 5), train_loss = 1.096, time/batch = 0.433\n",
      "2650/25250 (epoch 5), train_loss = 1.128, time/batch = 0.471\n",
      "2651/25250 (epoch 5), train_loss = 1.005, time/batch = 0.420\n",
      "2652/25250 (epoch 5), train_loss = 1.171, time/batch = 0.369\n",
      "2653/25250 (epoch 5), train_loss = 1.138, time/batch = 0.368\n",
      "2654/25250 (epoch 5), train_loss = 1.121, time/batch = 0.374\n",
      "2655/25250 (epoch 5), train_loss = 1.079, time/batch = 0.345\n",
      "2656/25250 (epoch 5), train_loss = 1.101, time/batch = 0.383\n",
      "2657/25250 (epoch 5), train_loss = 1.098, time/batch = 0.437\n",
      "2658/25250 (epoch 5), train_loss = 1.053, time/batch = 0.455\n",
      "2659/25250 (epoch 5), train_loss = 1.050, time/batch = 0.397\n",
      "2660/25250 (epoch 5), train_loss = 1.094, time/batch = 0.397\n",
      "2661/25250 (epoch 5), train_loss = 1.088, time/batch = 0.386\n",
      "2662/25250 (epoch 5), train_loss = 1.097, time/batch = 0.433\n",
      "2663/25250 (epoch 5), train_loss = 1.139, time/batch = 0.410\n",
      "2664/25250 (epoch 5), train_loss = 1.104, time/batch = 0.413\n",
      "2665/25250 (epoch 5), train_loss = 1.159, time/batch = 0.460\n",
      "2666/25250 (epoch 5), train_loss = 1.114, time/batch = 0.424\n",
      "2667/25250 (epoch 5), train_loss = 1.112, time/batch = 0.382\n",
      "2668/25250 (epoch 5), train_loss = 1.078, time/batch = 0.350\n",
      "2669/25250 (epoch 5), train_loss = 1.131, time/batch = 0.354\n",
      "2670/25250 (epoch 5), train_loss = 1.189, time/batch = 0.412\n",
      "2671/25250 (epoch 5), train_loss = 1.140, time/batch = 0.374\n",
      "2672/25250 (epoch 5), train_loss = 1.171, time/batch = 0.442\n",
      "2673/25250 (epoch 5), train_loss = 1.133, time/batch = 0.426\n",
      "2674/25250 (epoch 5), train_loss = 1.171, time/batch = 0.388\n",
      "2675/25250 (epoch 5), train_loss = 1.070, time/batch = 0.383\n",
      "2676/25250 (epoch 5), train_loss = 1.182, time/batch = 0.364\n",
      "2677/25250 (epoch 5), train_loss = 1.135, time/batch = 0.380\n",
      "2678/25250 (epoch 5), train_loss = 1.123, time/batch = 0.417\n",
      "2679/25250 (epoch 5), train_loss = 1.074, time/batch = 0.347\n",
      "2680/25250 (epoch 5), train_loss = 1.161, time/batch = 0.375\n",
      "2681/25250 (epoch 5), train_loss = 1.098, time/batch = 0.407\n",
      "2682/25250 (epoch 5), train_loss = 1.063, time/batch = 0.356\n",
      "2683/25250 (epoch 5), train_loss = 1.101, time/batch = 0.368\n",
      "2684/25250 (epoch 5), train_loss = 1.065, time/batch = 0.428\n",
      "2685/25250 (epoch 5), train_loss = 1.185, time/batch = 0.410\n",
      "2686/25250 (epoch 5), train_loss = 1.080, time/batch = 0.391\n",
      "2687/25250 (epoch 5), train_loss = 1.122, time/batch = 0.361\n",
      "2688/25250 (epoch 5), train_loss = 1.117, time/batch = 0.412\n",
      "2689/25250 (epoch 5), train_loss = 1.073, time/batch = 0.378\n",
      "2690/25250 (epoch 5), train_loss = 1.145, time/batch = 0.377\n",
      "2691/25250 (epoch 5), train_loss = 1.216, time/batch = 0.363\n",
      "2692/25250 (epoch 5), train_loss = 1.188, time/batch = 0.364\n",
      "2693/25250 (epoch 5), train_loss = 1.143, time/batch = 0.398\n",
      "2694/25250 (epoch 5), train_loss = 1.146, time/batch = 0.478\n",
      "2695/25250 (epoch 5), train_loss = 1.055, time/batch = 0.412\n",
      "2696/25250 (epoch 5), train_loss = 1.020, time/batch = 0.447\n",
      "2697/25250 (epoch 5), train_loss = 0.979, time/batch = 0.411\n",
      "2698/25250 (epoch 5), train_loss = 1.039, time/batch = 0.391\n",
      "2699/25250 (epoch 5), train_loss = 1.069, time/batch = 0.411\n",
      "2700/25250 (epoch 5), train_loss = 1.060, time/batch = 0.625\n",
      "2701/25250 (epoch 5), train_loss = 1.054, time/batch = 0.590\n",
      "2702/25250 (epoch 5), train_loss = 1.137, time/batch = 0.509\n",
      "2703/25250 (epoch 5), train_loss = 1.121, time/batch = 0.541\n",
      "2704/25250 (epoch 5), train_loss = 1.160, time/batch = 0.562\n",
      "2705/25250 (epoch 5), train_loss = 1.099, time/batch = 0.524\n",
      "2706/25250 (epoch 5), train_loss = 1.130, time/batch = 0.462\n",
      "2707/25250 (epoch 5), train_loss = 1.131, time/batch = 0.424\n",
      "2708/25250 (epoch 5), train_loss = 1.029, time/batch = 0.377\n",
      "2709/25250 (epoch 5), train_loss = 1.066, time/batch = 0.416\n",
      "2710/25250 (epoch 5), train_loss = 1.218, time/batch = 0.492\n",
      "2711/25250 (epoch 5), train_loss = 1.137, time/batch = 0.481\n",
      "2712/25250 (epoch 5), train_loss = 1.152, time/batch = 0.384\n",
      "2713/25250 (epoch 5), train_loss = 1.119, time/batch = 0.386\n",
      "2714/25250 (epoch 5), train_loss = 1.133, time/batch = 0.379\n",
      "2715/25250 (epoch 5), train_loss = 1.179, time/batch = 0.388\n",
      "2716/25250 (epoch 5), train_loss = 1.200, time/batch = 0.377\n",
      "2717/25250 (epoch 5), train_loss = 1.221, time/batch = 0.428\n",
      "2718/25250 (epoch 5), train_loss = 1.228, time/batch = 0.420\n",
      "2719/25250 (epoch 5), train_loss = 1.179, time/batch = 0.497\n",
      "2720/25250 (epoch 5), train_loss = 1.095, time/batch = 0.398\n",
      "2721/25250 (epoch 5), train_loss = 1.054, time/batch = 0.407\n",
      "2722/25250 (epoch 5), train_loss = 1.104, time/batch = 0.397\n",
      "2723/25250 (epoch 5), train_loss = 1.035, time/batch = 0.380\n",
      "2724/25250 (epoch 5), train_loss = 1.037, time/batch = 0.394\n",
      "2725/25250 (epoch 5), train_loss = 1.047, time/batch = 0.371\n",
      "2726/25250 (epoch 5), train_loss = 0.965, time/batch = 0.399\n",
      "2727/25250 (epoch 5), train_loss = 0.981, time/batch = 0.409\n",
      "2728/25250 (epoch 5), train_loss = 1.012, time/batch = 0.414\n",
      "2729/25250 (epoch 5), train_loss = 1.050, time/batch = 0.379\n",
      "2730/25250 (epoch 5), train_loss = 1.068, time/batch = 0.368\n",
      "2731/25250 (epoch 5), train_loss = 1.091, time/batch = 0.372\n",
      "2732/25250 (epoch 5), train_loss = 1.056, time/batch = 0.374\n",
      "2733/25250 (epoch 5), train_loss = 1.040, time/batch = 0.381\n",
      "2734/25250 (epoch 5), train_loss = 1.096, time/batch = 0.435\n",
      "2735/25250 (epoch 5), train_loss = 1.076, time/batch = 0.449\n",
      "2736/25250 (epoch 5), train_loss = 1.031, time/batch = 0.416\n",
      "2737/25250 (epoch 5), train_loss = 1.064, time/batch = 0.408\n",
      "2738/25250 (epoch 5), train_loss = 1.060, time/batch = 0.374\n",
      "2739/25250 (epoch 5), train_loss = 1.024, time/batch = 0.350\n",
      "2740/25250 (epoch 5), train_loss = 1.022, time/batch = 0.364\n",
      "2741/25250 (epoch 5), train_loss = 1.042, time/batch = 0.375\n",
      "2742/25250 (epoch 5), train_loss = 1.058, time/batch = 0.369\n",
      "2743/25250 (epoch 5), train_loss = 1.024, time/batch = 0.373\n",
      "2744/25250 (epoch 5), train_loss = 1.129, time/batch = 0.365\n",
      "2745/25250 (epoch 5), train_loss = 1.092, time/batch = 0.353\n",
      "2746/25250 (epoch 5), train_loss = 1.121, time/batch = 0.357\n",
      "2747/25250 (epoch 5), train_loss = 1.105, time/batch = 0.384\n",
      "2748/25250 (epoch 5), train_loss = 1.040, time/batch = 0.433\n",
      "2749/25250 (epoch 5), train_loss = 1.012, time/batch = 0.376\n",
      "2750/25250 (epoch 5), train_loss = 1.015, time/batch = 0.498\n",
      "2751/25250 (epoch 5), train_loss = 1.112, time/batch = 0.368\n",
      "2752/25250 (epoch 5), train_loss = 1.116, time/batch = 0.390\n",
      "2753/25250 (epoch 5), train_loss = 1.039, time/batch = 0.424\n",
      "2754/25250 (epoch 5), train_loss = 1.100, time/batch = 0.372\n",
      "2755/25250 (epoch 5), train_loss = 1.018, time/batch = 0.387\n",
      "2756/25250 (epoch 5), train_loss = 1.070, time/batch = 0.388\n",
      "2757/25250 (epoch 5), train_loss = 1.074, time/batch = 0.387\n",
      "2758/25250 (epoch 5), train_loss = 1.122, time/batch = 0.400\n",
      "2759/25250 (epoch 5), train_loss = 1.076, time/batch = 0.424\n",
      "2760/25250 (epoch 5), train_loss = 1.077, time/batch = 0.404\n",
      "2761/25250 (epoch 5), train_loss = 1.083, time/batch = 0.425\n",
      "2762/25250 (epoch 5), train_loss = 1.053, time/batch = 0.490\n",
      "2763/25250 (epoch 5), train_loss = 1.049, time/batch = 0.393\n",
      "2764/25250 (epoch 5), train_loss = 1.147, time/batch = 0.398\n",
      "2765/25250 (epoch 5), train_loss = 1.036, time/batch = 0.411\n",
      "2766/25250 (epoch 5), train_loss = 1.049, time/batch = 0.379\n",
      "2767/25250 (epoch 5), train_loss = 1.081, time/batch = 0.413\n",
      "2768/25250 (epoch 5), train_loss = 1.082, time/batch = 0.382\n",
      "2769/25250 (epoch 5), train_loss = 1.097, time/batch = 0.408\n",
      "2770/25250 (epoch 5), train_loss = 1.102, time/batch = 0.487\n",
      "2771/25250 (epoch 5), train_loss = 1.062, time/batch = 0.421\n",
      "2772/25250 (epoch 5), train_loss = 1.138, time/batch = 0.379\n",
      "2773/25250 (epoch 5), train_loss = 1.158, time/batch = 0.364\n",
      "2774/25250 (epoch 5), train_loss = 1.091, time/batch = 0.402\n",
      "2775/25250 (epoch 5), train_loss = 1.102, time/batch = 0.381\n",
      "2776/25250 (epoch 5), train_loss = 1.126, time/batch = 0.434\n",
      "2777/25250 (epoch 5), train_loss = 1.098, time/batch = 0.369\n",
      "2778/25250 (epoch 5), train_loss = 1.131, time/batch = 0.386\n",
      "2779/25250 (epoch 5), train_loss = 1.096, time/batch = 0.352\n",
      "2780/25250 (epoch 5), train_loss = 1.057, time/batch = 0.403\n",
      "2781/25250 (epoch 5), train_loss = 1.088, time/batch = 0.373\n",
      "2782/25250 (epoch 5), train_loss = 1.074, time/batch = 0.369\n",
      "2783/25250 (epoch 5), train_loss = 1.097, time/batch = 0.378\n",
      "2784/25250 (epoch 5), train_loss = 1.041, time/batch = 0.415\n",
      "2785/25250 (epoch 5), train_loss = 1.108, time/batch = 0.418\n",
      "2786/25250 (epoch 5), train_loss = 1.107, time/batch = 0.376\n",
      "2787/25250 (epoch 5), train_loss = 1.055, time/batch = 0.387\n",
      "2788/25250 (epoch 5), train_loss = 1.125, time/batch = 0.341\n",
      "2789/25250 (epoch 5), train_loss = 1.069, time/batch = 0.375\n",
      "2790/25250 (epoch 5), train_loss = 1.087, time/batch = 0.331\n",
      "2791/25250 (epoch 5), train_loss = 1.040, time/batch = 0.361\n",
      "2792/25250 (epoch 5), train_loss = 1.162, time/batch = 0.350\n",
      "2793/25250 (epoch 5), train_loss = 1.110, time/batch = 0.368\n",
      "2794/25250 (epoch 5), train_loss = 1.070, time/batch = 0.393\n",
      "2795/25250 (epoch 5), train_loss = 1.089, time/batch = 0.377\n",
      "2796/25250 (epoch 5), train_loss = 1.079, time/batch = 0.360\n",
      "2797/25250 (epoch 5), train_loss = 1.070, time/batch = 0.426\n",
      "2798/25250 (epoch 5), train_loss = 1.063, time/batch = 0.392\n",
      "2799/25250 (epoch 5), train_loss = 1.071, time/batch = 0.390\n",
      "2800/25250 (epoch 5), train_loss = 1.089, time/batch = 0.400\n",
      "2801/25250 (epoch 5), train_loss = 1.121, time/batch = 0.555\n",
      "2802/25250 (epoch 5), train_loss = 1.136, time/batch = 0.364\n",
      "2803/25250 (epoch 5), train_loss = 1.125, time/batch = 0.467\n",
      "2804/25250 (epoch 5), train_loss = 1.224, time/batch = 0.463\n",
      "2805/25250 (epoch 5), train_loss = 1.174, time/batch = 0.477\n",
      "2806/25250 (epoch 5), train_loss = 1.196, time/batch = 0.529\n",
      "2807/25250 (epoch 5), train_loss = 1.065, time/batch = 0.473\n",
      "2808/25250 (epoch 5), train_loss = 1.087, time/batch = 0.408\n",
      "2809/25250 (epoch 5), train_loss = 1.104, time/batch = 0.428\n",
      "2810/25250 (epoch 5), train_loss = 1.208, time/batch = 0.421\n",
      "2811/25250 (epoch 5), train_loss = 1.130, time/batch = 0.383\n",
      "2812/25250 (epoch 5), train_loss = 1.108, time/batch = 0.421\n",
      "2813/25250 (epoch 5), train_loss = 1.073, time/batch = 0.381\n",
      "2814/25250 (epoch 5), train_loss = 1.111, time/batch = 0.396\n",
      "2815/25250 (epoch 5), train_loss = 1.165, time/batch = 0.345\n",
      "2816/25250 (epoch 5), train_loss = 1.159, time/batch = 0.396\n",
      "2817/25250 (epoch 5), train_loss = 1.116, time/batch = 0.399\n",
      "2818/25250 (epoch 5), train_loss = 1.181, time/batch = 0.358\n",
      "2819/25250 (epoch 5), train_loss = 1.194, time/batch = 0.356\n",
      "2820/25250 (epoch 5), train_loss = 1.182, time/batch = 0.370\n",
      "2821/25250 (epoch 5), train_loss = 1.162, time/batch = 0.366\n",
      "2822/25250 (epoch 5), train_loss = 1.152, time/batch = 0.366\n",
      "2823/25250 (epoch 5), train_loss = 1.143, time/batch = 0.407\n",
      "2824/25250 (epoch 5), train_loss = 1.222, time/batch = 0.382\n",
      "2825/25250 (epoch 5), train_loss = 1.186, time/batch = 0.414\n",
      "2826/25250 (epoch 5), train_loss = 1.059, time/batch = 0.372\n",
      "2827/25250 (epoch 5), train_loss = 1.031, time/batch = 0.373\n",
      "2828/25250 (epoch 5), train_loss = 1.067, time/batch = 0.395\n",
      "2829/25250 (epoch 5), train_loss = 1.084, time/batch = 0.366\n",
      "2830/25250 (epoch 5), train_loss = 1.170, time/batch = 0.360\n",
      "2831/25250 (epoch 5), train_loss = 1.107, time/batch = 0.391\n",
      "2832/25250 (epoch 5), train_loss = 1.117, time/batch = 0.377\n",
      "2833/25250 (epoch 5), train_loss = 1.112, time/batch = 0.375\n",
      "2834/25250 (epoch 5), train_loss = 1.094, time/batch = 0.372\n",
      "2835/25250 (epoch 5), train_loss = 1.155, time/batch = 0.367\n",
      "2836/25250 (epoch 5), train_loss = 1.130, time/batch = 0.373\n",
      "2837/25250 (epoch 5), train_loss = 1.100, time/batch = 0.389\n",
      "2838/25250 (epoch 5), train_loss = 1.128, time/batch = 0.417\n",
      "2839/25250 (epoch 5), train_loss = 1.158, time/batch = 0.427\n",
      "2840/25250 (epoch 5), train_loss = 1.073, time/batch = 0.383\n",
      "2841/25250 (epoch 5), train_loss = 1.167, time/batch = 0.423\n",
      "2842/25250 (epoch 5), train_loss = 1.165, time/batch = 0.445\n",
      "2843/25250 (epoch 5), train_loss = 1.151, time/batch = 0.357\n",
      "2844/25250 (epoch 5), train_loss = 1.133, time/batch = 0.366\n",
      "2845/25250 (epoch 5), train_loss = 1.137, time/batch = 0.359\n",
      "2846/25250 (epoch 5), train_loss = 1.117, time/batch = 0.366\n",
      "2847/25250 (epoch 5), train_loss = 1.152, time/batch = 0.388\n",
      "2848/25250 (epoch 5), train_loss = 1.069, time/batch = 0.392\n",
      "2849/25250 (epoch 5), train_loss = 1.137, time/batch = 0.547\n",
      "2850/25250 (epoch 5), train_loss = 1.112, time/batch = 0.427\n",
      "2851/25250 (epoch 5), train_loss = 1.178, time/batch = 0.456\n",
      "2852/25250 (epoch 5), train_loss = 1.168, time/batch = 0.391\n",
      "2853/25250 (epoch 5), train_loss = 1.164, time/batch = 0.387\n",
      "2854/25250 (epoch 5), train_loss = 1.153, time/batch = 0.377\n",
      "2855/25250 (epoch 5), train_loss = 1.095, time/batch = 0.402\n",
      "2856/25250 (epoch 5), train_loss = 1.064, time/batch = 0.386\n",
      "2857/25250 (epoch 5), train_loss = 1.154, time/batch = 0.396\n",
      "2858/25250 (epoch 5), train_loss = 1.180, time/batch = 0.423\n",
      "2859/25250 (epoch 5), train_loss = 1.133, time/batch = 0.385\n",
      "2860/25250 (epoch 5), train_loss = 1.088, time/batch = 0.404\n",
      "2861/25250 (epoch 5), train_loss = 1.083, time/batch = 0.379\n",
      "2862/25250 (epoch 5), train_loss = 1.184, time/batch = 0.466\n",
      "2863/25250 (epoch 5), train_loss = 1.116, time/batch = 0.390\n",
      "2864/25250 (epoch 5), train_loss = 1.109, time/batch = 0.512\n",
      "2865/25250 (epoch 5), train_loss = 1.156, time/batch = 0.442\n",
      "2866/25250 (epoch 5), train_loss = 1.130, time/batch = 0.401\n",
      "2867/25250 (epoch 5), train_loss = 1.120, time/batch = 0.418\n",
      "2868/25250 (epoch 5), train_loss = 1.089, time/batch = 0.406\n",
      "2869/25250 (epoch 5), train_loss = 1.091, time/batch = 0.487\n",
      "2870/25250 (epoch 5), train_loss = 1.171, time/batch = 0.504\n",
      "2871/25250 (epoch 5), train_loss = 1.142, time/batch = 0.416\n",
      "2872/25250 (epoch 5), train_loss = 1.102, time/batch = 0.406\n",
      "2873/25250 (epoch 5), train_loss = 1.036, time/batch = 0.374\n",
      "2874/25250 (epoch 5), train_loss = 1.099, time/batch = 0.415\n",
      "2875/25250 (epoch 5), train_loss = 1.038, time/batch = 0.452\n",
      "2876/25250 (epoch 5), train_loss = 0.984, time/batch = 0.415\n",
      "2877/25250 (epoch 5), train_loss = 1.028, time/batch = 0.412\n",
      "2878/25250 (epoch 5), train_loss = 1.065, time/batch = 0.374\n",
      "2879/25250 (epoch 5), train_loss = 1.037, time/batch = 0.354\n",
      "2880/25250 (epoch 5), train_loss = 1.125, time/batch = 0.346\n",
      "2881/25250 (epoch 5), train_loss = 1.035, time/batch = 0.381\n",
      "2882/25250 (epoch 5), train_loss = 1.102, time/batch = 0.429\n",
      "2883/25250 (epoch 5), train_loss = 1.065, time/batch = 0.414\n",
      "2884/25250 (epoch 5), train_loss = 1.069, time/batch = 0.394\n",
      "2885/25250 (epoch 5), train_loss = 1.071, time/batch = 0.423\n",
      "2886/25250 (epoch 5), train_loss = 1.056, time/batch = 0.385\n",
      "2887/25250 (epoch 5), train_loss = 1.079, time/batch = 0.362\n",
      "2888/25250 (epoch 5), train_loss = 1.054, time/batch = 0.380\n",
      "2889/25250 (epoch 5), train_loss = 1.108, time/batch = 0.370\n",
      "2890/25250 (epoch 5), train_loss = 1.083, time/batch = 0.363\n",
      "2891/25250 (epoch 5), train_loss = 1.078, time/batch = 0.373\n",
      "2892/25250 (epoch 5), train_loss = 1.081, time/batch = 0.372\n",
      "2893/25250 (epoch 5), train_loss = 1.047, time/batch = 0.371\n",
      "2894/25250 (epoch 5), train_loss = 1.060, time/batch = 0.377\n",
      "2895/25250 (epoch 5), train_loss = 1.054, time/batch = 0.353\n",
      "2896/25250 (epoch 5), train_loss = 1.029, time/batch = 0.393\n",
      "2897/25250 (epoch 5), train_loss = 1.019, time/batch = 0.379\n",
      "2898/25250 (epoch 5), train_loss = 1.145, time/batch = 0.374\n",
      "2899/25250 (epoch 5), train_loss = 1.123, time/batch = 0.374\n",
      "2900/25250 (epoch 5), train_loss = 1.187, time/batch = 0.356\n",
      "2901/25250 (epoch 5), train_loss = 1.114, time/batch = 0.359\n",
      "2902/25250 (epoch 5), train_loss = 1.084, time/batch = 0.357\n",
      "2903/25250 (epoch 5), train_loss = 1.087, time/batch = 0.378\n",
      "2904/25250 (epoch 5), train_loss = 1.084, time/batch = 0.400\n",
      "2905/25250 (epoch 5), train_loss = 1.042, time/batch = 0.397\n",
      "2906/25250 (epoch 5), train_loss = 1.081, time/batch = 0.404\n",
      "2907/25250 (epoch 5), train_loss = 0.998, time/batch = 0.376\n",
      "2908/25250 (epoch 5), train_loss = 1.015, time/batch = 0.379\n",
      "2909/25250 (epoch 5), train_loss = 1.010, time/batch = 0.389\n",
      "2910/25250 (epoch 5), train_loss = 1.026, time/batch = 0.388\n",
      "2911/25250 (epoch 5), train_loss = 1.014, time/batch = 0.383\n",
      "2912/25250 (epoch 5), train_loss = 1.062, time/batch = 0.434\n",
      "2913/25250 (epoch 5), train_loss = 1.124, time/batch = 0.441\n",
      "2914/25250 (epoch 5), train_loss = 1.137, time/batch = 0.367\n",
      "2915/25250 (epoch 5), train_loss = 1.058, time/batch = 0.319\n",
      "2916/25250 (epoch 5), train_loss = 1.177, time/batch = 0.375\n",
      "2917/25250 (epoch 5), train_loss = 1.056, time/batch = 0.381\n",
      "2918/25250 (epoch 5), train_loss = 1.046, time/batch = 0.525\n",
      "2919/25250 (epoch 5), train_loss = 1.153, time/batch = 0.490\n",
      "2920/25250 (epoch 5), train_loss = 1.135, time/batch = 0.407\n",
      "2921/25250 (epoch 5), train_loss = 1.094, time/batch = 0.505\n",
      "2922/25250 (epoch 5), train_loss = 1.094, time/batch = 0.412\n",
      "2923/25250 (epoch 5), train_loss = 1.097, time/batch = 0.412\n",
      "2924/25250 (epoch 5), train_loss = 1.106, time/batch = 0.457\n",
      "2925/25250 (epoch 5), train_loss = 1.073, time/batch = 0.484\n",
      "2926/25250 (epoch 5), train_loss = 1.038, time/batch = 0.503\n",
      "2927/25250 (epoch 5), train_loss = 1.107, time/batch = 0.380\n",
      "2928/25250 (epoch 5), train_loss = 1.108, time/batch = 0.408\n",
      "2929/25250 (epoch 5), train_loss = 1.096, time/batch = 0.367\n",
      "2930/25250 (epoch 5), train_loss = 1.188, time/batch = 0.380\n",
      "2931/25250 (epoch 5), train_loss = 1.097, time/batch = 0.366\n",
      "2932/25250 (epoch 5), train_loss = 1.087, time/batch = 0.398\n",
      "2933/25250 (epoch 5), train_loss = 1.057, time/batch = 0.369\n",
      "2934/25250 (epoch 5), train_loss = 1.077, time/batch = 0.374\n",
      "2935/25250 (epoch 5), train_loss = 1.135, time/batch = 0.370\n",
      "2936/25250 (epoch 5), train_loss = 1.141, time/batch = 0.378\n",
      "2937/25250 (epoch 5), train_loss = 1.046, time/batch = 0.361\n",
      "2938/25250 (epoch 5), train_loss = 1.081, time/batch = 0.394\n",
      "2939/25250 (epoch 5), train_loss = 1.048, time/batch = 0.427\n",
      "2940/25250 (epoch 5), train_loss = 1.050, time/batch = 0.428\n",
      "2941/25250 (epoch 5), train_loss = 1.031, time/batch = 0.365\n",
      "2942/25250 (epoch 5), train_loss = 1.075, time/batch = 0.384\n",
      "2943/25250 (epoch 5), train_loss = 1.050, time/batch = 0.404\n",
      "2944/25250 (epoch 5), train_loss = 1.138, time/batch = 0.380\n",
      "2945/25250 (epoch 5), train_loss = 1.119, time/batch = 0.374\n",
      "2946/25250 (epoch 5), train_loss = 1.127, time/batch = 0.426\n",
      "2947/25250 (epoch 5), train_loss = 1.113, time/batch = 0.424\n",
      "2948/25250 (epoch 5), train_loss = 1.120, time/batch = 0.418\n",
      "2949/25250 (epoch 5), train_loss = 1.045, time/batch = 0.387\n",
      "2950/25250 (epoch 5), train_loss = 1.057, time/batch = 0.376\n",
      "2951/25250 (epoch 5), train_loss = 1.080, time/batch = 0.391\n",
      "2952/25250 (epoch 5), train_loss = 1.041, time/batch = 0.378\n",
      "2953/25250 (epoch 5), train_loss = 1.040, time/batch = 0.376\n",
      "2954/25250 (epoch 5), train_loss = 0.993, time/batch = 0.381\n",
      "2955/25250 (epoch 5), train_loss = 1.061, time/batch = 0.377\n",
      "2956/25250 (epoch 5), train_loss = 0.970, time/batch = 0.372\n",
      "2957/25250 (epoch 5), train_loss = 1.022, time/batch = 0.361\n",
      "2958/25250 (epoch 5), train_loss = 0.985, time/batch = 0.384\n",
      "2959/25250 (epoch 5), train_loss = 1.035, time/batch = 0.414\n",
      "2960/25250 (epoch 5), train_loss = 0.975, time/batch = 0.383\n",
      "2961/25250 (epoch 5), train_loss = 0.993, time/batch = 0.371\n",
      "2962/25250 (epoch 5), train_loss = 1.006, time/batch = 0.392\n",
      "2963/25250 (epoch 5), train_loss = 1.002, time/batch = 0.370\n",
      "2964/25250 (epoch 5), train_loss = 0.983, time/batch = 0.379\n",
      "2965/25250 (epoch 5), train_loss = 0.987, time/batch = 0.414\n",
      "2966/25250 (epoch 5), train_loss = 0.983, time/batch = 0.386\n",
      "2967/25250 (epoch 5), train_loss = 1.009, time/batch = 0.386\n",
      "2968/25250 (epoch 5), train_loss = 1.008, time/batch = 0.386\n",
      "2969/25250 (epoch 5), train_loss = 1.044, time/batch = 0.406\n",
      "2970/25250 (epoch 5), train_loss = 0.961, time/batch = 0.351\n",
      "2971/25250 (epoch 5), train_loss = 1.013, time/batch = 0.390\n",
      "2972/25250 (epoch 5), train_loss = 1.007, time/batch = 0.369\n",
      "2973/25250 (epoch 5), train_loss = 1.009, time/batch = 0.373\n",
      "2974/25250 (epoch 5), train_loss = 1.094, time/batch = 0.395\n",
      "2975/25250 (epoch 5), train_loss = 1.070, time/batch = 0.382\n",
      "2976/25250 (epoch 5), train_loss = 1.106, time/batch = 0.382\n",
      "2977/25250 (epoch 5), train_loss = 1.059, time/batch = 0.397\n",
      "2978/25250 (epoch 5), train_loss = 1.109, time/batch = 0.422\n",
      "2979/25250 (epoch 5), train_loss = 1.140, time/batch = 0.414\n",
      "2980/25250 (epoch 5), train_loss = 1.098, time/batch = 0.377\n",
      "2981/25250 (epoch 5), train_loss = 1.119, time/batch = 0.401\n",
      "2982/25250 (epoch 5), train_loss = 1.057, time/batch = 0.443\n",
      "2983/25250 (epoch 5), train_loss = 1.084, time/batch = 0.434\n",
      "2984/25250 (epoch 5), train_loss = 1.058, time/batch = 0.402\n",
      "2985/25250 (epoch 5), train_loss = 1.062, time/batch = 0.383\n",
      "2986/25250 (epoch 5), train_loss = 1.004, time/batch = 0.404\n",
      "2987/25250 (epoch 5), train_loss = 1.054, time/batch = 0.407\n",
      "2988/25250 (epoch 5), train_loss = 1.017, time/batch = 0.366\n",
      "2989/25250 (epoch 5), train_loss = 1.039, time/batch = 0.369\n",
      "2990/25250 (epoch 5), train_loss = 1.062, time/batch = 0.381\n",
      "2991/25250 (epoch 5), train_loss = 1.139, time/batch = 0.357\n",
      "2992/25250 (epoch 5), train_loss = 1.109, time/batch = 0.367\n",
      "2993/25250 (epoch 5), train_loss = 1.091, time/batch = 0.379\n",
      "2994/25250 (epoch 5), train_loss = 1.105, time/batch = 0.385\n",
      "2995/25250 (epoch 5), train_loss = 1.115, time/batch = 0.379\n",
      "2996/25250 (epoch 5), train_loss = 0.995, time/batch = 0.385\n",
      "2997/25250 (epoch 5), train_loss = 1.100, time/batch = 0.388\n",
      "2998/25250 (epoch 5), train_loss = 1.098, time/batch = 0.412\n",
      "2999/25250 (epoch 5), train_loss = 1.026, time/batch = 0.381\n",
      "3000/25250 (epoch 5), train_loss = 1.085, time/batch = 0.389\n",
      "model saved to ./char-rnn-tensorflow/save/model.ckpt\n",
      "3001/25250 (epoch 5), train_loss = 1.057, time/batch = 0.432\n",
      "3002/25250 (epoch 5), train_loss = 1.125, time/batch = 0.371\n",
      "3003/25250 (epoch 5), train_loss = 1.070, time/batch = 0.364\n",
      "3004/25250 (epoch 5), train_loss = 1.084, time/batch = 0.354\n",
      "3005/25250 (epoch 5), train_loss = 1.016, time/batch = 0.356\n",
      "3006/25250 (epoch 5), train_loss = 1.059, time/batch = 0.473\n",
      "3007/25250 (epoch 5), train_loss = 1.103, time/batch = 0.392\n",
      "3008/25250 (epoch 5), train_loss = 1.081, time/batch = 0.397\n",
      "3009/25250 (epoch 5), train_loss = 1.116, time/batch = 0.398\n",
      "3010/25250 (epoch 5), train_loss = 1.081, time/batch = 0.469\n",
      "3011/25250 (epoch 5), train_loss = 1.139, time/batch = 0.451\n",
      "3012/25250 (epoch 5), train_loss = 1.157, time/batch = 0.359\n",
      "3013/25250 (epoch 5), train_loss = 1.080, time/batch = 0.391\n",
      "3014/25250 (epoch 5), train_loss = 1.095, time/batch = 0.379\n",
      "3015/25250 (epoch 5), train_loss = 1.170, time/batch = 0.353\n",
      "3016/25250 (epoch 5), train_loss = 1.056, time/batch = 0.375\n",
      "3017/25250 (epoch 5), train_loss = 1.124, time/batch = 0.353\n",
      "3018/25250 (epoch 5), train_loss = 1.103, time/batch = 0.362\n",
      "3019/25250 (epoch 5), train_loss = 1.120, time/batch = 0.381\n",
      "3020/25250 (epoch 5), train_loss = 1.091, time/batch = 0.390\n",
      "3021/25250 (epoch 5), train_loss = 1.106, time/batch = 0.389\n",
      "3022/25250 (epoch 5), train_loss = 1.102, time/batch = 0.377\n",
      "3023/25250 (epoch 5), train_loss = 1.033, time/batch = 0.387\n",
      "3024/25250 (epoch 5), train_loss = 1.041, time/batch = 0.363\n",
      "3025/25250 (epoch 5), train_loss = 1.094, time/batch = 0.374\n",
      "3026/25250 (epoch 5), train_loss = 1.081, time/batch = 0.473\n",
      "3027/25250 (epoch 5), train_loss = 1.041, time/batch = 0.407\n",
      "3028/25250 (epoch 5), train_loss = 1.074, time/batch = 0.398\n",
      "3029/25250 (epoch 5), train_loss = 1.176, time/batch = 0.393\n",
      "3030/25250 (epoch 6), train_loss = 1.463, time/batch = 0.376\n",
      "3031/25250 (epoch 6), train_loss = 1.113, time/batch = 0.403\n",
      "3032/25250 (epoch 6), train_loss = 1.090, time/batch = 0.355\n",
      "3033/25250 (epoch 6), train_loss = 1.079, time/batch = 0.371\n",
      "3034/25250 (epoch 6), train_loss = 1.141, time/batch = 0.346\n",
      "3035/25250 (epoch 6), train_loss = 1.093, time/batch = 0.401\n",
      "3036/25250 (epoch 6), train_loss = 1.095, time/batch = 0.393\n",
      "3037/25250 (epoch 6), train_loss = 1.070, time/batch = 0.399\n",
      "3038/25250 (epoch 6), train_loss = 1.072, time/batch = 0.397\n",
      "3039/25250 (epoch 6), train_loss = 1.044, time/batch = 0.388\n",
      "3040/25250 (epoch 6), train_loss = 1.103, time/batch = 0.411\n",
      "3041/25250 (epoch 6), train_loss = 1.034, time/batch = 0.396\n",
      "3042/25250 (epoch 6), train_loss = 1.007, time/batch = 0.397\n",
      "3043/25250 (epoch 6), train_loss = 1.086, time/batch = 0.372\n",
      "3044/25250 (epoch 6), train_loss = 1.135, time/batch = 0.363\n",
      "3045/25250 (epoch 6), train_loss = 1.126, time/batch = 0.431\n",
      "3046/25250 (epoch 6), train_loss = 1.132, time/batch = 0.431\n",
      "3047/25250 (epoch 6), train_loss = 1.198, time/batch = 0.374\n",
      "3048/25250 (epoch 6), train_loss = 1.073, time/batch = 0.361\n",
      "3049/25250 (epoch 6), train_loss = 1.048, time/batch = 0.366\n",
      "3050/25250 (epoch 6), train_loss = 0.991, time/batch = 0.397\n",
      "3051/25250 (epoch 6), train_loss = 1.056, time/batch = 0.390\n",
      "3052/25250 (epoch 6), train_loss = 0.988, time/batch = 0.373\n",
      "3053/25250 (epoch 6), train_loss = 1.070, time/batch = 0.393\n",
      "3054/25250 (epoch 6), train_loss = 1.048, time/batch = 0.375\n",
      "3055/25250 (epoch 6), train_loss = 1.019, time/batch = 0.383\n",
      "3056/25250 (epoch 6), train_loss = 1.019, time/batch = 0.351\n",
      "3057/25250 (epoch 6), train_loss = 1.095, time/batch = 0.355\n",
      "3058/25250 (epoch 6), train_loss = 1.102, time/batch = 0.368\n",
      "3059/25250 (epoch 6), train_loss = 1.017, time/batch = 0.404\n",
      "3060/25250 (epoch 6), train_loss = 1.025, time/batch = 0.372\n",
      "3061/25250 (epoch 6), train_loss = 0.996, time/batch = 0.379\n",
      "3062/25250 (epoch 6), train_loss = 0.986, time/batch = 0.488\n",
      "3063/25250 (epoch 6), train_loss = 1.058, time/batch = 0.380\n",
      "3064/25250 (epoch 6), train_loss = 1.030, time/batch = 0.384\n",
      "3065/25250 (epoch 6), train_loss = 1.132, time/batch = 0.371\n",
      "3066/25250 (epoch 6), train_loss = 1.080, time/batch = 0.366\n",
      "3067/25250 (epoch 6), train_loss = 1.040, time/batch = 0.394\n",
      "3068/25250 (epoch 6), train_loss = 1.012, time/batch = 0.482\n",
      "3069/25250 (epoch 6), train_loss = 1.023, time/batch = 0.368\n",
      "3070/25250 (epoch 6), train_loss = 1.013, time/batch = 0.493\n",
      "3071/25250 (epoch 6), train_loss = 1.100, time/batch = 0.374\n",
      "3072/25250 (epoch 6), train_loss = 1.101, time/batch = 0.495\n",
      "3073/25250 (epoch 6), train_loss = 1.098, time/batch = 0.407\n",
      "3074/25250 (epoch 6), train_loss = 0.945, time/batch = 0.426\n",
      "3075/25250 (epoch 6), train_loss = 0.988, time/batch = 0.376\n",
      "3076/25250 (epoch 6), train_loss = 1.017, time/batch = 0.398\n",
      "3077/25250 (epoch 6), train_loss = 1.086, time/batch = 0.397\n",
      "3078/25250 (epoch 6), train_loss = 1.062, time/batch = 0.399\n",
      "3079/25250 (epoch 6), train_loss = 1.166, time/batch = 0.375\n",
      "3080/25250 (epoch 6), train_loss = 1.107, time/batch = 0.370\n",
      "3081/25250 (epoch 6), train_loss = 1.089, time/batch = 0.419\n",
      "3082/25250 (epoch 6), train_loss = 1.041, time/batch = 0.411\n",
      "3083/25250 (epoch 6), train_loss = 1.130, time/batch = 0.341\n",
      "3084/25250 (epoch 6), train_loss = 1.022, time/batch = 0.361\n",
      "3085/25250 (epoch 6), train_loss = 1.171, time/batch = 0.342\n",
      "3086/25250 (epoch 6), train_loss = 1.087, time/batch = 0.376\n",
      "3087/25250 (epoch 6), train_loss = 1.089, time/batch = 0.381\n",
      "3088/25250 (epoch 6), train_loss = 1.071, time/batch = 0.366\n",
      "3089/25250 (epoch 6), train_loss = 1.038, time/batch = 0.382\n",
      "3090/25250 (epoch 6), train_loss = 1.126, time/batch = 0.380\n",
      "3091/25250 (epoch 6), train_loss = 1.149, time/batch = 0.390\n",
      "3092/25250 (epoch 6), train_loss = 1.112, time/batch = 0.345\n",
      "3093/25250 (epoch 6), train_loss = 1.107, time/batch = 0.393\n",
      "3094/25250 (epoch 6), train_loss = 1.110, time/batch = 0.361\n",
      "3095/25250 (epoch 6), train_loss = 1.083, time/batch = 0.364\n",
      "3096/25250 (epoch 6), train_loss = 1.156, time/batch = 0.394\n",
      "3097/25250 (epoch 6), train_loss = 1.111, time/batch = 0.378\n",
      "3098/25250 (epoch 6), train_loss = 1.138, time/batch = 0.383\n",
      "3099/25250 (epoch 6), train_loss = 1.160, time/batch = 0.367\n",
      "3100/25250 (epoch 6), train_loss = 1.124, time/batch = 0.377\n",
      "3101/25250 (epoch 6), train_loss = 1.072, time/batch = 0.382\n",
      "3102/25250 (epoch 6), train_loss = 1.173, time/batch = 0.379\n",
      "3103/25250 (epoch 6), train_loss = 1.089, time/batch = 0.367\n",
      "3104/25250 (epoch 6), train_loss = 1.096, time/batch = 0.377\n",
      "3105/25250 (epoch 6), train_loss = 1.081, time/batch = 0.475\n",
      "3106/25250 (epoch 6), train_loss = 1.109, time/batch = 0.505\n",
      "3107/25250 (epoch 6), train_loss = 1.122, time/batch = 0.376\n",
      "3108/25250 (epoch 6), train_loss = 1.058, time/batch = 0.351\n",
      "3109/25250 (epoch 6), train_loss = 1.094, time/batch = 0.424\n",
      "3110/25250 (epoch 6), train_loss = 1.116, time/batch = 0.395\n",
      "3111/25250 (epoch 6), train_loss = 1.047, time/batch = 0.434\n",
      "3112/25250 (epoch 6), train_loss = 1.114, time/batch = 0.398\n",
      "3113/25250 (epoch 6), train_loss = 1.131, time/batch = 0.399\n",
      "3114/25250 (epoch 6), train_loss = 1.088, time/batch = 0.376\n",
      "3115/25250 (epoch 6), train_loss = 1.041, time/batch = 0.369\n",
      "3116/25250 (epoch 6), train_loss = 1.069, time/batch = 0.412\n",
      "3117/25250 (epoch 6), train_loss = 1.037, time/batch = 0.451\n",
      "3118/25250 (epoch 6), train_loss = 1.154, time/batch = 0.418\n",
      "3119/25250 (epoch 6), train_loss = 1.200, time/batch = 0.385\n",
      "3120/25250 (epoch 6), train_loss = 1.092, time/batch = 0.374\n",
      "3121/25250 (epoch 6), train_loss = 1.016, time/batch = 0.385\n",
      "3122/25250 (epoch 6), train_loss = 1.081, time/batch = 0.408\n",
      "3123/25250 (epoch 6), train_loss = 1.114, time/batch = 0.429\n",
      "3124/25250 (epoch 6), train_loss = 1.067, time/batch = 0.367\n",
      "3125/25250 (epoch 6), train_loss = 1.135, time/batch = 0.369\n",
      "3126/25250 (epoch 6), train_loss = 1.085, time/batch = 0.424\n",
      "3127/25250 (epoch 6), train_loss = 1.058, time/batch = 0.359\n",
      "3128/25250 (epoch 6), train_loss = 1.091, time/batch = 0.369\n",
      "3129/25250 (epoch 6), train_loss = 1.029, time/batch = 0.373\n",
      "3130/25250 (epoch 6), train_loss = 1.139, time/batch = 0.395\n",
      "3131/25250 (epoch 6), train_loss = 1.118, time/batch = 0.437\n",
      "3132/25250 (epoch 6), train_loss = 1.127, time/batch = 0.478\n",
      "3133/25250 (epoch 6), train_loss = 1.149, time/batch = 0.439\n",
      "3134/25250 (epoch 6), train_loss = 1.065, time/batch = 0.447\n",
      "3135/25250 (epoch 6), train_loss = 1.081, time/batch = 0.394\n",
      "3136/25250 (epoch 6), train_loss = 1.035, time/batch = 0.368\n",
      "3137/25250 (epoch 6), train_loss = 0.979, time/batch = 0.363\n",
      "3138/25250 (epoch 6), train_loss = 1.063, time/batch = 0.346\n",
      "3139/25250 (epoch 6), train_loss = 1.068, time/batch = 0.382\n",
      "3140/25250 (epoch 6), train_loss = 1.026, time/batch = 0.374\n",
      "3141/25250 (epoch 6), train_loss = 1.097, time/batch = 0.334\n",
      "3142/25250 (epoch 6), train_loss = 1.143, time/batch = 0.412\n",
      "3143/25250 (epoch 6), train_loss = 1.076, time/batch = 0.393\n",
      "3144/25250 (epoch 6), train_loss = 1.060, time/batch = 0.480\n",
      "3145/25250 (epoch 6), train_loss = 1.033, time/batch = 0.485\n",
      "3146/25250 (epoch 6), train_loss = 1.051, time/batch = 0.431\n",
      "3147/25250 (epoch 6), train_loss = 1.065, time/batch = 0.375\n",
      "3148/25250 (epoch 6), train_loss = 1.037, time/batch = 0.399\n",
      "3149/25250 (epoch 6), train_loss = 1.049, time/batch = 0.411\n",
      "3150/25250 (epoch 6), train_loss = 1.056, time/batch = 0.381\n",
      "^CTraceback (most recent call last):\n",
      "  File \"char-rnn-tensorflow/train.py\", line 111, in <module>\n",
      "    main()\n",
      "  File \"char-rnn-tensorflow/train.py\", line 48, in main\n",
      "    train(args)\n",
      "  File \"char-rnn-tensorflow/train.py\", line 98, in train\n",
      "    train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 315, in run\n",
      "    return self._run(None, fetches, feed_dict)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 511, in _run\n",
      "    feed_dict_string)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 564, in _do_run\n",
      "    target_list)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 571, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 555, in _run_fn\n",
      "    return tf_session.TF_Run(session, feed_dict, fetch_list, target_list)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python char-rnn-tensorflow/train.py --model gru --data_dir ./char-rnn-tensorflow/data --save_dir ./char-rnn-tensorflow/save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
